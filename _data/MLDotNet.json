{"Data":{"GitHub":{"Issues":[{"Id":"795881414","IsPullRequest":false,"CreatedAt":"2021-01-29T23:10:51","Actor":"mintninthiporn","Number":"5605","RawContent":null,"Title":"mint.nithiporn@gmail.com","State":"closed","Body":"mint.nithiporn@gmail.com\r\n\r\n_Originally posted by @mintninthiporn in https://github.com/dotnet/installer/pull/9562#discussion_r565986960_","Url":"https://github.com/dotnet/machinelearning/issues/5605","RelatedDescription":"Closed issue \"mint.nithiporn@gmail.com\" (#5605)"},{"Id":"796237997","IsPullRequest":false,"CreatedAt":"2021-01-28T18:14:06","Actor":"clthorre","Number":"5606","RawContent":null,"Title":"Cannot use LearnerFeatureSelectionTransform due to \"does not derive from 'Microsoft.ML.ITrainer\" error","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: ubuntu 18.04\r\n- **.NET Version (eg., dotnet --info)**:  2.1\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\nI build mlnet from source and I'm running a command like this:\r\n`dotnet path_to/MML.dll Train trainer=LightGBMBinary{} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-5 col=Text:TX:6  hasHeader=-} xf=TextTransform{col=TextTransformed:Text wordExtractor=NGramExtractorTransform{ngram=2}} xf=LearnerFeatureSelectionTransform{topk=1000 f=AveragedPerceptron{iter=10} feat=TextTransformed} xf=Concat{col=Features:Features,TextTransformed} cache=- randomSeed=1 norm=No v=+`\r\n\r\n- **What happened?**\r\nThe program failed with this error:\r\n\r\nError during class instantiation\r\nLoadable class 'AveragedPerceptron' does not derive from 'Microsoft.ML.ITrainer`1[[Microsoft.ML.Model.IPredictorWithFeatureWeights`1[[System.Single, System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], Microsoft.ML.Data, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51]]'\r\n\r\nException context:\r\n    Throwing component: Environment\r\n\r\n\r\n- **What did you expect?**\r\nI expected it to train a model using the LearnerFeatureSelectionTransform on the Text features.\r\n\r\nI think it may be related to this issue: https://github.com/dotnet/machinelearning/issues/726\r\nThe code has this comment which may also be related. https://github.com/dotnet/machinelearning/blob/5dbfd8acac0bf798957eea122f1413209cdf07dc/src/Microsoft.ML.Transforms/LearnerFeatureSelection.cs#L43\r\n\r\n// ML.Transforms doesn't have a direct reference to ML.StandardTrainers, so use ComponentCatalog to create the Filter\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5606","RelatedDescription":"Open issue \"Cannot use LearnerFeatureSelectionTransform due to \"does not derive from 'Microsoft.ML.ITrainer\" error\" (#5606)"},{"Id":"794608416","IsPullRequest":true,"CreatedAt":"2021-01-28T07:39:37","Actor":"michaelgsharp","Number":"5601","RawContent":null,"Title":"Release/1.5.2 nuget fixes","State":"closed","Body":"Since the myget feeds are deprecated, this PR is to change the feeds to the new URLs. Any version changes are due to different version being in the new feeds. It is the same fix as #5599, expect that since this release is on the old build system, it has additional changes relating to that.\r\n\r\nI decided to fix this release since its the last one on the old build system and its also the last release that uses version 1 of tensorflow. I can imagine that people who can't upgrade to version 2 will need to use this in the future.","Url":"https://github.com/dotnet/machinelearning/pull/5601","RelatedDescription":"Closed or merged PR \"Release/1.5.2 nuget fixes\" (#5601)"},{"Id":"795552318","IsPullRequest":true,"CreatedAt":"2021-01-28T00:14:16","Actor":"michaelgsharp","Number":"5604","RawContent":null,"Title":"Release/1.5.4 fix","State":"closed","Body":"Monday I made this PR but I accidentally had it go into master (#5599). Since the same code was already in there, it caused no code changes. This PR is to actually take the changes to fix the nuget feeds into the correct release/1.5.4 branch.","Url":"https://github.com/dotnet/machinelearning/pull/5604","RelatedDescription":"Closed or merged PR \"Release/1.5.4 fix\" (#5604)"},{"Id":"795429145","IsPullRequest":false,"CreatedAt":"2021-01-27T20:42:18","Actor":"Pierre-datascience","Number":"5603","RawContent":null,"Title":"Discrepancies between nimbusml.LightGbmClassifier and lightgbm.LGBMClassifier scores","State":"open","Body":"### System information\r\n\r\n- **OS**\r\ncompiler   : GCC 7.5.0\r\nsystem     : Linux\r\nrelease    : 4.19.121-linuxkit\r\nmachine    : x86_64\r\nprocessor  : x86_64\r\nCPU cores  : 10\r\ninterpreter: 64bit\r\n\r\n- **libraries (including nimbusml)**: \r\nnumpy 1.19.2\r\npandas 0.25.3\r\nlightgbm 2.3.1\r\nnimbusml 1.8.0\r\n\r\n### Issue\r\n\r\nHello!\r\n\r\nI've been experimenting with nimbusml.LightGbmClassifier to replicate the results from the actual lightgbm.LGBMClassifier.\r\n\r\nFirst, I mapped all the parameters from NimbusML to the LightGBM equivalents (https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html). It seems that some parameters are missing from the NimbusML implementation (like `subsample_for_bin`). In principle, both models should be identical.\r\n\r\nThen, I trained the two models on a toy dataset with 100_000 rows and only 1 column. As you can see below, the stdout/stderr from the .fit() returns the same info (Total Bins=255, Start training from score=0.003080). But the predicted probabilities are slightly different, while I would expect them to be exactly the same.\r\n\r\nGiven the parameters I chose, there shouldn't be any source of randomness, as confirmed by the use of different random seeds for the lightgbm that give the same results. But when I change the `random_state` to None for the nimbusml model, the results are different.\r\n\r\nSo my two (related) questions are\r\n- why are the results different given all parameters (including random state) are identical?\r\n- why changing `random_state` to None gives different results for the nimbusml model? Even though there's no randomness from the model in this example.\r\n\r\n### Source code / logs\r\n\r\n```python3\r\n\r\nfrom nimbusml.ensemble import LightGbmBinaryClassifier\r\nfrom nimbusml.ensemble.booster import Gbdt\r\n\r\nfrom lightgbm import LGBMClassifier\r\n\r\nnimbus_params={\r\n    'number_of_iterations': 1,  # n_estimators\r\n    'learning_rate': 0.1,  # learning_rate\r\n    # subsample_for_bin=200_000,\r\n    'number_of_leaves': 100,  # num_leaves\r\n    'minimum_example_count_per_leaf': 5_000,  # min_child_samples\r\n    'booster': Gbdt(\r\n        feature_fraction=1.0,  # colsample_bytree,\r\n        l1_regularization=0.0,  # reg_alpha\r\n        l2_regularization=0.0,  # reg_lambda\r\n        maximum_tree_depth=0,  # max_depth\r\n        minimum_child_weight=0.001,  # min_child_weight\r\n        minimum_split_gain=0.0,  # min_split_gain\r\n        subsample_fraction=1.0,  # subsample\r\n        subsample_frequency=0,  # subsample_freq\r\n    ),\r\n    'unbalanced_sets': False,  # is_unbalance\r\n    'sigmoid': 1.0,  # sigmoid\r\n    'evaluation_metric': 'Logloss',  # metric\r\n    'maximum_bin_count_per_feature': 254,  # max_bin (matches the 255 from ligthgbm)\r\n    'verbose': 10,  # verbose\r\n    'silent': False,  # silent\r\n    'number_of_threads': 1,  # num_threads, n_jobs\r\n    'early_stopping_round': 0,  # early_stopping_round\r\n    'use_categorical_split': True,  # ?\r\n    'handle_missing_value': True,  # use_missing\r\n    'use_zero_as_missing_value': False,  # zero_as_missing\r\n    'minimum_example_count_per_group': 100,  # min_data_per_group\r\n    'maximum_categorical_split_point_count': 32,  # max_cat_threshold\r\n    'categorical_smoothing': 10.0,  # cat_smooth\r\n    'l2_categorical_regularization': 10.0,  # cat_l2\r\n    'random_state': None,  # random_state,\r\n    'batch_size': None,\r\n    'normalize': None,\r\n    'caching': None,\r\n    'parallel_trainer': None,\r\n    'feature': None,\r\n    'group_id': None,\r\n    'label': None,\r\n    'weight': None,\r\n}\r\n\r\nlgbm_params = {\r\n    'n_estimators': 1,\r\n    'learning_rate': 0.1,\r\n    'subsample_for_bin': 200_000, # Nb of sample for constructing bins (bin_construct_sample_cnt)\r\n    'num_leaves': 100,  # num_leaves\r\n    'min_child_samples': 5_000,\r\n    'boosting_type': 'gbdt',\r\n    'colsample_bytree': 1.0,\r\n    'reg_alpha': 0.0,\r\n    'reg_lambda': 0.0,\r\n    'max_depth': -1,\r\n    'min_child_weight': 0.001,\r\n    'min_split_gain': 0.0,\r\n    'subsample': 1.0,\r\n    'subsample_freq': 0,\r\n    'is_unbalance': False,\r\n    'class_weight': None,\r\n    'sigmoid': 1.0,  # default=1.0\r\n    'metric': 'Logloss',\r\n    'max_bin': 255,\r\n    'verbose': 10,\r\n    'silent': False,\r\n    'n_jobs': 1,\r\n    'early_stopping_round': None,\r\n    'use_missing': True,\r\n    'zero_as_missing': False,\r\n    'min_data_per_group': 100,\r\n    'max_cat_threshold': 32,\r\n    'cat_smooth': 10.0,\r\n    'cat_l2': 10.0,\r\n    'random_state': 42,\r\n}\r\n```\r\n```python3\r\n# Data\r\nX_train = np.random.randn(100_000, 1)\r\ny_train = np.random.binomial(1, .5, size=(100_000, 1)).ravel()\r\n\r\n# Train (actual) lightgbm classifier\r\nlgbm = LGBMClassifier(**lgbm_params)\r\nlgbm.fit(\r\n    X=X_train,\r\n    y=y_train,\r\n)\r\n# Train NimbusML lightgbm classifier\r\nnimbus_lgbm = LightGbmBinaryClassifier(**nimbus_params)\r\nnimbus_lgbm.fit(\r\n    X=X_train,\r\n    y=y_train,\r\n)\r\n```\r\nConsole output\r\n```sh\r\n[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\r\nThis may cause significantly different results comparing to the previous versions of LightGBM.\r\nTry to set boost_from_average=false, if your old models produce bad results\r\n[LightGBM] [Info] Number of positive: 50077, number of negative: 49923\r\n[LightGBM] [Info] Total Bins 255\r\n[LightGBM] [Info] Number of data: 100000, number of used features: 1\r\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500770 -> initscore=0.003080\r\n[LightGBM] [Info] Start training from score 0.003080\r\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\r\n[LightGBM] [Debug] Trained a tree with leaves = 16 and max_depth = 6\r\n\r\n[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\r\nThis may cause significantly different results comparing to the previous versions of LightGBM.\r\nTry to set boost_from_average=false, if your old models produce bad results\r\n[LightGBM] [Info] Number of positive: 50077, number of negative: 49923\r\n[LightGBM] [Info] Total Bins 255\r\n[LightGBM] [Info] Number of data: 100000, number of used features: 1\r\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500770 -> initscore=0.003080\r\n[LightGBM] [Info] Start training from score 0.003080\r\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\r\n```\r\n```python3\r\npd.DataFrame({\r\n    'lightgbm': lgbm.predict_proba(X_train)[:, 1],\r\n    'nimbus_ml': nimbus_lgbm.predict_proba(X_train)[:, 1],\r\n}).head()\r\n```\r\nlightgbm | nimbus_ml\r\n-- | --\r\n0.499577 | 0.499775\r\n0.499391 | 0.499388\r\n0.501513 | 0.501395\r\n0.499577 | 0.501532\r\n0.502616 | 0.50232\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5603","RelatedDescription":"Open issue \"Discrepancies between nimbusml.LightGbmClassifier and lightgbm.LGBMClassifier scores\" (#5603)"},{"Id":"794664693","IsPullRequest":true,"CreatedAt":"2021-01-27T20:32:14","Actor":"ericstj","Number":"5602","RawContent":null,"Title":"Remove references to Microsoft.ML.Scoring","State":"closed","Body":"This was the very first ONNX .NET bindings, it was replaced with Microsoft.ML.OnnxRuntime\r\nthen Microsoft.ML.OnnxRuntime.Managed.\r\n","Url":"https://github.com/dotnet/machinelearning/pull/5602","RelatedDescription":"Closed or merged PR \"Remove references to Microsoft.ML.Scoring\" (#5602)"},{"Id":"794421397","IsPullRequest":true,"CreatedAt":"2021-01-26T19:58:16","Actor":"michaelgsharp","Number":"5599","RawContent":null,"Title":" Release/1.5.4 fix","State":"closed","Body":"Updating 1.5.4 with cherry-picked commits to correct the nuget feeds and CI fixes so it will build correctly. ","Url":"https://github.com/dotnet/machinelearning/pull/5599","RelatedDescription":"Closed or merged PR \" Release/1.5.4 fix\" (#5599)"},{"Id":"794460577","IsPullRequest":false,"CreatedAt":"2021-01-26T18:25:39","Actor":"ericstj","Number":"5600","RawContent":null,"Title":"Enable SDL / release tooling on ML.NET","State":"open","Body":"We should try to plug ML.NET into the automated engineering tooling that we use on other dotnet repos.\r\n\r\nFor example:\r\nbinskim\r\napiscan\r\npolicheck\r\n\r\ncc @Anipik @michaelgsharp ","Url":"https://github.com/dotnet/machinelearning/issues/5600","RelatedDescription":"Open issue \"Enable SDL / release tooling on ML.NET\" (#5600)"},{"Id":"787319901","IsPullRequest":false,"CreatedAt":"2021-01-26T01:17:47","Actor":"nnoradie","Number":"5592","RawContent":null,"Title":"LocalizeRootCauses not returning results for some points, even with \"IsAnomaly\":true and high deltas in the localization input","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**:\r\n- **.NET Version (eg., dotnet --info)**: \r\n![image](https://user-images.githubusercontent.com/69877427/104793862-102ca800-5759-11eb-95bb-ea0baac51236.png)\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\n    - Call LocalizeRootCauses on various anomaly points\r\n\r\n- **What happened?**\r\n    - There were time series points that were found to be an anomaly, but no root causes were returned in the results\r\n\r\n- **What did you expect?**\r\n    - At least one root cause to be returned\r\n\r\n### Source code / logs\r\nzip contains csv with the localization input and output serialized to json. \r\n[localization_input.zip](https://github.com/dotnet/machinelearning/files/5823641/localization_input.zip)\r\n\r\nFor all cases, rootCauseThreshold=0.55, beta=0.\r\n\r\nPlease paste or attach the code or logs or traces that would be helpful to diagnose the issue you are reporting.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5592","RelatedDescription":"Closed issue \"LocalizeRootCauses not returning results for some points, even with \"IsAnomaly\":true and high deltas in the localization input\" (#5592)"},{"Id":"793289923","IsPullRequest":false,"CreatedAt":"2021-01-25T11:14:46","Actor":"justinormont","Number":"5598","RawContent":null,"Title":"SymSGD Excessively Verbose","State":"open","Body":"Remove logging line, or modify from `ch.Info` to `ch.Trace`:\r\nhttps://github.com/dotnet/machinelearning/blob/5dbfd8acac0bf798957eea122f1413209cdf07dc/src/Microsoft.ML.Mkl.Components/SymSgdClassificationTrainer.cs#L813\r\n\r\nFor my text dataset, this logging line dumps ~100 pages of floats to my console. That level of verbosity is unneeded at the `Info` level.\r\n\r\nI'd recommend just removing the logging line.","Url":"https://github.com/dotnet/machinelearning/issues/5598","RelatedDescription":"Open issue \"SymSGD Excessively Verbose\" (#5598)"},{"Id":"792266526","IsPullRequest":true,"CreatedAt":"2021-01-22T21:53:47","Actor":"michaelgsharp","Number":"5596","RawContent":null,"Title":"Fixing official build","State":"closed","Body":"Last week we fixed the CI build with a macos homebrew bug workaround. The official build YAML and the CI YAML are different files, and the workaround was not added to the official build YAML. This PR adds in the fix so the official builds will pass.","Url":"https://github.com/dotnet/machinelearning/pull/5596","RelatedDescription":"Closed or merged PR \"Fixing official build\" (#5596)"},{"Id":"792304346","IsPullRequest":false,"CreatedAt":"2021-01-22T21:04:28","Actor":"farhanrw","Number":"5597","RawContent":null,"Title":"Slow inference on CPU ","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: Windows 10.0 Build 18363\r\n- **.NET Version (eg., dotnet --info)**: 3.1.405 / 5.0.102\r\n\r\n![image](https://user-images.githubusercontent.com/43019847/105546891-bd03a980-5cb2-11eb-9500-cb28ea81dd2a.png)\r\n\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\n- I am very new to ML.Net, to be honest, I have never used it until recently. I have a model trained on pytorch which I exported to ONNX. I used a yolov5 network (_https://github.com/ultralytics/yolov5_) and used this code (_https://github.com/BobLd/YOLOv4MLNet/tree/yolo-v5-incl_) to run the inference. Please note that I am not running tests in batches, rather one image at a time or **batch size = 1**. My inferences are all the time ran in the **CPU**.\r\n- **What happened?**\r\n- In python/using pytorch the inference time for one image on the CPU would take me around **100~300 ms** on average. However, when I ran it using the implementation I found on the second repo I mentioned above the runtime for each image varies from **2000~3700 ms**. And these timings are just to run the prediction function, not to setup the model or anything else. \r\n- **What did you expect?**\r\n- I was expecting to get a runtime similar to the one I go in python if not better. As I am not including the times taken for post-processing or the time for parsing the outputs the timing should stay close if not a little better/worse. But going from about 200/300ms to 2000/4000ms is a big jump.\r\n\r\n### Source code / logs\r\nOnce again these two are the repos \r\nhttps://github.com/ultralytics/yolov5 (Original python implementation of yolov5)\r\nhttps://github.com/BobLd/YOLOv4MLNet/tree/yolo-v5-incl (ML.Net code to take the ONNX model and run inference)\r\n\r\nPlease paste or attach the code or logs or traces that would be helpful to diagnose the issue you are reporting.\r\n![image](https://user-images.githubusercontent.com/43019847/105546216-f38cf480-5cb1-11eb-9b73-09a721c96214.png)\r\n\r\nLastly, **line number 75** is what is taking the longest amount of time. It takes **80-90%** of the total runtime.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5597","RelatedDescription":"Open issue \"Slow inference on CPU \" (#5597)"},{"Id":"785337400","IsPullRequest":true,"CreatedAt":"2021-01-21T20:59:00","Actor":"michaelgsharp","Number":"5586","RawContent":null,"Title":"Fixed Averaged Perceptron default value","State":"closed","Body":"Fixes #5568. Several months ago the default value of iterations for averaged perceptron was changed from 1 to 10. It was missed in 1 location, so this pr addresses that and fixes it there as well.","Url":"https://github.com/dotnet/machinelearning/pull/5586","RelatedDescription":"Closed or merged PR \"Fixed Averaged Perceptron default value\" (#5586)"},{"Id":"790902226","IsPullRequest":false,"CreatedAt":"2021-01-21T10:12:54","Actor":"Titibo26","Number":"5595","RawContent":null,"Title":"Searching for documentation on image regression","State":"open","Body":"Hi,\r\n1) I'd like to train a model to make depth estimation on monocular rgb picture. \r\nI think this can be done though regression with resnet or densenet.\r\n\r\nI have a dataset ( [https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html](url) ) with pairs of pictures (input / result needed) :\r\nRgb_img_1 / depth_img_1\r\n And i have an Excel file with each path to each files.\r\n\r\nI started with the multiclassification tutorial ( [https://docs.microsoft.com/fr-fr/dotnet/machine-learning/tutorials/image-classification](url) ) but now, i have to translate it to a regression model as i'm searching for depth values for each pixel of a picture.\r\n\r\nI know that i have to change my model generation : \r\n`\r\npublic static ITransformer GenerateModel(MLContext mlContext)\r\n        {\r\n\r\n            IDataView trainingData = mlContext.Data.LoadFromTextFile<ImageData>(path: _trainTagsCsv, separatorChar: ',', hasHeader: false);\r\n\r\n\r\n\r\n\r\n            IEstimator<ITransformer> pipeline = mlContext.Transforms.LoadImages(outputColumnName: \"input\", imageFolder: _imagesFolder, inputColumnName: nameof(ImageData.InputImagePath))\r\n                  // The image transforms transform the images into the model's expected format.\r\n                  .Append(mlContext.Transforms.ResizeImages(outputColumnName: \"input\", imageWidth: InceptionSettings.ImageWidth, imageHeight: InceptionSettings.ImageHeight, inputColumnName: \"input\"))\r\n                  .Append(mlContext.Transforms.ExtractPixels(outputColumnName: \"input\", interleavePixelColors: InceptionSettings.ChannelsLast, offsetImage: InceptionSettings.Mean))\r\n                  .Append(mlContext.Model.LoadTensorFlowModel(_inceptionTensorFlowModel)\r\n                  .ScoreTensorFlowModel(outputColumnNames: new[] { \"softmax2_pre_activation\" }, inputColumnNames: new[] { \"input\" }, addBatchDimensionInput: true))\r\n                  .Append(mlContext.Transforms.Conversion.MapValueToKey(outputColumnName: \"LabelKey\", inputColumnName: \"Label\"))\r\n                  .Append(mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy(labelColumnName: \"LabelKey\", featureColumnName: \"softmax2_pre_activation\"))\r\n                  .Append(mlContext.Transforms.Conversion.MapKeyToValue(\"PredictedLabelValue\", \"PredictedLabel\"))\r\n                  .AppendCacheCheckpoint(mlContext);\r\n\r\n            ITransformer model = pipeline.Fit(trainingData);\r\n\r\n            IDataView testData = mlContext.Data.LoadFromTextFile<ImageData>(path: _testTagsCsv, hasHeader: false);\r\n            IDataView predictions = model.Transform(testData);\r\n\r\n            // Create an IEnumerable for the predictions for displaying results\r\n            IEnumerable<ImagePrediction> imagePredictionData = mlContext.Data.CreateEnumerable<ImagePrediction>(predictions, true);\r\n            DisplayResults(imagePredictionData);\r\n\r\n            MulticlassClassificationMetrics metrics = mlContext.MulticlassClassification\r\n                .Evaluate(predictions, labelColumnName: \"LabelKey\", predictedLabelColumnName: \"PredictedLabel\");\r\n\r\n            Console.WriteLine($\"LogLoss is: {metrics.LogLoss}\");\r\n            Console.WriteLine($\"PerClassLogLoss is: {String.Join(\" , \", metrics.PerClassLogLoss.Select(c => c.ToString()))}\");\r\n\r\n            return model;\r\n        }\r\n`\r\n\r\nCould you tell me where can i find docs and ressources to understand : \r\n- how to choose and use a appropriated model\r\n- how to transform my inputs to make it usable for the model.\r\n\r\n2) Also, i have a .onnx of densenet. would it be easier to go this way instead of using a ml.net model ? (but i'd like to deeply understand ml.net framework)\r\n\r\n3) Also i took a look on autoMl but i dont think it can resolve my regression problem with images input. Is this right ?\r\n\r\nThanks,\r\n\r\n\r\n ","Url":"https://github.com/dotnet/machinelearning/issues/5595","RelatedDescription":"Open issue \"Searching for documentation on image regression\" (#5595)"},{"Id":"790375314","IsPullRequest":false,"CreatedAt":"2021-01-20T22:12:25","Actor":"luisquintanilla","Number":"5594","RawContent":null,"Title":"[GPU] What is the required minimum version of CUDA?","State":"open","Body":"Raising a question based on docs issue dotnet/docs#21251\r\n\r\nDo the existing docs need to be updated with another CUDA version? https://github.com/dotnet/machinelearning/blob/master/docs/api-reference/tensorflow-usage.md\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5594","RelatedDescription":"Open issue \"[GPU] What is the required minimum version of CUDA?\" (#5594)"},{"Id":"789774462","IsPullRequest":false,"CreatedAt":"2021-01-20T09:26:08","Actor":"vivek-kumar-gupt","Number":"5593","RawContent":null,"Title":"v1.5.4 model training failing","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**: Windows 10 pro x64\r\n- **.NET Version (eg., dotnet --info)**: .net framework 4.6/4.7\r\n\r\n### Issue\r\n\r\n- Tried training a model for image classification based on example \"Image Classification Model Training\" from \"machinelearning-samples\" repository\r\n- Process execution failed with unhandled exception while training model \r\n- Expected that the example should have worked (tried with release v1.5.4)\r\n\r\n### Source code / logs\r\n\r\nUnhandled Exception: System.EntryPointNotFoundException: Unable to find an entry point named 'TF_StringEncodedSize' in DLL 'tensorflow'.\r\n   at Tensorflow.c_api.TF_StringEncodedSize(UInt64 len)\r\n   at Microsoft.ML.Vision.ImageClassificationTrainer.EncodeByteAsString(VBuffer`1 buffer)\r\n   at Microsoft.ML.Vision.ImageClassificationTrainer.ImageProcessor.ProcessImage(VBuffer`1& imageBuffer)","Url":"https://github.com/dotnet/machinelearning/issues/5593","RelatedDescription":"Closed issue \"v1.5.4 model training failing\" (#5593)"},{"Id":"786856139","IsPullRequest":false,"CreatedAt":"2021-01-15T12:22:48","Actor":"CreedsCode","Number":"5591","RawContent":null,"Title":"System.Threading.ThreadAbortException: \"System error.\"","State":"open","Body":"### System information\r\n\r\n- **Win10 V.1909**:\r\n- **5.0.101**: \r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\nThe model is embedded as a resource in the singe file build and gets extracted to the user's temp directory. For the use in the ConsumerModel (here: RLNModel)\r\n- **What happened?**\r\nAt the execution of the model,\r\n- **What did you expect?**\r\nTo run like before, when the model is in the directory of the executable. Accessed over Directory.GetCurrentDirectory() + \"RLN.zip\";\r\n\r\n### Source code / logs\r\n```\r\nSystem.Threading.ThreadAbortException\r\n  HResult=0x80131530\r\n  Message=System error.\r\n  Source=System.Private.CoreLib\r\n  StackTrace:\r\n   at System.RuntimeMethodHandle.InvokeMethod(Object target, Object[] arguments, Signature sig, Boolean constructor, Boolean wrapExceptions)\r\n   at System.Reflection.RuntimeMethodInfo.Invoke(Object obj, BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture)\r\n   at System.Reflection.MethodBase.Invoke(Object obj, Object[] parameters)\r\n   at Microsoft.ML.Runtime.ComponentCatalog.LoadableClassInfo.CreateInstanceCore(Object[] ctorArgs)\r\n   at Microsoft.ML.Runtime.ComponentCatalog.LoadableClassInfo.CreateInstance(IHostEnvironment env, Object args, Object[] extra)\r\n   at Microsoft.ML.Runtime.ComponentCatalog.TryCreateInstance[TRes](IHostEnvironment env, Type signatureType, TRes& result, String name, String options, Object[] extra)\r\n   at Microsoft.ML.Runtime.ComponentCatalog.TryCreateInstance[TRes,TSig](IHostEnvironment env, TRes& result, String name, String options, Object[] extra)\r\n   at Microsoft.ML.ModelLoadContext.TryLoadModelCore[TRes,TSig](IHostEnvironment env, TRes& result, Object[] extra)\r\n   at Microsoft.ML.ModelLoadContext.TryLoadModel[TRes,TSig](IHostEnvironment env, TRes& result, RepositoryReader rep, Entry ent, String dir, Object[] extra)\r\n   at Microsoft.ML.ModelLoadContext.LoadModel[TRes,TSig](IHostEnvironment env, TRes& result, RepositoryReader rep, Entry ent, String dir, Object[] extra)\r\n   at Microsoft.ML.ModelLoadContext.LoadModelOrNull[TRes,TSig](IHostEnvironment env, TRes& result, RepositoryReader rep, String dir, Object[] extra)\r\n   at Microsoft.ML.ModelLoadContext.LoadModel[TRes,TSig](IHostEnvironment env, TRes& result, RepositoryReader rep, String dir, Object[] extra)\r\n   at Microsoft.ML.ModelOperationsCatalog.Load(Stream stream, DataViewSchema& inputSchema)\r\n   at Microsoft.ML.ModelOperationsCatalog.Load(String filePath, DataViewSchema& inputSchema)\r\n   at ****.****.BaseProcessor.RLNModel.RLNModel.CreatePredictionEngine() in C:\\Users\\hoffm\\source\\repos\\****:****\\****.*****.BaseProcessor\\RLNModel.cs:line 29\r\n   at System.Lazy`1.ViaFactory(LazyThreadSafetyMode mode)\r\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\r\n```\r\n\r\nPlease paste or attach the code or logs or traces that would be helpful to diagnose the issue you are reporting.\r\n\r\n\r\n```\r\npublic class RLNModel\r\n    {\r\n        private static Lazy<PredictionEngine<ModelInput, ModelOutput>> PredictionEngine = new Lazy<PredictionEngine<ModelInput, ModelOutput>>(CreatePredictionEngine);\r\n\r\n        public static string _modelFile;\r\n\r\n        public static ModelOutput Predict(ModelInput input) \r\n        {\r\n           \r\n            ModelOutput result = PredictionEngine.Value.Predict(input); // Throws here ################\r\n            return result;\r\n        }\r\n\r\n        public static PredictionEngine<ModelInput, ModelOutput> CreatePredictionEngine()\r\n        {\r\n            MLContext mlContext = new MLContext();\r\n\r\n            string userTempPath = Path.GetTempPath();\r\n            string extractedModelsPath = userTempPath + \"****.****.BaseProcessor\\\\\"; \r\n            var fileDir = extractedModelsPath + \"****.****.BaseProcessor.RLN.zip\";\r\n            \r\n            ITransformer mlModel = mlContext.Model.Load(fileDir, out var modelInputSchema); # Line 29\r\n            var predEngine = mlContext.Model.CreatePredictionEngine<ModelInput, ModelOutput>(mlModel);\r\n            return predEngine;\r\n        }\r\n    }\r\n```\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5591","RelatedDescription":"Open issue \"System.Threading.ThreadAbortException: \"System error.\"\" (#5591)"},{"Id":"786699562","IsPullRequest":false,"CreatedAt":"2021-01-15T08:15:55","Actor":"DirkKramer","Number":"5590","RawContent":null,"Title":"How to set GpuOptions in ML.NET","State":"open","Body":"System information\r\n- Windows 10\r\n- .NET Framework 4.8\r\n- CUDA 10.1\r\n- Cudnn 7.6.5\r\n- Scisharp.Tensorflow.redist.window.gpu 2.3.1\r\n- Microsoft.ML 1.5.4 (vision, imageanalytics)\r\n\r\nHardware information\r\n-AMD Ryzen 7 4800\r\n-Nvidia GTX 1650\r\n\r\nTraining and predictions fail on GPU because of probably Memory out of range exception.   Everything works fine on my other laptop and desktop. but this laptop gives errors (maybe because of AMD??) . \r\n\r\n![image](https://user-images.githubusercontent.com/46933603/104698269-637b0800-5711-11eb-8f9a-f9077d7472a5.png)\r\n\r\n\r\n\r\n\r\n- **What did you do?**\r\n-I tried different combinations of CUDA-----cuDNN----Scisharp tensorflow redist gpu. (same error)\r\n-I tried rebooting.\r\n-I read on different issues on tensorflow.net and stackoverflow that you can set the gpuoptions. but I dont know how to do that in ML.NET.\r\n\r\n\r\n- **What do I expect?**\r\nCorrect training and predictions on GPU without exceptions.\r\n\r\n\r\n\r\n\r\n`var mlContext = new MLContext(seed: 1);`\r\n        \r\n            // 2. Load the initial full image-set into an IDataView and shuffle so it'll be better balanced\r\n            IEnumerable<ImageData> images = LoadImagesFromDirectory(folder: fullImagesetFolderPath, useFolderNameAsLabel: true);\r\n            IDataView fullImagesDataset = mlContext.Data.LoadFromEnumerable(images);\r\n            IDataView shuffledFullImageFilePathsDataset = mlContext.Data.ShuffleRows(fullImagesDataset);\r\n\r\n            // 3. Load Images with in-memory type within the IDataView and Transform Labels to Keys (Categorical)\r\n            IDataView shuffledFullImagesDataset = mlContext.Transforms.Conversion.\r\n                    MapValueToKey(outputColumnName: \"LabelAsKey\", inputColumnName: \"Label\", keyOrdinality: KeyOrdinality.ByValue)\r\n                .Append(mlContext.Transforms.LoadRawImageBytes(\r\n                                                outputColumnName: \"Image\",\r\n                                                imageFolder: fullImagesetFolderPath,\r\n                                                inputColumnName: \"ImagePath\"))\r\n                .Fit(shuffledFullImageFilePathsDataset)\r\n                .Transform(shuffledFullImageFilePathsDataset);\r\n\r\n            // 4. Split the data 80:20 into train and test sets, train and evaluate.\r\n            var trainTestData = mlContext.Data.TrainTestSplit(shuffledFullImagesDataset, testFraction: 0.2);\r\n            IDataView trainDataView = trainTestData.TrainSet;\r\n            IDataView testDataView = trainTestData.TestSet;\r\n\r\n            // 5. Define the model's training pipeline using DNN default values            \r\n            var pipeline = mlContext.MulticlassClassification.Trainers\r\n                    .ImageClassification(featureColumnName: \"Image\",\r\n                                            labelColumnName: \"LabelAsKey\",\r\n                                            validationSet: testDataView)\r\n                .Append(mlContext.Transforms.Conversion.MapKeyToValue(outputColumnName: \"PredictedLabel\",\r\n                                                                      inputColumnName: \"PredictedLabel\"));\r\n\r\n\r\n\r\n            // 6. Train/create the ML model       \r\n            ITransformer trainedModel = pipeline.Fit(trainDataView);`\r\n\r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5590","RelatedDescription":"Open issue \"How to set GpuOptions in ML.NET\" (#5590)"},{"Id":"779748188","IsPullRequest":true,"CreatedAt":"2021-01-15T05:02:04","Actor":"Lynx1820","Number":"5577","RawContent":null,"Title":"Onnx Export for ValueMapping estimator","State":"closed","Body":"LabelEncoder, the onnx operator used for this export, doesn't support mapping between the same types, which is allowed in ML.NET. This can be bypassed by casting to other types, except for String to String mappings. \r\n\r\n   \r\n","Url":"https://github.com/dotnet/machinelearning/pull/5577","RelatedDescription":"Closed or merged PR \"Onnx Export for ValueMapping estimator\" (#5577)"},{"Id":"785644572","IsPullRequest":false,"CreatedAt":"2021-01-14T04:39:21","Actor":"clthorre","Number":"5589","RawContent":null,"Title":"Install CLI tool using nuget. ","State":"open","Body":"Hello. I am wondering if there is a way to get the command line utilities while installing using nuget.\r\n\r\nCurrently I have a system where I clone the repo, and do something along these lines.\r\n```\r\nbash build.sh -release  \r\ndotnet publish -c Release --no-build  machinelearning/src/Microsoft.ML.Console --output mlnet\r\n```\r\n\r\nThen in the resulting folder I have the following files, MML.dll, MML.pdb, MML.xml, MML.runtimeconfig.json, MML.deps.json and many other dll files of the library.\r\n\r\nI use this by doing dotnet mlnet/MML.dll <my argument string>\r\n\r\nI am wondering if it is possible to use nuget to install the dotnet machine learning package and have the same files available?\r\nI have tried installing https://www.nuget.org/packages/Microsoft.ML/ and https://www.nuget.org/packages/mlnet/.\r\nThe first does not have the same MML.dll file I would use, and the second is a command line tool but has a totally different API.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5589","RelatedDescription":"Open issue \"Install CLI tool using nuget. \" (#5589)"},{"Id":"785597157","IsPullRequest":false,"CreatedAt":"2021-01-14T02:22:56","Actor":"mthalman","Number":"5588","RawContent":null,"Title":"Broken link for SqueezeNet","State":"open","Body":"\r\nThe link for SqueezeNet needs to be changed from https://github.com/onnx/models/tree/master/squeezenet to https://github.com/onnx/models/tree/master/vision/classification/squeezenet\r\n\r\n\r\n---\r\n#### Document Details\r\n\r\n⚠ *Do not edit this section. It is required for docs.microsoft.com ➟ GitHub issue linking.*\r\n\r\n* ID: 272720a2-7bf8-4e2c-743c-114ca1ce887b\r\n* Version Independent ID: 6c82d0d0-a8cc-868c-3d20-1a210658c8d2\r\n* Content: [ImageEstimatorsCatalog.ExtractPixels(TransformsCatalog, String, String, ImagePixelExtractingEstimator+ColorBits, ImagePixelExtractingEstimator+ColorsOrder, Boolean, Single, Single, Boolean) Method (Microsoft.ML)](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.imageestimatorscatalog.extractpixels?view=ml-dotnet)\r\n* Content Source: [dotnet/xml/Microsoft.ML/ImageEstimatorsCatalog.xml](https://github.com/dotnet/ml-api-docs/blob/live/dotnet/xml/Microsoft.ML/ImageEstimatorsCatalog.xml)\r\n* Product: **dotnet-ml-api**\r\n* GitHub Login: @natke\r\n* Microsoft Alias: **nakersha**","Url":"https://github.com/dotnet/machinelearning/issues/5588","RelatedDescription":"Open issue \"Broken link for SqueezeNet\" (#5588)"},{"Id":"781449346","IsPullRequest":false,"CreatedAt":"2021-01-13T23:47:42","Actor":"voges316","Number":"5580","RawContent":null,"Title":"Mlnet CLI tool 16.2 generated model fails when being loaded to create a prediction engine","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**: CentOS 7.9\r\n- **.NET Version (eg., dotnet --info)**: 3.1.404\r\n\r\n### Issue\r\nUpgrading from the mlnet cli tool from v0.15.1 to v16.2 I am unable to import a generated model using the C# api that is generated using the most recent mlnet cli tool on a yelp sentiment dataset.\r\n\r\n- **What did you do?**\r\nTried to import a model generated from mlnet cli v16.2 and predict an input.\r\n```\r\nITransformer predictionPipeline = mlContext.Model.Load(\"models/MLModelv16.2.zip\", out predictionPipelineSchema);\r\nPredictionEngine<ModelInput, ModelOutput> predictionEngine = mlContext.Model.CreatePredictionEngine<ModelInput, ModelOutput>(predictionPipeline);\r\nModelInput input = new ModelInput{ Col0 = \"Meh, food was cold.\" };\r\nvar result = predictionEngine.Predict(input);\r\n```\r\n\r\n- **What happened?**\r\n\r\nI get the following error\r\n```\r\nUnhandled exception. System.ArgumentOutOfRangeException: Could not find input column 'col1' (Parameter 'inputSchema')\r\n   at Microsoft.ML.Data.OneToOneTransformerBase.CheckInput(DataViewSchema inputSchema, Int32 col, Int32& srcCol)\r\n   at Microsoft.ML.Data.OneToOneTransformerBase.OneToOneMapperBase..ctor(IHost host, OneToOneTransformerBase parent, DataViewSchema inputSchema)\r\n   at Microsoft.ML.Transforms.ValueToKeyMappingTransformer.Mapper..ctor(ValueToKeyMappingTransformer parent, DataViewSchema inputSchema)\r\n   at Microsoft.ML.Transforms.ValueToKeyMappingTransformer.MakeRowMapper(DataViewSchema schema)\r\n   at Microsoft.ML.Data.RowToRowTransformerBase.Microsoft.ML.ITransformer.GetRowToRowMapper(DataViewSchema inputSchema)\r\n   at Microsoft.ML.Data.TransformerChain`1.Microsoft.ML.ITransformer.GetRowToRowMapper(DataViewSchema inputSchema)\r\n   at Microsoft.ML.Data.TransformerChain`1.Microsoft.ML.ITransformer.GetRowToRowMapper(DataViewSchema inputSchema)\r\n   at Microsoft.ML.Data.TransformerChain`1.Microsoft.ML.ITransformer.GetRowToRowMapper(DataViewSchema inputSchema)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MLConsoleApp.Program.Main(String[] args) in /home/devel/code/mlnet_example/MLConsoleApp/Program.cs:line 34\r\n```\r\n- **What did you expect?**\r\nUsing v0.15.1 of the mlnet cli tool I am able to train a model, and then in dotnet load the model, create a prediction engine and score a new input correctly.\r\nI expected to have the same behavior using the latest mlnet cli tool, but it doesn't work.\r\n\r\n### Source code / logs\r\n\r\nPrevious mlnet cli tool:\r\nMlnet --version => 0.15.28007.4\r\nYelp labelled datset from here http://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\r\n\r\nPrevious command to train a model on a dataset:\r\nmlnet auto-train --task binary-classification --dataset data/yelp_labelled.txt --label-column-index 1 --has-header false --max-exploration-time 30 --name YelpDemo\r\n\r\nNow I can load that model into a console app and run it fine. Showing the ModelInput.cs generated by the mlnet cli tool. I can just create another input class like this and use it to load the model, create a prediction engine, and score a new input.\r\n\r\n![image](https://user-images.githubusercontent.com/6192880/103916547-2dff6a80-50d2-11eb-9cf2-fb32dc363bcb.png)\r\n\r\nHowever, if I upgrade the mlnet cli tool it doesn't work\r\nmlnet –version => 16.2.0\r\nCommand to train a model:\r\nmlnet classification --dataset data/yelp_labelled.txt --has-header false --label-col 1 --train-time 30 --name YelpML16\r\nThe tool appears to pring a warning about a header being detected in the dataset, even though no header is used in this dataset.\r\n![image](https://user-images.githubusercontent.com/6192880/103916757-6b63f800-50d2-11eb-85dd-fc93df9b2c6c.png)\r\n\r\nWhen I try and import the model and create a prediction engine I encounter the error I pasted above, saying: \r\nUnhandled exception. System.ArgumentOutOfRangeException: Could not find input column 'col1' (Parameter 'inputSchema')\r\n\r\nDiving into the mlnet v16.2 classes, it appears the ModelInput.cs has changed. The second column is no longer a boolean Label, but a string Col1\r\n![image](https://user-images.githubusercontent.com/6192880/103917197-ee854e00-50d2-11eb-8539-897211a81e78.png)\r\n\r\nThis is different than the ModelInput.cs generated by v0.15.1, but if I change the modelinput class to match the mlnet cli output I still get an exception loading the model and creating the prediction engine.\r\n\r\n```\r\nUnhandled exception. System.InvalidOperationException: Can't bind the IDataView column 'PredictedLabel' of type 'String' to field or property 'Prediction' of type 'System.Boolean'.\r\n   at Microsoft.ML.Data.TypedCursorable`1..ctor(IHostEnvironment env, IDataView data, Boolean ignoreMissingColumns, InternalSchemaDefinition schemaDefn)\r\n   at Microsoft.ML.Data.TypedCursorable`1.Create(IHostEnvironment env, IDataView data, Boolean ignoreMissingColumns, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2.PredictionEngineCore(IHostEnvironment env, InputRow`1 inputRow, IRowToRowMapper mapper, Boolean ignoreMissingColumns, SchemaDefinition outputSchemaDefinition, Action& disposer, IRowReadableAs`1& outputRow)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MLConsoleApp.Program.Main(String[] args) in /home/devel/code/mlnet_example/MLConsoleApp/Program.cs:line 37\r\n\r\n```\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5580","RelatedDescription":"Closed issue \"Mlnet CLI tool 16.2 generated model fails when being loaded to create a prediction engine\" (#5580)"},{"Id":"785443500","IsPullRequest":false,"CreatedAt":"2021-01-13T20:46:04","Actor":"michaelgsharp","Number":"5587","RawContent":null,"Title":"Migrate to VSTest for all Unit Tests","State":"open","Body":"Based on the conversation in #5583 and this [comment](https://github.com/dotnet/machinelearning/pull/5583#issuecomment-759711391), we need to migrate all our unit testing over to VSTest to make debugging easier. This issue is being created to track that so we don't lose it.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5587","RelatedDescription":"Open issue \"Migrate to VSTest for all Unit Tests\" (#5587)"},{"Id":"783735050","IsPullRequest":true,"CreatedAt":"2021-01-13T20:43:46","Actor":"michaelgsharp","Number":"5583","RawContent":null,"Title":"Added in note in the documentation that the PredictionEngine is not thread safe.","State":"closed","Body":"Since we have had several questions about PredictionEngine and whether its thread safe (#5582 for example), I have just added a small comment to the PredictionEngine class docs saying its not thread safe.","Url":"https://github.com/dotnet/machinelearning/pull/5583","RelatedDescription":"Closed or merged PR \"Added in note in the documentation that the PredictionEngine is not thread safe.\" (#5583)"},{"Id":"783818389","IsPullRequest":true,"CreatedAt":"2021-01-13T05:59:13","Actor":"michaelgsharp","Number":"5584","RawContent":null,"Title":"Nuget.config url fix for roslyn compilers","State":"closed","Body":"The old `dotnet.myget.org` for `myget-roslyn` was removed. This PR updates the url to be the correct one, and then updates the versions to be the versions the new Nuget store has. Fixes the couple of test failures due to version updates as well.","Url":"https://github.com/dotnet/machinelearning/pull/5584","RelatedDescription":"Closed or merged PR \"Nuget.config url fix for roslyn compilers\" (#5584)"},{"Id":"784699625","IsPullRequest":false,"CreatedAt":"2021-01-13T00:50:53","Actor":"s-tory","Number":"5585","RawContent":null,"Title":"Why `RecursionLimit = 10` in `OnnxTransformer`?","State":"open","Body":"What is the purpose that `Google.Protobuf.CodedInputStream.RecursionLimit` is set `10` at the following code?\r\n\r\nhttps://github.com/dotnet/machinelearning/blob/2a6cf9d9c9655f9e7d8ab7332efb5e2a2b70ce7e/src/Microsoft.ML.OnnxTransformer/OnnxUtils.cs#L208\r\n\r\n----\r\nIt is set `100` by default in protocol-buffers C#-wrapper.\r\n\r\nhttps://github.com/protocolbuffers/protobuf/blob/10599e6c8dde8a9875258e03054a696d53cadebd/csharp/src/Google.Protobuf/CodedInputStream.cs#L83\r\n```\r\n        internal const int DefaultRecursionLimit = 100;\r\n```\r\n\r\n----\r\nI could not load some network / `.onnx` file by the following exception be caused by `RecursionLimit = 10` without the monkey-patched `Microsoft.ML.OnnxTransformer.dll`.\r\n```\r\nGoogle.Protobuf.InvalidProtocolBufferException: Protocol message had too many levels of nesting.  May be malicious.  Use CodedInputStream.SetRecursionLimit() to increase the depth limit.\r\n```\r\n\r\nI think that the networks are including `Inception` construction especially.\r\n(ex. [tensorflow/models`s Faster-RCNN-Inception-V2](https://github.com/tensorflow/models/blob/master/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py) converted)\r\n\r\nThank you for coding great tools!","Url":"https://github.com/dotnet/machinelearning/issues/5585","RelatedDescription":"Open issue \"Why `RecursionLimit = 10` in `OnnxTransformer`?\" (#5585)"},{"Id":"783642971","IsPullRequest":false,"CreatedAt":"2021-01-11T21:41:58","Actor":"SpeedyCraftah","Number":"5582","RawContent":null,"Title":"AutoML - Cannot multi-thread predictions","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**: Windows 10 Pro\r\n- **.NET Version (eg., dotnet --info)**: .NET Core 3.1\r\n\r\n### Issue\r\n\r\n- **What did you do?** I compiled a model with the visual studio model builder then attempted to predict multiple labels with multi-threading.\r\n- **What happened?** An exception gets thrown: An exception of type 'System.IndexOutOfRangeException' occurred in Microsoft.ML.Transforms.dll but was not handled in user code - Index was outside the bounds of the array.\r\n- **What did you expect?** Not throw an error and run the predictions in parallel.\r\n\r\n### Source code / logs\r\n\r\n```cs\r\nusing Ml_net_sentimentML.Model;\r\nusing System;\r\nusing System.Threading.Tasks;\r\n\r\nnamespace myMLApp\r\n{\r\n    class Program\r\n    {\r\n        public static void Main()\r\n        {\r\n            MainAsync().GetAwaiter().GetResult();\r\n        }\r\n\r\n        async static Task MainAsync()\r\n        {\r\n            string[] toPredict = { \"This is a great!\", \"This is bad\", \"It doesn't work\" };\r\n            Task<string>[] resultTasks = new Task<string>[toPredict.Length];\r\n\r\n            for (int i = 0; i < toPredict.Length; i++)\r\n            {\r\n                string review = toPredict[i];\r\n\r\n                resultTasks[i] = Task.Run(() => PredictAndParse(review));\r\n            }\r\n\r\n            string[] results = await Task.WhenAll(resultTasks);\r\n\r\n            for (int i = 0; i < results.Length; i++)\r\n            {\r\n                Console.WriteLine(results[i] + \"\\n\");\r\n            }\r\n        }\r\n\r\n        static string PredictAndParse(string review)\r\n        {\r\n            ModelInput input = new ModelInput()\r\n            {\r\n                Review = review\r\n            };\r\n\r\n            ModelOutput prediction = ConsumeModel.Predict(input);\r\n\r\n            string sentiment = prediction.Prediction == \"1\" ? \"Positive\" : \"Negative\";\r\n\r\n            return $\"Text: {input.Review}\\nSentiment: {sentiment}\\nScore: {prediction.Score[0]} | {prediction.Score[1]}\";\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n**Full exception**:\r\n```\r\nSystem.IndexOutOfRangeException\r\n  HResult=0x80131508\r\n  Message=Index was outside the bounds of the array.\r\n  Source=Microsoft.ML.Transforms\r\n  StackTrace:\r\n   at Microsoft.ML.Transforms.Text.TokenizingByCharactersTransformer.Mapper.<>c__DisplayClass15_0.<MakeGetterOne>b__0(VBuffer`1& dst)\r\n   at Microsoft.ML.Data.RowCursorUtils.<>c__DisplayClass11_0`2.<GetVecGetterAsCore>b__0(VBuffer`1& dst)\r\n   at Microsoft.ML.Transforms.Text.NgramExtractingTransformer.Mapper.<>c__DisplayClass11_0.<MakeGetter>b__2(VBuffer`1& dst)\r\n   at Microsoft.ML.Transforms.LpNormNormalizingTransformer.Mapper.<>c__DisplayClass8_0.<MakeGetter>b__5(VBuffer`1& dst)\r\n   at Microsoft.ML.Data.ColumnConcatenatingTransformer.Mapper.BoundColumn.<>c__DisplayClass20_0`1.<MakeGetter>b__0(VBuffer`1& dst)\r\n   at Microsoft.ML.Transforms.NormalizeTransform.AffineColumnFunction.Sng.ImplVec.<>c__DisplayClass5_0.<GetGetter>b__0(VBuffer`1& dst)\r\n   at Microsoft.ML.Data.SchemaBindablePredictorWrapperBase.<>c__DisplayClass18_0`2.<GetValueGetter>b__0(TDst& dst)\r\n   at Microsoft.ML.Data.PredictedLabelScorerBase.EnsureCachedPosition[TScore](Int64& cachedPosition, TScore& score, DataViewRow boundRow, ValueGetter`1 scoreGetter)\r\n   at Microsoft.ML.Data.MulticlassClassificationScorer.<>c__DisplayClass16_0.<GetPredictedLabelGetter>b__0(UInt32& dst)\r\n   at Microsoft.ML.Transforms.KeyToValueMappingTransformer.Mapper.KeyToValueMap`2.<>c__DisplayClass8_0.<GetMappingGetter>b__0(TValue& dst)\r\n   at Microsoft.ML.Data.TypedCursorable`1.TypedRowBase.<>c__DisplayClass9_0`2.<CreateConvertingActionSetter>b__0(TRow row)\r\n   at Microsoft.ML.Data.TypedCursorable`1.TypedRowBase.FillValues(TRow row)\r\n   at Microsoft.ML.Data.TypedCursorable`1.RowImplementation.FillValues(TRow row)\r\n   at Microsoft.ML.PredictionEngineBase`2.FillValues(TDst prediction)\r\n   at Microsoft.ML.PredictionEngine`2.Predict(TSrc example, TDst& prediction)\r\n   at Microsoft.ML.PredictionEngineBase`2.Predict(TSrc example)\r\n   at Ml_net_sentimentML.Model.ConsumeModel.Predict(ModelInput input) in C:\\Users\\USER\\source\\repos\\ml.net sentiment\\ml.net sentimentML.Model\\ConsumeModel.cs:line 20\r\n   at myMLApp.Program.PredictAndParse(String review) in C:\\Users\\USER\\source\\repos\\ml.net sentiment\\ml.net sentiment\\Program.cs:line 48\r\n   at myMLApp.Program.<>c__DisplayClass1_0.<MainAsync>b__0() in C:\\Users\\USER\\source\\repos\\ml.net sentiment\\ml.net sentiment\\Program.cs:line 26\r\n   at System.Threading.Tasks.Task`1.InnerInvoke()\r\n   at System.Threading.Tasks.Task.<>c.<.cctor>b__274_0(Object obj)\r\n   at System.Threading.ExecutionContext.RunFromThreadPoolDispatchLoop(Thread threadPoolThread, ExecutionContext executionContext, ContextCallback callback, Object state)```\r\n\r\n```\r\n\r\nI'm quite new to C# so I wouldn't be surprised if this was an issue on my end.\r\n\r\n### Note\r\n\r\nWhen not attempting to multi-thread while predicting, it works fine (iterating over the `toPredict` array and calling `ConsumeModel.Predict` on all of them separately), no exceptions get thrown other than the fact that it's slow since it runs inconcurrently.","Url":"https://github.com/dotnet/machinelearning/issues/5582","RelatedDescription":"Closed issue \"AutoML - Cannot multi-thread predictions\" (#5582)"},{"Id":"782579558","IsPullRequest":false,"CreatedAt":"2021-01-09T10:32:53","Actor":"CJX-nice","Number":"5581","RawContent":null,"Title":".net for  sqark and  ML","State":"open","Body":"### System information\r\nwindows 10\r\n.net core  3.1\r\nspark 3.0.1\r\n.net for  spark  1.0.0\r\n\r\n### Issue\r\n Learn    <  Sentiment analysis with .NET for Apache Spark and ML.NET>    in  https://docs.microsoft.com/zh-cn/dotnet/spark/tutorials/ml-sentiment-analysis\r\n\r\nI   create  my  project   and   copy     the  official code   ;\r\nbut   An error occurred\r\n **[Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.**\r\n\r\n\r\n### Source code / logs\r\n\r\n// This file was auto-generated by ML.NET Model Builder. \r\n**this is  my Code  :**\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing Microsoft.ML;\r\nusing Microsoft.ML.Data;\r\nusing Microsoft.Spark.Sql;\r\nusing MySparkAppML.Model;\r\n\r\n\r\nnamespace MySparkAppML.ConsoleApp\r\n{\r\n    public class Program\r\n    {\r\n        public static void Main(string[] args)\r\n        {\r\n            SparkSession spark = SparkSession\r\n             .Builder()\r\n             .AppName(\".NET for Apache Spark Sentiment Analysis\")\r\n             .GetOrCreate();\r\n            DataFrame df = spark .Read() .Option(\"header\", true).Option(\"inferSchema\", true) .Csv(\"yelptest.csv\");\r\n            df.Show();\r\n            Console.WriteLine(predict(\"aaa\"));\r\n            Console.WriteLine(predict(\"bbb\"));\r\n            spark.Udf() .Register<string, float>(\"MLudf\", predict);\r\n            df.CreateOrReplaceTempView(\"Reviews\");\r\n            DataFrame sqlDf = spark.Sql(\"SELECT ReviewText, MLudf(ReviewText) FROM Reviews\");\r\n            sqlDf.Show();\r\n            Console.ReadLine();\r\n        }\r\n        static float predict(string text)\r\n        {\r\n            MLContext mlContext = new MLContext();\r\n            ITransformer model = mlContext.Model.Load(\"MLModel.zip\", out var schema);\r\n            var Engine = mlContext.Model.CreatePredictionEngine<ModelInput, ModelOutput>(model);\r\n            return Engine.Predict(new ModelInput() { ReviewText = text }).Score;\r\n        }\r\n    }\r\n}\r\n\r\n\r\n\r\n**my  log** \r\n\r\n\r\nC:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\bin\\Debug\\netcoreapp3.1\\publish>spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local microsoft-spark-3-0_2.12-1.0.0.jar dotnet MySparkAppML.ConsoleApp.dll\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/bin/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\r\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\n21/01/09 18:29:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\n21/01/09 18:29:55 INFO DotnetRunner: Starting DotnetBackend with dotnet.\r\n21/01/09 18:29:55 INFO DotnetBackend: The number of DotnetBackend threads is set to 10.\r\n21/01/09 18:29:57 INFO DotnetRunner: Port number used by DotnetBackend is 53683\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.jars and value=file:/C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/microsoft-spark-3-0_2.12-1.0.0.jar to environment\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.app.name and value=org.apache.spark.deploy.dotnet.DotnetRunner to environment\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.submit.pyFiles and value= to environment\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.submit.deployMode and value=client to environment\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.master and value=local to environment\r\n[2021-01-09T10:29:57.7611666Z] [LAPTOP-8R49BD47] [Info] [ConfigurationService] Using port 53683 for connection.\r\n[2021-01-09T10:29:57.7701007Z] [LAPTOP-8R49BD47] [Info] [JvmBridge] JvMBridge port is 53683\r\n[2021-01-09T10:29:57.7750557Z] [LAPTOP-8R49BD47] [Info] [JvmBridge] The number of JVM backend thread is set to 10. The max number of concurrent sockets in JvmBridge is set to 7.\r\n21/01/09 18:29:58 INFO SparkContext: Running Spark version 3.0.1\r\n21/01/09 18:29:58 INFO ResourceUtils: ==============================================================\r\n21/01/09 18:29:58 INFO ResourceUtils: Resources for spark.driver:\r\n\r\n21/01/09 18:29:58 INFO ResourceUtils: ==============================================================\r\n21/01/09 18:29:58 INFO SparkContext: Submitted application: .NET for Apache Spark Sentiment Analysis\r\n21/01/09 18:29:58 INFO SecurityManager: Changing view acls to: YD\r\n21/01/09 18:29:58 INFO SecurityManager: Changing modify acls to: YD\r\n21/01/09 18:29:58 INFO SecurityManager: Changing view acls groups to:\r\n21/01/09 18:29:58 INFO SecurityManager: Changing modify acls groups to:\r\n21/01/09 18:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YD); groups with view permissions: Set(); users  with modify permissions: Set(YD); groups with modify permissions: Set()\r\n21/01/09 18:29:58 INFO Utils: Successfully started service 'sparkDriver' on port 53691.\r\n21/01/09 18:29:58 INFO SparkEnv: Registering MapOutputTracker\r\n21/01/09 18:29:58 INFO SparkEnv: Registering BlockManagerMaster\r\n21/01/09 18:29:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n21/01/09 18:29:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n21/01/09 18:29:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\r\n21/01/09 18:29:58 INFO DiskBlockManager: Created local directory at C:\\Users\\YD\\AppData\\Local\\Temp\\blockmgr-f20af9bd-7dc2-4ff4-9595-1d93a63b47b4\r\n21/01/09 18:29:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\r\n21/01/09 18:29:58 INFO SparkEnv: Registering OutputCommitCoordinator\r\n21/01/09 18:29:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\r\n21/01/09 18:29:58 INFO Utils: Successfully started service 'SparkUI' on port 4041.\r\n21/01/09 18:29:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://LAPTOP-8R49BD47:4041\r\n21/01/09 18:29:59 INFO SparkContext: Added JAR file:/C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/microsoft-spark-3-0_2.12-1.0.0.jar at spark://LAPTOP-8R49BD47:53691/jars/microsoft-spark-3-0_2.12-1.0.0.jar with timestamp 1610188199046\r\n21/01/09 18:29:59 INFO Executor: Starting executor ID driver on host LAPTOP-8R49BD47\r\n21/01/09 18:29:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53706.\r\n21/01/09 18:29:59 INFO NettyBlockTransferService: Server created on LAPTOP-8R49BD47:53706\r\n21/01/09 18:29:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n21/01/09 18:29:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-8R49BD47, 53706, None)\r\n21/01/09 18:29:59 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-8R49BD47:53706 with 434.4 MiB RAM, BlockManagerId(driver, LAPTOP-8R49BD47, 53706, None)\r\n21/01/09 18:29:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-8R49BD47, 53706, None)\r\n21/01/09 18:29:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-8R49BD47, 53706, None)\r\n21/01/09 18:29:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/spark-warehouse').\r\n21/01/09 18:30:00 INFO SharedState: Warehouse path is 'file:/C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/spark-warehouse'.\r\n21/01/09 18:30:00 INFO InMemoryFileIndex: It took 53 ms to list leaf files for 1 paths.\r\n21/01/09 18:30:01 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\r\n21/01/09 18:30:03 INFO FileSourceStrategy: Pruning directories with:\r\n21/01/09 18:30:03 INFO FileSourceStrategy: Pushed Filters:\r\n21/01/09 18:30:03 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\r\n21/01/09 18:30:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\r\n21/01/09 18:30:04 INFO CodeGenerator: Code generated in 307.516 ms\r\n21/01/09 18:30:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 171.3 KiB, free 434.2 MiB)\r\n21/01/09 18:30:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.2 MiB)\r\n21/01/09 18:30:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:04 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4225524 bytes, open cost is considered as scanning 4194304 bytes.\r\n21/01/09 18:30:05 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:05 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\r\n21/01/09 18:30:05 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\r\n21/01/09 18:30:05 INFO DAGScheduler: Parents of final stage: List()\r\n21/01/09 18:30:05 INFO DAGScheduler: Missing parents: List()\r\n21/01/09 18:30:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n21/01/09 18:30:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 434.2 MiB)\r\n21/01/09 18:30:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 434.2 MiB)\r\n21/01/09 18:30:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 5.3 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223\r\n21/01/09 18:30:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\r\n21/01/09 18:30:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\r\n21/01/09 18:30:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, LAPTOP-8R49BD47, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)\r\n21/01/09 18:30:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\r\n21/01/09 18:30:05 INFO Executor: Fetching spark://LAPTOP-8R49BD47:53691/jars/microsoft-spark-3-0_2.12-1.0.0.jar with timestamp 1610188199046\r\n21/01/09 18:30:05 INFO TransportClientFactory: Successfully created connection to LAPTOP-8R49BD47/192.168.11.81:53691 after 24 ms (0 ms spent in bootstraps)\r\n21/01/09 18:30:05 INFO Utils: Fetching spark://LAPTOP-8R49BD47:53691/jars/microsoft-spark-3-0_2.12-1.0.0.jar to C:\\Users\\YD\\AppData\\Local\\Temp\\spark-977ec410-19ef-4910-8e46-6e58ab7def14\\userFiles-7db99229-141a-40b4-ad4c-108082a93a68\\fetchFileTemp1545562991836894884.tmp\r\n21/01/09 18:30:05 INFO Executor: Adding file:/C:/Users/YD/AppData/Local/Temp/spark-977ec410-19ef-4910-8e46-6e58ab7def14/userFiles-7db99229-141a-40b4-ad4c-108082a93a68/microsoft-spark-3-0_2.12-1.0.0.jar to class loader\r\n21/01/09 18:30:05 INFO FileScanRDD: Reading File path: file:///C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/yelptest.csv, range: 0-31220, partition values: [empty row]\r\n21/01/09 18:30:05 INFO CodeGenerator: Code generated in 16.4157 ms\r\n21/01/09 18:30:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1583 bytes result sent to driver\r\n21/01/09 18:30:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 549 ms on LAPTOP-8R49BD47 (executor driver) (1/1)\r\n21/01/09 18:30:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool\r\n21/01/09 18:30:05 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.730 s\r\n21/01/09 18:30:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\r\n21/01/09 18:30:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\r\n21/01/09 18:30:05 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.786605 s\r\n21/01/09 18:30:05 INFO CodeGenerator: Code generated in 15.1734 ms\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 5.3 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Pruning directories with:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Pushed Filters:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Post-Scan Filters:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 171.3 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4225524 bytes, open cost is considered as scanning 4194304 bytes.\r\n21/01/09 18:30:06 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:06 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\r\n21/01/09 18:30:06 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\r\n21/01/09 18:30:06 INFO DAGScheduler: Parents of final stage: List()\r\n21/01/09 18:30:06 INFO DAGScheduler: Missing parents: List()\r\n21/01/09 18:30:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 7.7 KiB, free: 434.3 MiB)\r\n21/01/09 18:30:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223\r\n21/01/09 18:30:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\r\n21/01/09 18:30:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, LAPTOP-8R49BD47, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)\r\n21/01/09 18:30:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO FileScanRDD: Reading File path: file:///C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/yelptest.csv, range: 0-31220, partition values: [empty row]\r\n21/01/09 18:30:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1595 bytes result sent to driver\r\n21/01/09 18:30:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 158 ms on LAPTOP-8R49BD47 (executor driver) (1/1)\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool\r\n21/01/09 18:30:06 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.227 s\r\n21/01/09 18:30:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\r\n21/01/09 18:30:06 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0.244445 s\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Pruning directories with:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Pushed Filters:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Post-Scan Filters:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Output Data Schema: struct<ReviewText: string, Sentiment: string>\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 171.3 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Removed broadcast_3_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 7.7 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4225524 bytes, open cost is considered as scanning 4194304 bytes.\r\n21/01/09 18:30:06 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:06 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\r\n21/01/09 18:30:06 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\r\n21/01/09 18:30:06 INFO DAGScheduler: Parents of final stage: List()\r\n21/01/09 18:30:06 INFO DAGScheduler: Missing parents: List()\r\n21/01/09 18:30:06 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.8 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 4.9 KiB, free: 434.3 MiB)\r\n21/01/09 18:30:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223\r\n21/01/09 18:30:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\r\n21/01/09 18:30:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, LAPTOP-8R49BD47, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)\r\n21/01/09 18:30:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\r\n21/01/09 18:30:06 INFO FileScanRDD: Reading File path: file:///C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/yelptest.csv, range: 0-31220, partition values: [empty row]\r\n21/01/09 18:30:06 INFO CodeGenerator: Code generated in 22.741099 ms\r\n21/01/09 18:30:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2563 bytes result sent to driver\r\n21/01/09 18:30:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 107 ms on LAPTOP-8R49BD47 (executor driver) (1/1)\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool\r\n21/01/09 18:30:06 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.142 s\r\n21/01/09 18:30:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\r\n21/01/09 18:30:06 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.162733 s\r\n21/01/09 18:30:06 INFO CodeGenerator: Code generated in 26.014001 ms\r\n+--------------------+---------+\r\n|          ReviewText|Sentiment|\r\n+--------------------+---------+\r\n|Waitress was swee...|        1|\r\n|I also had to tas...|        1|\r\n|I'd rather eat ai...|        0|\r\n|Cant say enough g...|        1|\r\n|The ambiance was ...|        1|\r\n|The waitress and ...|        1|\r\n|I would not recom...|        0|\r\n|Overall I wasn't ...|        0|\r\n|My gyro was basic...|        0|\r\n|   Terrible service!|        0|\r\n|Thoroughly disapp...|        0|\r\n|I don't each much...|        1|\r\n|Give it a try, yo...|        1|\r\n|By far the BEST c...|        1|\r\n|Reasonably priced...|        1|\r\n|Everything was pe...|        1|\r\n|The food is very ...|        1|\r\n|it was a drive to...|        0|\r\n|At first glance i...|        1|\r\n|Anyway, I do not ...|        0|\r\n+--------------------+---------+\r\nonly showing top 20 rows\r\n\r\n0.61130136\r\n0.61130136\r\n[2021-01-09T10:30:08.1248483Z] [LAPTOP-8R49BD47] [Debug] [ConfigurationService] Using the environment variable to construct .NET worker path: C:\\bin\\Microsoft.Spark.Worker-1.0.0\\Microsoft.Spark.Worker.exe.\r\n21/01/09 18:30:08 INFO BlockManagerInfo: Removed broadcast_5_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 4.9 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:08 INFO FileSourceStrategy: Pruning directories with:\r\n21/01/09 18:30:08 INFO FileSourceStrategy: Pushed Filters:\r\n21/01/09 18:30:08 INFO FileSourceStrategy: Post-Scan Filters:\r\n21/01/09 18:30:08 INFO FileSourceStrategy: Output Data Schema: struct<ReviewText: string>\r\n21/01/09 18:30:08 INFO CodeGenerator: Code generated in 31.628 ms\r\n21/01/09 18:30:08 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 171.3 KiB, free 434.0 MiB)\r\n21/01/09 18:30:08 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.0 MiB)\r\n21/01/09 18:30:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:08 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4225524 bytes, open cost is considered as scanning 4194304 bytes.\r\n21/01/09 18:30:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:08 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:09 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\r\n21/01/09 18:30:09 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\r\n21/01/09 18:30:09 INFO DAGScheduler: Parents of final stage: List()\r\n21/01/09 18:30:09 INFO DAGScheduler: Missing parents: List()\r\n21/01/09 18:30:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n21/01/09 18:30:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 17.1 KiB, free 434.2 MiB)\r\n21/01/09 18:30:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 434.2 MiB)\r\n21/01/09 18:30:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 8.6 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223\r\n21/01/09 18:30:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\r\n21/01/09 18:30:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks\r\n21/01/09 18:30:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)\r\n21/01/09 18:30:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\r\n21/01/09 18:30:09 INFO CodeGenerator: Code generated in 23.0987 ms\r\nDotnetWorker PID:[22312] Args:[-m pyspark.worker] SparkVersion:[3.0.1]\r\n[2021-01-09T10:30:09.6927376Z] [LAPTOP-8R49BD47] [Info] [SimpleWorker] RunSimpleWorker() is starting with port = 53719.\r\n[2021-01-09T10:30:09.7797201Z] [LAPTOP-8R49BD47] [Info] [TaskRunner] [0] Starting with ReuseSocket[False].\r\n21/01/09 18:30:09 INFO FileScanRDD: Reading File path: file:///C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/yelptest.csv, range: 0-31220, partition values: [empty row]\r\n[2021-01-09T10:30:09.7980951Z] [LAPTOP-8R49BD47] [Info] [ConfigurationService] 'DOTNETBACKEND_PORT' environment variable is not set.\r\n[2021-01-09T10:30:09.7981653Z] [LAPTOP-8R49BD47] [Info] [ConfigurationService] Using port 5567 for connection.\r\n[2021-01-09T10:30:09.8053556Z] [LAPTOP-8R49BD47] [Info] [JvmBridge] JvMBridge port is 5567\r\n21/01/09 18:30:09 INFO CodeGenerator: Code generated in 19.9682 ms\r\n[2021-01-09T10:30:09.8144165Z] [LAPTOP-8R49BD47] [Info] [JvmBridge] The number of JVM backend thread is set to 10. The max number of concurrent sockets in JvmBridge is set to 7.\r\n[2021-01-09T10:30:10.6518123Z] [LAPTOP-8R49BD47] [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n[2021-01-09T10:30:10.6527954Z] [LAPTOP-8R49BD47] [Error] [TaskRunner] [0] Exiting with exception: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs21/01/09 18:30:10 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\r\norg.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n:line 154\r\n   at Microsoft.Spark.Worker.TaskRunner.Run() in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 66\r\n[2021-01-09T10:30:10.6537382Z] [LAPTOP-8R49BD47] [Info] [TaskRunner] [0] Finished running 0 task(s).\r\n[2021-01-09T10:30:10.6537724Z] [LAPTOP-8R49BD47] [Info] [SimpleWorker] RunSimpleWorker() finished successfully\r\n21/01/09 18:30:10 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver): org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\n21/01/09 18:30:10 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\r\n21/01/09 18:30:10 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool\r\n21/01/09 18:30:10 INFO TaskSchedulerImpl: Cancelling stage 3\r\n21/01/09 18:30:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled\r\n21/01/09 18:30:10 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) failed in 1.700 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver): org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\nDriver stacktrace:\r\n21/01/09 18:30:10 INFO DAGScheduler: Job 3 failed: showString at NativeMethodAccessorImpl.java:0, took 1.716029 s\r\n21/01/09 18:30:10 ERROR DotnetBackendHandler: Failed to execute 'showString' on 'org.apache.spark.sql.Dataset' with args=([Type=java.lang.Integer, Value: 20], [Type=java.lang.Integer, Value: 20], [Type=java.lang.Boolean, Value: false])\r\n[2021-01-09T10:30:10.7275550Z] [LAPTOP-8R49BD47] [Error] [JvmBridge] JVM method execution failed: Nonstatic method 'showString' failed for class '18' when called with 3 arguments ([Index=1, Type=Int32, Value=20], [Index=2, Type=Int32, Value=20], [Index=3, Type=Boolean, Value=False], )\r\n[2021-01-09T10:30:10.7276810Z] [LAPTOP-8R49BD47] [Error] [JvmBridge] org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver): org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\nDriver stacktrace:\r\n        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n        at scala.Option.foreach(Option.scala:407)\r\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n        at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n        at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n        at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n        at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n        at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n        at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n        at org.apache.spark.api.dotnet.DotnetBackendHandler.handleMethodCall(DotnetBackendHandler.scala:159)\r\n        at org.apache.spark.api.dotnet.DotnetBackendHandler.$anonfun$handleBackendRequest$2(DotnetBackendHandler.scala:99)\r\n        at org.apache.spark.api.dotnet.ThreadPool$$anon$1.run(ThreadPool.scala:34)\r\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r\n        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\nCaused by: org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        ... 3 more\r\n\r\n[2021-01-09T10:30:10.7801524Z] [LAPTOP-8R49BD47] [Exception] [JvmBridge] JVM method execution failed: Nonstatic method 'showString' failed for class '18' when called with 3 arguments ([Index=1, Type=Int32, Value=20], [Index=2, Type=Int32, Value=20], [Index=3, Type=Boolean, Value=False], )\r\n   at Microsoft.Spark.Interop.Ipc.JvmBridge.CallJavaMethod(Boolean isStatic, Object classNameOrJvmObjectReference, String methodName, Object[] args)\r\nUnhandled exception. System.Exception: JVM method execution failed: Nonstatic method 'showString' failed for class '18' when called with 3 arguments ([Index=1, Type=Int32, Value=20], [Index=2, Type=Int32, Value=20], [Index=3, Type=Boolean, Value=False], )\r\n ---> Microsoft.Spark.JvmException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver): org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\nDriver stacktrace:\r\n        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n        at scala.Option.foreach(Option.scala:407)\r\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n        at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n        at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n        at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n        at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n        at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n        at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n        at org.apache.spark.api.dotnet.DotnetBackendHandler.handleMethodCall(DotnetBackendHandler.scala:159)\r\n        at org.apache.spark.api.dotnet.DotnetBackendHandler.$anonfun$handleBackendRequest$2(DotnetBackendHandler.scala:99)\r\n        at org.apache.spark.api.dotnet.ThreadPool$$anon$1.run(ThreadPool.scala:34)\r\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r\n        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\nCaused by: org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        ... 3 more\r\n\r\n   --- End of inner exception stack trace ---\r\n   at Microsoft.Spark.Interop.Ipc.JvmBridge.CallJavaMethod(Boolean isStatic, Object classNameOrJvmObjectReference, String methodName, Object[] args)\r\n   at Microsoft.Spark.Interop.Ipc.JvmBridge.CallNonStaticJavaMethod(JvmObjectReference objectId, String methodName, Object[] args)\r\n   at Microsoft.Spark.Interop.Ipc.JvmObjectReference.Invoke(String methodName, Object[] args)\r\n   at Microsoft.Spark.Sql.DataFrame.Show(Int32 numRows, Int32 truncate, Boolean vertical)\r\n   at MySparkAppML.ConsoleApp.Program.Main(String[] args) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 28\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5581","RelatedDescription":"Open issue \".net for  sqark and  ML\" (#5581)"},{"Id":"779911512","IsPullRequest":false,"CreatedAt":"2021-01-08T01:37:34","Actor":"nnoradie","Number":"5578","RawContent":null,"Title":"Boundaries and anomalies not correct after implementing 3-sigma approach for < 12 data points","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**: \r\n- **.NET Version (eg., dotnet --info)**: \r\n![image](https://user-images.githubusercontent.com/69877427/103716200-4d35b500-4f77-11eb-9b10-41c9396b4dc1.png)\r\n\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\n    - Ran 3-sigma approach then passed the padded dataset to SR\r\n        1. Calculated a range of 3 standard deviations above and below the mean of the data\r\n        2. Removed any outliers from the sum and recalculated the sum and a new mean without the outliers (in this case, no points were detected as outliers)\r\n        3. Evenly added enough points to fill out the batch to at least 12 points (ex. if the original data had 8 points, we added 2 points of the new mean value to the beginning and another 2 points to the end of the data)\r\n        4. Passed that data (in zip below) to DetectEntireAnomalyBySrCnn\r\n\r\n- **What happened?**\r\n    - Even though we found no outliers, the data point at the end was detected as an anomaly \r\n    - The confidence band was very off, many points were outside of the band, but none were detected as anomalies\r\n    \r\n![image](https://user-images.githubusercontent.com/69877427/103722432-ca1b5b80-4f84-11eb-8a98-3396d36eee41.png)\r\n(One point in the csv is not present here (11/1/1997) it was filtered out in post-processing since it was an interpolated data point)\r\n\r\n- **What did you expect?**\r\n    - No anomalies to be detected in this case\r\n\r\n### Source code / logs\r\n**3sigma code changes**\r\nhttps://powerbi.visualstudio.com/AI/_git/AI/pullrequest/131502?_a=files&path=%2Fsrc%2Fextensions%2FMicrosoft.AI.Dax.Extensions%2FAnomalies%2FDetection%2FAnomalyDetector.cs\r\n\r\n**AD options**\r\n![image](https://user-images.githubusercontent.com/69877427/103720633-a5bd8000-4f80-11eb-928f-9d1a23ca1167.png)\r\n\r\n**Data**\r\n[3sigma_data.zip](https://github.com/dotnet/machinelearning/files/5773621/3sigma_data.zip)\r\n\r\nPlease paste or attach the code or logs or traces that would be helpful to diagnose the issue you are reporting.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5578","RelatedDescription":"Closed issue \"Boundaries and anomalies not correct after implementing 3-sigma approach for < 12 data points\" (#5578)"},{"Id":"781190132","IsPullRequest":true,"CreatedAt":"2021-01-07T10:03:58","Actor":"guinao","Number":"5579","RawContent":null,"Title":"Fix issue in SRCnnEntireAnomalyDetector","State":"open","Body":"We are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [ ] There's a descriptive title that will make sense to other developers some time from now. \r\n- [ ] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [ ] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [ ] You have included any necessary tests in the same PR.\r\n\r\n#5516 ","Url":"https://github.com/dotnet/machinelearning/pull/5579","RelatedDescription":"Open PR \"Fix issue in SRCnnEntireAnomalyDetector\" (#5579)"}],"ResultType":"GitHubIssue"}},"RunOn":"2021-01-30T05:30:32.9989487Z","RunDurationInMilliseconds":584}