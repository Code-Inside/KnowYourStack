{"Data":{"GitHub":{"Issues":[{"Id":"2301548458","IsPullRequest":true,"CreatedAt":"2024-05-17T14:22:18","Actor":"ericstj","Number":"7155","RawContent":null,"Title":"Remove Codeql.SourceRoot","State":"closed","Body":"This was resulting in issues with bug reports and paths as well as with CodeQL honoring our exception config for submodules.\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7155","RelatedDescription":"Closed or merged PR \"Remove Codeql.SourceRoot\" (#7155)"},{"Id":"2301544637","IsPullRequest":false,"CreatedAt":"2024-05-16T23:30:32","Actor":"Xan-Kun","Number":"7154","RawContent":null,"Title":"Still no o200k_base support","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: all\r\n - OS & Version: all\r\n - ML.NET Version: all\r\n - .NET Version: all\r\n\r\n**Describe the bug**\r\nNo way to tokenize gpt-4o strings!\r\n\r\n**To Reproduce**\r\nTokenize a string for gpt-4o\r\n\r\n**Expected behavior**\r\nThe most recent models are supported.\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7154","RelatedDescription":"Open issue \"Still no o200k_base support\" (#7154)"},{"Id":"2300399375","IsPullRequest":false,"CreatedAt":"2024-05-16T13:23:53","Actor":"robalexclark","Number":"7153","RawContent":null,"Title":"Schema mismatch for label column ': expected Boolean, got Single ","State":"open","Body":"I have an AutoML experiment as follows:\r\n```\r\n     List<QuoteModel> quoteModels = new List<QuoteModel>();\r\n\r\n   //...get data from db and load into quoteModels\r\n\r\n     MLContext mlContext = new MLContext();\r\n\r\n     IDataView dataView = mlContext.Data.LoadFromEnumerable(quoteModels);\r\n     TrainTestData splitDataView = mlContext.Data.TrainTestSplit(dataView, testFraction: 0.8);\r\n\r\n     SweepablePipeline pipeline = mlContext.Auto().Featurizer(\r\n         dataView)\r\n         .Append(mlContext.Auto().BinaryClassification(labelColumnName: \"LeadWon\", featureColumnName: \"PreviouslyFlooded\"));\r\n\r\n     AutoMLExperiment experiment = mlContext.Auto().CreateExperiment();\r\n\r\n     experiment\r\n         .SetPipeline(pipeline)\r\n         .SetBinaryClassificationMetric(BinaryClassificationMetric.Accuracy, labelColumn: \"LeadWon\")\r\n         .SetTrainingTimeInSeconds(60)\r\n         .SetDataset(splitDataView);\r\n\r\n     TrialResult experimentResults = await experiment.RunAsync();\r\n```\r\n\r\nThe QuoteModel class is:\r\n\r\n```\r\n\r\npublic class QuoteModel\r\n{\r\n    public string PreviouslyFlooded { get; set; }\r\n\r\n    public Boolean LeadWon { get; set; }\r\n}\r\n\r\n```\r\n\r\nWhen I run the experiment it gives the error:\r\n\r\nSystem.ArgumentOutOfRangeException: 'Schema mismatch for label column 'LeadWon': expected Boolean, got Single \r\n\r\nIf you look at the QuoteModel class, the label column LeadWon is definately a boolean!!\r\n\r\nAny ideas why/how it can give this seemingly impossible error?\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7153","RelatedDescription":"Open issue \"Schema mismatch for label column ': expected Boolean, got Single \" (#7153)"},{"Id":"2299037834","IsPullRequest":false,"CreatedAt":"2024-05-15T23:43:29","Actor":"lucaspastorduran","Number":"7152","RawContent":null,"Title":"DataFrame.LoadCsv() Could not load file or assembly 'System.Runtime.CompilerServices.Unsafe, Version=4.0.4.1'","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: Windows 10 Enterprise\r\n - ML.NET Version: Microsoft.Data.Analysis 0.21.1\r\n - .NET Version: .Net Framework 4.8 (NET SDK 6.0.417)\r\n\r\n**Describe the bug**\r\nUsing the method DataFrame Data = DataFrame.LoadCsv(\"myFIlePath\") returns runtime exception:\r\n\"Could not load file or assembly 'System.Runtime.CompilerServices.Unsafe, Version=4.0.4.1, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' or one of its dependencies. The located assembly's manifest definition does not match the assembly reference. (Exception from HRESULT: 0x80131040)\" \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Create a DataFrame object from CSV file: DataFrame Data = DataFrame.LoadCsv(\"myFIlePath\")\r\n\r\n**Expected behavior**\r\nIt should parse the csv data into the DataFrame object.\r\n\r\n**Screenshots, Code, Sample Projects**\r\n![image](https://github.com/dotnet/machinelearning/assets/36445004/ee27bd6b-b99b-4aef-ad2d-3284a80d155a)\r\n\r\n**Additional context**\r\nIt is looking for the System.Runtime.CompilerServices.Unsafe, Version=4.0.4.1, which is contained in the nuget package 'System.Runtime.CompilerServices.Unsafe 4.5.3'.\r\nHowever, the ML version 0.21.1 requires to install System.Runtime.CompilerServices.Unsafe with version >= 6.0 (which has a higer version than the one is looking for).\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7152","RelatedDescription":"Open issue \"DataFrame.LoadCsv() Could not load file or assembly 'System.Runtime.CompilerServices.Unsafe, Version=4.0.4.1'\" (#7152)"},{"Id":"2292807929","IsPullRequest":true,"CreatedAt":"2024-05-13T16:54:01","Actor":"dotnet-maestro[bot]","Number":"7151","RawContent":null,"Title":"[main] Update dependencies from dotnet/arcade","State":"closed","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: c692823c-b896-437f-4f57-08dc434cc8f6\r\n- **Build**: 20240510.2\r\n- **Date Produced**: May 10, 2024 5:18:03 PM UTC\r\n- **Commit**: 480401b003bfd2eb989c315da5d6b99ad13a968c\r\n- **Branch**: refs/heads/main\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.SignTool**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.XliffTasks**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n\r\n[1]: https://github.com/dotnet/arcade/compare/020255bcf7...480401b003\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7151","RelatedDescription":"Closed or merged PR \"[main] Update dependencies from dotnet/arcade\" (#7151)"},{"Id":"2290412166","IsPullRequest":true,"CreatedAt":"2024-05-13T16:53:31","Actor":"ericstj","Number":"7150","RawContent":null,"Title":"Fix iterator type so that it matches boundary condition type","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/7150","RelatedDescription":"Closed or merged PR \"Fix iterator type so that it matches boundary condition type\" (#7150)"},{"Id":"2256524274","IsPullRequest":true,"CreatedAt":"2024-05-10T20:39:26","Actor":"dotnet-maestro[bot]","Number":"7138","RawContent":null,"Title":"[main] Update dependencies from dotnet/arcade","State":"closed","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: c692823c-b896-437f-4f57-08dc434cc8f6\r\n- **Build**: 20240503.1\r\n- **Date Produced**: May 3, 2024 9:02:59 AM UTC\r\n- **Commit**: 020255bcf7d0b8beed7de05338d97396982ae527\r\n- **Branch**: refs/heads/main\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.SignTool**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.XliffTasks**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n\r\n[4]: https://github.com/dotnet/arcade/compare/87b015b938...020255bcf7\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7138","RelatedDescription":"Closed or merged PR \"[main] Update dependencies from dotnet/arcade\" (#7138)"},{"Id":"2288110461","IsPullRequest":true,"CreatedAt":"2024-05-10T15:14:43","Actor":"directhex","Number":"7149","RawContent":null,"Title":"Try enabling TSA scan during build","State":"closed","Body":"The method for TSA scanning has changed over time, this ought to do the trick","Url":"https://github.com/dotnet/machinelearning/pull/7149","RelatedDescription":"Closed or merged PR \"Try enabling TSA scan during build\" (#7149)"},{"Id":"2288024884","IsPullRequest":false,"CreatedAt":"2024-05-09T16:16:09","Actor":"winscripter","Number":"7148","RawContent":null,"Title":"Is it possible to use ML.NET for image processing (such as remove background)?","State":"open","Body":"Hello,\r\n\r\nDoes ML.NET support image processing, such as removing background or\r\nmaking a specific item different color, using a set of images? If so, is there\r\na documented example for image processing with ML.NET?\r\n\r\nThanks.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7148","RelatedDescription":"Open issue \"Is it possible to use ML.NET for image processing (such as remove background)?\" (#7148)"},{"Id":"2279025739","IsPullRequest":true,"CreatedAt":"2024-05-04T14:34:46","Actor":"ravibaghel","Number":"7147","RawContent":null,"Title":"Added error handling, removed unwanted null check and enhanced readability","State":"open","Body":"The most significant changes include the removal of the null check for \"args\" in the \"Main\" method, the addition of a \"try-catch\" block to handle exceptions during the execution of the \"sample\" method, and the modification of the final console output line to use string interpolation for better readability.\r\n\r\n1. The null check for \"args\" in the \"Main\" method has been removed, indicating that \"args\" is always expected to be an array, even if it's an empty one. This change simplifies the code and makes the assumption about the nature of \"args\" more explicit.\r\n\r\n2. A \"try-catch\" block has been added around the invocation of the \"sample\" method and the increment of \"samples\". This change improves the robustness of the code by handling potential exceptions that might occur during the execution of the \"sample\" method. If an exception is thrown, it is caught and an error message is printed to the console, providing useful information about the error.\r\n\r\n3. The final console output line has been updated to use string interpolation to display the number of samples that ran without any exception. This change enhances the readability of the code by using a more modern and readable way to format strings in C#.\r\n\r\nWe are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [ ] There's a descriptive title that will make sense to other developers some time from now. \r\n- [ ] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [ ] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [ ] You have included any necessary tests in the same PR.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7147","RelatedDescription":"Open PR \"Added error handling, removed unwanted null check and enhanced readability\" (#7147)"},{"Id":"2257816675","IsPullRequest":true,"CreatedAt":"2024-05-02T18:58:56","Actor":"tarekgh","Number":"7139","RawContent":null,"Title":"Introducing CodeGen Tokenizer","State":"closed","Body":"This change is implementing the [CodeGen](https://huggingface.co/Salesforce/codegen-350M-mono/tree/main) which also support the [Phi-2](https://huggingface.co/microsoft/phi-2/tree/main) tokenizer.","Url":"https://github.com/dotnet/machinelearning/pull/7139","RelatedDescription":"Closed or merged PR \"Introducing CodeGen Tokenizer\" (#7139)"},{"Id":"2273370726","IsPullRequest":false,"CreatedAt":"2024-05-01T11:40:39","Actor":"superichmann","Number":"7146","RawContent":null,"Title":"Modify IDataView in AutoML Experiment After Transform and Before Evaluate","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nno\r\n\r\n**Describe the solution you'd like**\r\nAdd the option to modify the idataview (such as in preFeaturizer) but a \"postFeaturizer\" which will transform the idataview after the Transform has occurred on it inside the experiment and before the evaluation metrics are calculated.\r\n\r\n**Describe alternatives you've considered**\r\nCreate my own trial runner, if possible and this feature is not planned in automl please provide me with a start code :]\r\n\r\n**Additional context**\r\nSome use cases require alteration of the idataview based on the Score column which is not present before Transform is called.\r\nAnother solution would be to add the possibility to call a custom evaluate function based on LINQ","Url":"https://github.com/dotnet/machinelearning/issues/7146","RelatedDescription":"Open issue \"Modify IDataView in AutoML Experiment After Transform and Before Evaluate\" (#7146)"},{"Id":"2273166524","IsPullRequest":false,"CreatedAt":"2024-05-01T08:47:48","Actor":"sportbilly21","Number":"7145","RawContent":null,"Title":"Dll version of Microsoft.ML.OnnxRuntime.dll is 0.0.0.0","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: either Windows 10 or 11 \r\n - ML.NET Version: versions 1.7 and 3.0\r\n - .NET Version: .netframework 4.7\r\n\r\n**Describe the bug**\r\nAfter building the solution the version of the Microsoft.ML.OnnxRuntime.dll is 0.0.0.0\r\nThis is not causing any issues if you are running the application through the Visual Studio.\r\nBut we create an installer with Wix for installation in production PC\r\nWhen you install the software in Windows, the OS and wix due to the version of the dll being zero, they think the dll is corrupted and the installation cannot continue as the installer tries to recover the corrupted dll. \r\n\r\nWe have a work around by changing the version of the above dll but from my understanding dlls should not have 0 as a version\r\n\r\n\r\n\r\n\r\n![image](https://github.com/dotnet/machinelearning/assets/60097348/7e596786-3ff8-4750-acf1-51f2a168f3e9)\r\n\r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7145","RelatedDescription":"Open issue \"Dll version of Microsoft.ML.OnnxRuntime.dll is 0.0.0.0\" (#7145)"},{"Id":"2267992526","IsPullRequest":false,"CreatedAt":"2024-04-29T02:12:59","Actor":"tarekgh","Number":"7144","RawContent":null,"Title":"Tokenizers Library Design","State":"open","Body":"LLM tokenizers are a crucial component in Large Language Models (LLMs) like GPT-3 or BERT. They are responsible for the tokenization process, which involves breaking down natural language text into smaller, manageable pieces called tokens. These tokens can be words, characters, sub-words, numbers, or symbols, and they allow the LLM to process and understand the text.\r\n\r\nThis issue presents the APIs proposed for the Microsoft.ML.Tokenizers library, intended for design review. The design introduces an abstract class named `Tokenizer`, which defines the primary interfaces for all supported tokenizers. Additionally, the Tokenizer class includes a factory method for creating various types of tokenizers.\r\n\r\nThe Tokenizer can be optionally configured with normalizers, which are used to normalize the text before processing it. Normalization can take various forms such as uppercasing, lowercasing, [Unicode Normalization](https://www.unicode.org/reports/tr15/), and removing or inserting specific characters from the input text. The normalization feature is optional for the tokenizer, and it is left to the discretion of either the tokenizer or the user to decide whether to utilize any normalizers.\r\n\r\nPre-tokenization is an additional component that the tokenizer can be configured with, aimed at splitting the input text into smaller units prior to processing. While pre-tokenization is also an optional feature, it is commonly utilized in most tokenizers. Many pre-tokenizers employ regex for this purpose.\r\n\r\nThe typical sequence of operations for the Tokenizer involves:\r\n\r\n- Normalizing the input text if a normalizer is configured.\r\n- Pre-tokenizing the input or normalized text to segment it into smaller units.\r\n- Encoding each unit of text, potentially dividing it into smaller tokens and generating string tokens, IDs for the tokens, and/or offsets that map each token to a portion of the input or normalized text.\r\n\r\nTokenizers offer the following functionalities:\r\n\r\n- Encoding the input text into IDs, which can be utilized as input for Language Models. This operation is referred to as `EncodeToIds` in the proposed design.\r\n- Counting the tokens within the input text, aiding in calculating the quota allowed for processing at any given time. This operation is named `CountTokens` in the proposed design.\r\n- Full encoding, providing detailed results such as string tokens, IDs, and offsets mapping the tokens to parts of the input text. This operation is labeled as `Encode` in the proposed design.\r\n- Given a maximum token count, the tokenizer can determine how far into the input text tokens can be produced, either from the beginning or the end. These operations are denoted as `IndexOfTokenCount` and `LastIndexOfTokenCount`.\r\n- Decoding the generated IDs back into text. This operation is named `Decode` in the proposed design.\r\n- Establishing mappings between string tokens and IDs. These operations are termed `MapTokenToId` and `MapIdToToken` in the proposed design.\r\n\r\nTokenizers typically rely on vocabulary files, which are provided to the tokenizer during instantiation. Users commonly pass these vocabularies as either a file or a stream to the tokenizer constructor. Vocabulary files can vary in format, such as JSON, plain text, protobuf, and more. Each tokenizer determines the specific formats of files it can be instantiated with.\r\n\r\n# Usage Example:\r\n\r\n### Create BPE tokenizer using the constructor\r\n\r\n```C#\r\n    Tokenizer tokenizer = new Bpe(vocabStream: vocabStream, , mergesStream: mergesStream, normalizer: null, preTokenizer: WhiteSpace.Instance);\r\n```\r\n\r\n### Create Tiktoken tokenizer using factory method:\r\n\r\n```C#\r\n    Dictionary<string, int> specialTokens = new Dictionary<string, int> { { IMStart, 100264}, { IMEnd, 100265}, };\r\n    Tokenizer tokenizer = Tokenizer.CreateTiktokenForModel(\"gpt-4\", specialTokens);\r\n```\r\n\r\n### Encode to Ids:\r\n\r\n```C#\r\n    IReadOnlyList<int> encoded = tokenizer.EncodeToIds(\"Hello World\");\r\n```\r\n\r\n### Count Tokens\r\n\r\n```C#\r\n    int idsCount = tokenizer.CountTokens(\"Hello World\");\r\n```\r\n\r\n### Ful Encoding:\r\n\r\n```C#\r\n    // APIs return any information related to the input or normalized text will usually out normalizedString which can be null if there is no normalization performed.\r\n    // Token contain the string token, the token ID, and the offset of the token mapped to the input or normalized text.\r\n    IReadOnlyList<Token> result = tokenizer.Encode(text, out string? normalizedString);\r\n```\r\n\r\n### Count tokens up to max token count:\r\n\r\n```C#\r\n    int length = tokenizer.IndexOfTokenCount(text, maxTokenCount: 10, out string? normalizedString, out int tokenCount);\r\n    \r\n    int index = tokenizer.LastIndexOfTokenCount(text, maxTokenCount: 3, out normalizedString, out tokenCount)\r\n```\r\n\r\n### Decoding Ids back to string\r\n\r\n```C#\r\nstring decodedText = tokenizer.Decode(idsArray);\r\n```\r\n\r\n### Map string token to Id and vice versa\r\n\r\n```C#\r\nint? id = tokenizer.MapTokenToId(\"Hello\");\r\n\r\nstring? token = MapIdToToken(tokenId);\r\n```\r\n\r\n# Proposal:\r\n\r\n### Namespace\r\n\r\n```C#\r\nnamespace Microsoft.ML.Tokenizers\r\n```\r\n\r\n### Tokenizer Abstraction\r\n\r\n```C#\r\n    public abstract partial class Tokenizer\r\n    {\r\n        protected Tokenizer() { }\r\n\r\n        public virtual Normalizer? Normalizer { get { throw null; } }\r\n\r\n        public virtual PreTokenizer? PreTokenizer { get { throw null; } }\r\n\r\n        public virtual IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedText, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedText, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual int LastIndexOfTokenCount(string text, int maxTokenCount, out string? processedText, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? processedText, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual string? Decode(IEnumerable<int> ids) { throw null; }\r\n\r\n        public virtual int? MapTokenToId(string token) { throw null; }\r\n        public abstract int? MapTokenToId(ReadOnlySpan<char> token);\r\n\r\n        public abstract string? MapIdToToken(int? id);\r\n\r\n       //\r\n       // Factory methods\r\n       // \r\n\r\n        public static Task<Tokenizer> CreateTiktokenAsync(Stream vocabStream, PreTokenizer? preTokenizer, Normalizer? normalizer, IReadOnlyDictionary<string, int> specialTokens = null, \r\n                                                                                                  int cacheSize = 8192, Threading.CancellationToken cancellationToken = null) { throw null; }\r\n\r\n        public static Task<Tokenizer> CreateTiktokenAsync(string vocabFilePath, PreTokenizer? preTokenizer, Normalizer? normalizer, IReadOnlyDictionary<string, int> specialTokensEncoder = null, \r\n                                                                                                  int cacheSize = 8192, Threading.CancellationToken cancellationToken = null) { throw null; }\r\n\r\n        public static Tokenizer CreateTiktokenForEncoding(string encodingName, IReadOnlyDictionary<string, int> extraSpecialTokens = null, Normalizer? normalizer = null) { throw null; }\r\n\r\n        public static Tokenizer CreateTiktokenForModel(string modelName, IReadOnlyDictionary<string, int> extraSpecialTokens = null, Normalizer? normalizer = null) { throw null; }\r\n\r\n        public static Tokenizer CreateTiktokenForModel(string modelName, Stream vocabStream, IReadOnlyDictionary<string, int> extraSpecialTokens = null, \r\n                                                                                                    int cacheSize = 8192, Normalizer? normalizer = null) { throw null; }\r\n\r\n        public static Task<Tokenizer> CreateTiktokenForModelAsync(string modelName, Stream vocabStream, IReadOnlyDictionary<string, int> extraSpecialTokens = null, \r\n                                                                                                   int cacheSize = 8192, Normalizer? normalizer = null, Threading.CancellationToken cancellationToken = null) { throw null; }\r\n\r\n        public static Tokenizer CreateLlama(Stream modelStream, bool addBeginOfSentence = true, bool addEndOfSentence = false) { throw null; }\r\n\r\n        public static Tokenizer CreateCodeGen(Stream vocabStream, Stream mergesStream, bool addPrefixSpace = false, bool addBeginOfSentence = false, bool addEndOfSentence = false) { throw null; }\r\n\r\n        public static Tokenizer CreatePhi2(Stream vocabStream, Stream mergesStream, bool addPrefixSpace = false, bool addBeginOfSentence = false, bool addEndOfSentence = false) { throw null; }\r\n    }\r\n```\r\n\r\n### Normalization abstraction \r\n\r\n```C#\r\n    public abstract partial class Normalizer\r\n    {\r\n        protected Normalizer() { }\r\n\r\n        public abstract string Normalize(string original);\r\n        public abstract string Normalize(ReadOnlySpan<char> original);\r\n    }\r\n\r\n```\r\n\r\n### Pre-tokenization abstraction \r\n\r\n```C#\r\n    public abstract partial class PreTokenizer\r\n    {\r\n        protected PreTokenizer() { }\r\n\r\n        public abstract IEnumerable<(int, int)> PreTokenize(string text);\r\n        public abstract IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text);\r\n    }\r\n```\r\n\r\n### Token class \r\n\r\n```C#\r\n   // returned from Tokenizer.Encode(...)\r\n   \r\n    public readonly struct Token\r\n    {\r\n        public Token(int id, string value, (int, int) offset) { }\r\n\r\n        public int Id { get { throw null; } }\r\n\r\n        public (int Index, int Length) Offset { get { throw null; } }\r\n\r\n        public string Value { get { throw null; } }\r\n    }\r\n```\r\n\r\n### Concrete Normalizers \r\n\r\n```C#\r\n    public sealed partial class LowerCaseNormalizer : Normalizer\r\n    {\r\n        public override string Normalize(ReadOnlySpan<char> original) { throw null; }\r\n        public override string Normalize(string original) { throw null; }\r\n    }\r\n\r\n    public sealed partial class UpperCaseNormalizer : Normalizer\r\n    {\r\n        public override string Normalize(ReadOnlySpan<char> original) { throw null; }\r\n\r\n        public override string Normalize(string original) { throw null; }\r\n    }\r\n    \r\n    public sealed partial class SentencePieceNormalizer : Normalizer\r\n    {\r\n        public SentencePieceNormalizer(bool removeExtraWhiteSpaces, bool addDummyPrefix, bool escapeWhiteSpaces, bool treatWhitespaceAsSuffix) { }\r\n        public bool AddDummyPrefix { get { throw null; } }\r\n        public bool EscapeWhiteSpaces { get { throw null; } }\r\n        public bool RemoveExtraWhiteSpaces { get { throw null; } }\r\n        public bool TreatWhitespaceAsSuffix { get { throw null; } }\r\n\r\n        public override string Normalize(ReadOnlySpan<char> original) { throw null; }\r\n        public override string Normalize(string original) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Pre-tokenizers\r\n\r\n```C#\r\n    public sealed partial class TiktokenPreTokenizer : PreTokenizer\r\n    {\r\n        public TiktokenPreTokenizer(Text.RegularExpressions.Regex regex, IReadOnlyDictionary<string, int> specialTokensEncoder) { }\r\n\r\n        public override IEnumerable<(int, int)> PreTokenize(string text) { throw null; }\r\n        public override IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text) { throw null; }\r\n    }\r\n\r\n    public sealed partial class WhiteSpace : PreTokenizer\r\n    {\r\n        public static WhiteSpace Instance { get { throw null; } }\r\n\r\n        public override IEnumerable<(int, int)> PreTokenize(string text) { throw null; }\r\n        public override IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text) { throw null; }\r\n    }\r\n\r\n    public sealed partial class RobertaPreTokenizer : PreTokenizer\r\n    {\r\n        public static RobertaPreTokenizer Instance { get { throw null; } }\r\n\r\n        public override IEnumerable<(int, int)> PreTokenize(string text) { throw null; }\r\n        public override IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - Bpe\r\n\r\n```C#\r\n    public sealed partial class Bpe : Tokenizer\r\n    {\r\n        public Bpe(string vocabFile, string? mergesFile, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, string? unknownToken = null, \r\n                           string? continuingSubwordPrefix = null, string? endOfWordSuffix = null, bool? fuseUnknownTokens = false) { }\r\n\r\n        public Bpe(Stream vocabStream, Stream? mergesStream, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, string? unknownToken = null, \r\n                          string? continuingSubwordPrefix = null, string? endOfWordSuffix = null, bool? fuseUnknownTokens = false) { }\r\n\r\n        public string? ContinuingSubwordPrefix { get { throw null; } }\r\n\r\n        public string? EndOfWordSuffix { get { throw null; } }\r\n\r\n        public bool? FuseUnknownTokens { get { throw null; } }\r\n\r\n        public string? UnknownToken { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public string? Decode(IEnumerable<int> ids, bool considerSpecialTokens) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - Tiktoken \r\n\r\n```C#\r\n    public sealed partial class Tiktoken : Tokenizer\r\n    {\r\n        public Tiktoken(Stream vocabStream, PreTokenizer? preTokenizer, IReadOnlyDictionary<string, int> specialTokens = null, Normalizer? normalizer = null, int? cacheSize = 8192) { }\r\n\r\n        public Tiktoken(string vocabFilePath, PreTokenizer? preTokenizer, IReadOnlyDictionary<string, int> specialTokens = null, Normalizer? normalizer = null, int? cacheSize = 8192) { }\r\n\r\n        public IReadOnlyDictionary<int, ReadOnlyMemory<Byte>> Decoder { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<ReadOnlyMemory<Byte>, int> Encoder { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> SpecialTokens { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - EnglishRoberta\r\n\r\n```C#\r\n    public sealed partial class EnglishRoberta : Tokenizer\r\n    {\r\n        public EnglishRoberta(Stream vocabularyStream, Stream mergeStream, Stream highestOccurrenceMappingStream, PreTokenizer? preTokenizer, Normalizer? normalizer, bool filterUnsupportedChars, bool disposeStream) { }\r\n\r\n        public EnglishRoberta(Stream vocabularyStream, Stream mergeStream, Stream highestOccurrenceMappingStream, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, bool filterUnsupportedChars = true) { }\r\n\r\n        public EnglishRoberta(string vocabularyPath, string mergePath, string highestOccurrenceMappingPath, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, bool filterUnsupportedChars = true) { }\r\n\r\n        public bool FilterUnsupportedChars { get { throw null; } }\r\n\r\n        public int PadIndex { get { throw null; } }\r\n\r\n        public int SymbolsCount { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public int AddMaskSymbol(string mask = \"<mask>\") { throw null; }\r\n\r\n        public IReadOnlyList<int> ConvertIdsToOccurrenceRanks(IReadOnlyList<int> ids) { throw null; }\r\n\r\n        public IReadOnlyList<int> ConvertIdsToOccurrenceValues(IReadOnlyList<int> ids) { throw null; }\r\n\r\n        public IReadOnlyList<int> ConvertOccurrenceRanksToIds(IReadOnlyList<int> ranks) { throw null; }\r\n\r\n        public bool IsSupportedChar(char ch) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - CodeGen \r\n\r\n```C#\r\n    public sealed partial class CodeGen : Tokenizer\r\n    {\r\n        public CodeGen(string vocabularyPath, string mergePath, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, IReadOnlyDictionary<string, int> addedTokens = null, \r\n                                     bool? addPrefixSpace = false, bool? addBeginningOfSentence = false, bool? addEndOfSentence = false, string? unknownToken = \"<|endoftext|>\", \r\n                                     string? beginningOfSentenceToken = \"<|endoftext|>\", string? endOfSentenceToken = \"<|endoftext|>\") { }\r\n\r\n        public CodeGen(Stream vocabularyStream, Stream mergeStream, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, IReadOnlyDictionary<string, int> addedTokens = null, \r\n                                    bool? addPrefixSpace = false, bool? addBeginningOfSentence = false, bool? addEndOfSentence = false, string? unknownToken = \"<|endoftext|>\", \r\n                                    string? beginningOfSentenceToken = \"<|endoftext|>\", string? endOfSentenceToken = \"<|endoftext|>\") { }\r\n\r\n        public bool AddBeginningOfSentence { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> AddedTokens { get { throw null; } }\r\n\r\n        public bool AddEndOfSentence { get { throw null; } }\r\n\r\n        public bool AddPrefixSpace { get { throw null; } }\r\n\r\n        public int? BeginningOfSentenceId { get { throw null; } }\r\n\r\n        public string? BeginningOfSentenceToken { get { throw null; } }\r\n\r\n        public int? EndOfSentenceId { get { throw null; } }\r\n\r\n        public string? EndOfSentenceToken { get { throw null; } }\r\n\r\n        public string? UnknownToken { get { throw null; } }\r\n\r\n        public int? UnknownTokenId { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                               out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                               out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int CountTokens(ReadOnlySpan<char> text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int CountTokens(string text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        \r\n        public int IndexOfTokenCount(string text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                               out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int LastIndexOfTokenCount(string text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                 out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        \r\n        public IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                 bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<Token> Encode(string text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                  bool considerPreTokenization = true,  bool considerNormalization = true) { throw null; }\r\n\r\n        public string? Decode(IEnumerable<int> ids, bool hasPrefixSpace, bool considerSpecialTokens) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - SentencePieceBpe\r\n\r\n```C#\r\n    public sealed partial class SentencePieceBpe : Tokenizer\r\n    {\r\n        internal SentencePieceBpe() { }\r\n\r\n        public bool AddBeginningOfSentence { get { throw null; } }\r\n\r\n        public bool AddDummyPrefix { get { throw null; } }\r\n\r\n        public bool AddEndOfSentence { get { throw null; } }\r\n\r\n        public int BeginningOfSentenceId { get { throw null; } }\r\n\r\n        public string BeginningOfSentenceToken { get { throw null; } }\r\n\r\n        public bool ByteFallback { get { throw null; } }\r\n\r\n        public int EndOfSentenceId { get { throw null; } }\r\n\r\n        public string EndOfSentenceToken { get { throw null; } }\r\n\r\n        public bool EscapeWhiteSpaces { get { throw null; } }\r\n\r\n\r\n        public bool TreatWhitespaceAsSuffix { get { throw null; } }\r\n\r\n        public int UnknownId { get { throw null; } }\r\n\r\n        public string UnknownToken { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public int CountTokens(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int CountTokens(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerNormalization, out string? normalizedString, out int textLength, int maxTokenCount = int.MaxValue) { throw null; }\r\n\r\n        public int CountTokens(string text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerNormalization, \r\n                                                            out string? normalizedString, out int textLength, int maxTokenCount = int.MaxValue) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                             out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                             out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int IndexOfTokenCount(string text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                            out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int IndexOfTokenCount(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                             out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, bool addBeginningOfSentence, bool addEndOfSentence, bool considerNormalization, \r\n                                                              out string? normalizedString, out int tokenCount) { throw null; }\r\n\r\n        public IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/7144","RelatedDescription":"Open issue \"Tokenizers Library Design\" (#7144)"},{"Id":"2266915088","IsPullRequest":false,"CreatedAt":"2024-04-27T08:37:12","Actor":"r-Larch","Number":"7143","RawContent":null,"Title":"[Tokenizers] Question regarding performance","State":"open","Body":"Hi, thanks for the effort put into the Microsoft.ML.Tokenizers!\r\n\r\nI'm the author of the last performance improvements in `SharpToken` library.\r\nSince MLTokenizers are faster now than SharpToken I looked into the sources to understand where this performance comes from.\r\n\r\n**Now I have a question (out of curiosity)**\r\n\r\nWhy is it required to copy a `ReadOnlySpan<char>` to a buffer, when the rest of the code just uses `ReadOnlySpan<char>` again?\r\n\r\n**TiktokenPreTokenizer.cs** line: 104\r\nhttps://github.com/dotnet/machinelearning/blob/72cfdf611a510ba0570170a708ddcc1a1928f329/src/Microsoft.ML.Tokenizers/PreTokenizer/TiktokenPreTokenizer.cs#L95-L107\r\n\r\n**PreTokenizer.cs** line: 74\r\nhttps://github.com/dotnet/machinelearning/blob/72cfdf611a510ba0570170a708ddcc1a1928f329/src/Microsoft.ML.Tokenizers/PreTokenizer/PreTokenizer.cs#L43-L54","Url":"https://github.com/dotnet/machinelearning/issues/7143","RelatedDescription":"Open issue \"[Tokenizers] Question regarding performance\" (#7143)"},{"Id":"2266549113","IsPullRequest":true,"CreatedAt":"2024-04-26T21:50:38","Actor":"sevenzees","Number":"7142","RawContent":null,"Title":"Allow developers to supply their own function to infer column data types from data while loading CSVs","State":"open","Body":"Fixes #7141\r\n\r\nCurrently when you use `LoadCsv` or `LoadCsvFromString` without supplying data types for each column, the code will try to guess the data types based on the data in the CSV file. This is good, but the problem is that the default type inference code only considers `bool`, `float`, `DateTime`, and `string` for column types. Sometimes the user may need another data type, such as `int`, `long`, or `double` (see [issue 6347](https://github.com/dotnet/machinelearning/issues/6347) for an example where someone had a problem with the `float` data type that was chosen by default) but not know the structure of the data ahead of time.\r\n\r\nI would like to be able to pass in my own custom type inference logic to override the default `GuessKind` implementation that is given in the library right now. If no custom guess type function is provided to the `LoadCsv` or `LoadCsvFromString` methods, then the code should work the same as it does today.","Url":"https://github.com/dotnet/machinelearning/pull/7142","RelatedDescription":"Open PR \"Allow developers to supply their own function to infer column data types from data while loading CSVs\" (#7142)"},{"Id":"2266545151","IsPullRequest":false,"CreatedAt":"2024-04-26T21:46:25","Actor":"sevenzees","Number":"7141","RawContent":null,"Title":"Allow developers to supply their own function to infer column data types from data while loading CSVs","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nCurrently when you use `LoadCsv` or `LoadCsvFromString` without supplying data types for each column, the code will try to guess the data types based on the data in the CSV file. This is good, but the problem is that the default type inference code only considers `bool`, `float`, `DateTime`, and `string` for column types. Sometimes the user may need another data type, such as `int`, `long`, or `double` (see [issue 6347](https://github.com/dotnet/machinelearning/issues/6347) for an example where someone had a problem with the `float` data type that was chosen by default) but not know the structure of the data ahead of time.\r\n\r\n**Describe the solution you'd like**\r\nI would like to be able to pass in my own custom type inference logic to override the default `GuessKind` implementation that is given in the library right now. If no custom guess type function is provided to the `LoadCsv` or `LoadCsvFromString` methods, then the code should work the same as it does today.\r\n\r\n**Describe alternatives you've considered**\r\nThe alternative is to call `LoadCsv`/`LoadCsvFromString` with `dataTypes` set to an array filled with `typeof(string)` for each column in your data, and then run your logic on the `DataFrame` with all string type columns to convert the columns to the data types that make sense for each column based on the data that is in each column.\r\n\r\n**Additional context**\r\nI already have implemented a fix for this issue that I would like to merge in with a pull request.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7141","RelatedDescription":"Open issue \"Allow developers to supply their own function to infer column data types from data while loading CSVs\" (#7141)"},{"Id":"2259274067","IsPullRequest":false,"CreatedAt":"2024-04-23T16:17:50","Actor":"chrisevans9629","Number":"7140","RawContent":null,"Title":"Get Loss During Training for Visualization (Learning Curve Graph)","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nI need a way to visualize how my model is learning during training, which is a comparison between training loss and test loss.\r\n\r\n**Describe the solution you'd like**\r\nAn event handler that enables the ability to extract loss during training.\r\n\r\n**Describe alternatives you've considered**\r\nRunning the model for x epochs, evaluating the model, then retraining the model in a loop.  This unfortunately does not work for all models, such as LightGbm that can't be retrained.\r\n\r\n```csharp\r\nvar kfold = ctx.BinaryClassification.CrossValidate(training, estimator, param.kfold);\r\nvar bestModel = kfold.OrderByDescending(p => p.Metrics.Accuracy).Select(p => p.Model).First();\r\n\r\nvar testOutput = bestModel.Transform(test);\r\n\r\nvar metrics = ctx.BinaryClassification.Evaluate(testOutput);\r\n\r\n// This code doesn't work as estimator is IEstimator<ITransform> and bestModel is ITransform. Not sure how you would do this...\r\nestimator = bestModel;\r\n```\r\n\r\n**Additional context**\r\nUltimately, I am trying to analyze what the models I'm comparing are actually doing and so far I haven't found any documentation or any straightforward way to do it.\r\n![image](https://github.com/dotnet/machinelearning/assets/33850520/79ca7603-a42b-4746-a8b3-76e7f2c3aa2a)\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7140","RelatedDescription":"Open issue \"Get Loss During Training for Visualization (Learning Curve Graph)\" (#7140)"},{"Id":"2256140857","IsPullRequest":false,"CreatedAt":"2024-04-22T10:13:44","Actor":"matnatx","Number":"7137","RawContent":null,"Title":"Specify Categorical Features in LightGBM","State":"open","Body":"I'm trying to use LightGBM algorithm in ML.NET to train a model using a dataset.\r\n\r\nThe dataset has a number of categorical features such as \"gender\", \"race\",  \"country_of_birth\".\r\n\r\nI can convert/encode these categorical data as integers.\r\n\r\nMy question is how I can specify that these features are categorical and not (continuous variables), so that LightGBM can handle them as categorical data.\r\n\r\nI can use OneHotEncoding/OneHotHashEncoding, but I believe it's not the same.\r\n\r\nTIA","Url":"https://github.com/dotnet/machinelearning/issues/7137","RelatedDescription":"Open issue \"Specify Categorical Features in LightGBM\" (#7137)"},{"Id":"2255452625","IsPullRequest":true,"CreatedAt":"2024-04-22T02:31:25","Actor":"feiyun0112","Number":"7136","RawContent":null,"Title":"Accessing data by column after adding columns to a DataFrame returns error data","State":"open","Body":"fix #7135 \r\n\r\nDescribe the bug\r\nAccessing data by column after adding columns to a DataFrame returns error data\r\n\r\n````\r\nvar df = DataFrame.LoadCsvFromString(\"a1,a2\\n1,2\\n3,4\");\r\nvar dc0 = DataFrameColumn.Create(\"a0\", new int[] { 0, 0 });\r\ndf.Columns.Insert(0, dc0);\r\nvar dc1 = df[\"a1\"];\r\nConsole.WriteLine(dc1.ToString());\r\n````\r\n\r\n\r\nThis code expected print: a1: 1 3\r\nBut it print: a0: 0 0","Url":"https://github.com/dotnet/machinelearning/pull/7136","RelatedDescription":"Open PR \"Accessing data by column after adding columns to a DataFrame returns error data\" (#7136)"},{"Id":"2254553728","IsPullRequest":false,"CreatedAt":"2024-04-21T12:33:49","Actor":"CodingOctocat","Number":"7134","RawContent":null,"Title":"How to predict text type based on input text?","State":"closed","Body":"I'm a newbie and I read the tutorial [github-issue-classification](https://learn.microsoft.com/en-us/dotnet/machine-learning/tutorials/github-issue-classification), but I don't quite understand it, and it seems that my question should be `classification`.\r\n\r\nMy specific requirement is to develop a database query function for a single text box (search box), where the program automatically predicts that the input keyword belongs to a certain column of data in the database and filters the query against it. For example:\r\n\r\n- text box input: _KS 25-3LM_, the program should be predicted to be: **Model**\r\n- text box input: _A543148543143_, the program should be predicted to be: **CertificateNumber**\r\n\r\nHere is my data model where `ProductCategory`, `ProductName`, `Models`, `Enterprise`, `CertificateNumber`, `ReportNumbers` are the fields that I need to involve in the training, note that `Models`, `ReportNumbers` are list types. ReportNumbers` are list types.\r\n\r\nMy database uses Ef Core/SQLite for storage and has a record size of about 150,000 rows.\r\n\r\nCan you all help me to implement this feature? I want to learn and understand ML.NET with this example.\r\n\r\n```csharp\r\n[Table(\"FooSet\")]\r\n[Index(nameof(CertificateNumber), nameof(ReportNumbers), nameof(Models), nameof(Enterprise), nameof(ProductName))]\r\npublic partial record Foo\r\n{\r\n    private string _productCategory = \"\";\r\n\r\n    [Column(\"cert_exp\")]\r\n    public DateOnly CertificateExpires { get; set; }\r\n\r\n    [Column(\"cert_iss\")]\r\n    public DateOnly CertificateIssue { get; set; }\r\n\r\n    [Key]\r\n    [Column(\"cert_no\")]\r\n    public string CertificateNumber { get; set; }\r\n\r\n    [Column(\"ent\")]\r\n    public string Enterprise { get; set; }\r\n\r\n    [Column(\"models\")]\r\n    public List<string> Models { get; set; }\r\n\r\n    [Column(\"cat\")]\r\n    public string ProductCategory\r\n    {\r\n        get => _productCategory ?? \"\";\r\n        set => _productCategory = value ?? \"\";\r\n    }\r\n\r\n    [Column(\"name\")]\r\n    public string ProductName { get; set; }\r\n\r\n    [Column(\"rpt_nos\")]\r\n    public List<string> ReportNumbers { get; set; }\r\n\r\n    [Column(\"status\")]\r\n    public string Status { get; set; }\r\n\r\n    public Foo(string productCategory, string productName, List<string> models, string enterprise,\r\n        string certificateNumber, List<string> reportNumbers,\r\n        DateOnly certificateIssue, DateOnly certificateExpires, string status)\r\n    {\r\n        ProductCategory = productCategory;\r\n        ProductName = productName;\r\n        Models = models;\r\n        Enterprise = enterprise;\r\n        CertificateNumber = certificateNumber;\r\n        ReportNumbers = reportNumbers;\r\n\r\n        CertificateIssue = certificateIssue;\r\n        CertificateExpires = certificateExpires;\r\n        Status = status;\r\n    }\r\n}\r\n```\r\n\r\nrefer link: [machinelearning-samples/issues/1028](https://github.com/dotnet/machinelearning-samples/issues/1028)","Url":"https://github.com/dotnet/machinelearning/issues/7134","RelatedDescription":"Closed issue \"How to predict text type based on input text?\" (#7134)"},{"Id":"2254864513","IsPullRequest":false,"CreatedAt":"2024-04-21T02:26:35","Actor":"wildwind2000","Number":"7135","RawContent":null,"Title":"Accessing data by column after adding columns to a DataFrame returns error  data","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: Windows 11\r\n - ML.NET Version: 3.01\r\n - .NET Version: .NET 8.0\r\n\r\n**Describe the bug**\r\nAccessing data by column after adding columns to a DataFrame returns error data\r\n\r\n```code\r\nvar df = DataFrame.LoadCsvFromString(\"a1,a2\\n1,2\\n3,4\");\r\nvar dc0 = DataFrameColumn.Create(\"a0\", new int[] { 0, 0 });\r\ndf.Columns.Insert(0, dc0);\r\nvar dc1 = df[\"a1\"];\r\nConsole.WriteLine(dc1.ToString());\r\n```\r\nThis code expected print: a1: 1 3   \r\nBut it print: a0: 0 0\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7135","RelatedDescription":"Open issue \"Accessing data by column after adding columns to a DataFrame returns error  data\" (#7135)"},{"Id":"2247028859","IsPullRequest":false,"CreatedAt":"2024-04-19T22:56:52","Actor":"muhamedkarajic","Number":"7130","RawContent":null,"Title":"ML.NET can't add Evaluate logic into pipeline","State":"closed","Body":"**System Information (please complete the following information):**\r\n - OS & Version: macOS Monterey 12.6.6\r\n - ML.NET Version: ML.NET 3.0.1 (Had it also with 2.X)\r\n - .NET Version: .NET 6.0\r\n\r\n**Describe the bug**\r\nI want to split the training and test set and evaluate the model. Therefor I have created a function:\r\n```csharp\r\nprivate static void EvaluateModel(MLContext mlContext, ITransformer trainedModel, IDataView testData)\r\n{\r\n    var predictedData = trainedModel.Transform(testData);\r\n\r\n    var metrics = mlContext.BinaryClassification.EvaluateNonCalibrated(predictedData, \"target\", \"Score\", \"PredictedLabel\");\r\n\r\n    Console.WriteLine($\"Accurecy: {metrics.Accuracy: 0.###}\");\r\n    Console.WriteLine($\"---------------------------\");\r\n    Console.WriteLine($\"Confusion Matrix\");\r\n    Console.WriteLine(metrics.ConfusionMatrix.GetFormattedConfusionTable());\r\n    Console.WriteLine();\r\n}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run `mlnet classification --dataset \"./FILE_PATH/FILE_NAME.csv\" --label-col 11 --has-header true --train-time 60`\r\n2. Add to `SampleClassification.training.cs` the function `EvaluateModel`\r\n3. Adjust the already existing `Train` function by\r\n4. run `dotnet run`\r\n5. Error: `An unhandled exception of type 'System.ArgumentOutOfRangeException' occurred in Microsoft.ML.Data.dll: 'Schema mismatch for score column 'Score': expected Single, got Vector<Single, 2>'`\r\n\r\n**Expected behavior**\r\nI would expect the code to work since the Score is something which ML.NET creates. **It seems like it expects the score to be a sinle value while its a compex vector.**\r\n\r\n**Screenshots, Code, Sample Projects**\r\n\r\nHere is the adjusted train function:\r\n```csharp\r\npublic static void Train(string outputModelPath, string inputDataFilePath = RetrainFilePath, char separatorChar = RetrainSeparatorChar, bool hasHeader = RetrainHasHeader)\r\n{\r\n    var mlContext = new MLContext();\r\n\r\n    var data = LoadIDataViewFromFile(mlContext, inputDataFilePath, separatorChar, hasHeader);\r\n    var splitedData = mlContext.Data.TrainTestSplit(data, 0.2, null, 0);\r\n    var model = RetrainModel(mlContext, splitedData.TrainSet);\r\n    EvaluateModel(mlContext, model, data);\r\n    SaveModel(mlContext, model, data, outputModelPath);\r\n}\r\n```\r\n\r\n\r\n**Additional context**\r\nI ahve found that I am supposed to use`EvaluateNonCalibrated` instead of `Evaluate`. Have similar when using `Evaluate` its says that its missing Predictions. Error in that case `Probability column 'Probability' not found (Parameter 'schema')`. \r\n","Url":"https://github.com/dotnet/machinelearning/issues/7130","RelatedDescription":"Closed issue \"ML.NET can't add Evaluate logic into pipeline\" (#7130)"},{"Id":"2242316679","IsPullRequest":true,"CreatedAt":"2024-04-19T16:21:27","Actor":"tarekgh","Number":"7128","RawContent":null,"Title":"Tokenizer's APIs Update","State":"closed","Body":"Updating the Tokenizer's APIs:\r\n- Simplifying the APIs by merging the `Model` abstraction into the `Tokenizer` abstracted class.\r\n- Adding overloads to work with spans.\r\n- doing some clean up optimization and adding more tests.","Url":"https://github.com/dotnet/machinelearning/pull/7128","RelatedDescription":"Closed or merged PR \"Tokenizer's APIs Update\" (#7128)"},{"Id":"2249443247","IsPullRequest":true,"CreatedAt":"2024-04-18T00:29:47","Actor":"RussKie","Number":"7133","RawContent":null,"Title":"Update locker.yml","State":"closed","Body":"* Bump the Locker action version\r\nRefer to https://github.com/microsoft/vscode-github-triage-actions/pull/210\r\n\r\n* Restrict the Locker action to dotnet org","Url":"https://github.com/dotnet/machinelearning/pull/7133","RelatedDescription":"Closed or merged PR \"Update locker.yml\" (#7133)"},{"Id":"2248568194","IsPullRequest":true,"CreatedAt":"2024-04-18T00:10:16","Actor":"directhex","Number":"7132","RawContent":null,"Title":"[release/3.0] Don't use deprecated -pt images","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/7132","RelatedDescription":"Closed or merged PR \"[release/3.0] Don't use deprecated -pt images\" (#7132)"},{"Id":"2248565494","IsPullRequest":true,"CreatedAt":"2024-04-18T00:09:42","Actor":"directhex","Number":"7131","RawContent":null,"Title":"Don't use deprecated -pt images","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/7131","RelatedDescription":"Closed or merged PR \"Don't use deprecated -pt images\" (#7131)"},{"Id":"2242069777","IsPullRequest":false,"CreatedAt":"2024-04-17T19:56:53","Actor":"80LevelElf","Number":"7127","RawContent":null,"Title":"Regression FairLearn with AutoML?","State":"closed","Body":"At this moment we have a FairLearn extension method only for Binary Classification - SetBinaryClassificationMetricWithFairLearn method.\r\n\r\nBut during the code ML.net also have Regression Metric FairLearn support - https://github.com/dotnet/machinelearning/blob/main/src/Microsoft.ML.Fairlearn/Metrics/FairlearnMetricCatalog.cs\r\n\r\nIs it possible to use FairLearn with regression via AutoML?","Url":"https://github.com/dotnet/machinelearning/issues/7127","RelatedDescription":"Closed issue \"Regression FairLearn with AutoML?\" (#7127)"},{"Id":"2243822616","IsPullRequest":false,"CreatedAt":"2024-04-16T00:58:56","Actor":"yueyinqiu","Number":"7129","RawContent":null,"Title":"Is there an equivalent to pandas' get_dummies, in Microsoft.Data.Analysis?","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/issues/7129","RelatedDescription":"Closed issue \"Is there an equivalent to pandas' get_dummies, in Microsoft.Data.Analysis?\" (#7129)"},{"Id":"2237275563","IsPullRequest":false,"CreatedAt":"2024-04-11T09:17:22","Actor":"markbeij","Number":"7126","RawContent":null,"Title":"Create HTML from C# in Jupyter","State":"open","Body":"I hope someone here can help me with this.. I'm exploring the possibilities of data analysis using c# and see a lot of samples where you generate html from your Jupyter notebooks, like this blog from Microsoft: https://devblogs.microsoft.com/dotnet/an-introduction-to-dataframe/ by @pgovind\r\n\r\n```c#\r\nusing Microsoft.AspNetCore.Html;\r\nFormatter<DataFrame>.Register((df, writer) =>\r\n{\r\n    var headers = new List<IHtmlContent>();\r\n    headers.Add(th(i(\"index\")));\r\n    headers.AddRange(df.Columns.Select(c => (IHtmlContent) th(c.Name)));\r\n    var rows = new List<List<IHtmlContent>>();\r\n    var take = 20;\r\n    for (var i = 0; i < Math.Min(take, df.Rows.Count); i++)\r\n    {\r\n        var cells = new List<IHtmlContent>();\r\n        cells.Add(td(i));\r\n        foreach (var obj in df.Rows[i])\r\n        {\r\n            cells.Add(td(obj));\r\n        }\r\n        rows.Add(cells);\r\n    }\r\n\r\n    var t = table(\r\n        thead(\r\n            headers),\r\n        tbody(\r\n            rows.Select(\r\n                r => tr(r))));\r\n\r\n    writer.Write(t);\r\n}, \"text/html\");\r\n```\r\n\r\nIf I paste this in my notebook, I get\r\n`Error: (2,1): error CS0103: The name 'Formatter' does not exist in the current context`\r\n\r\nI also tried with a smaller example:\r\n\r\n```c#\r\nusing Microsoft.AspNetCore.Html;\r\nth(i(\"index\"))\r\n```\r\n\r\nBut I get \r\n`Error: (2,1): error CS0103: The name 'th' does not exist in the current context\r\n(2,4): error CS0103: The name 'i' does not exist in the current context`\r\n\r\nI'm using polyglot notebook in VS Code using C# Script.\r\nNot sure what more info is helpfull.","Url":"https://github.com/dotnet/machinelearning/issues/7126","RelatedDescription":"Open issue \"Create HTML from C# in Jupyter\" (#7126)"}],"ResultType":"GitHubIssue"}},"RunOn":"2024-05-19T03:30:15.6247373Z","RunDurationInMilliseconds":415}