{"Data":{"GitHub":{"Issues":[{"Id":"2625063601","IsPullRequest":true,"CreatedAt":"2024-10-30T19:03:16","Actor":"michaelgsharp","Number":"7282","RawContent":null,"Title":"Fixing native lookup","State":"open","Body":"Fixes native lookup with adds a bunch more testing for M1 and normal mac as well.","Url":"https://github.com/dotnet/machinelearning/pull/7282","RelatedDescription":"Open PR \"Fixing native lookup\" (#7282)"},{"Id":"2568178701","IsPullRequest":false,"CreatedAt":"2024-10-29T00:11:39","Actor":"mehdihadeli","Number":"7257","RawContent":null,"Title":"How To use Phi-3.5-mini tokenizer","State":"closed","Body":"Hi,\r\nHow can I create a tokenizer based on [microsoft/Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) model?\r\nShould I load [tokenzier.model](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/tokenizer.model)?\r\n\r\nI saw similar approach for llama in the [documentation](https://learn.microsoft.com/en-us/dotnet/machine-learning/whats-new/overview#additional-tokenizer-support):\r\n\r\n``` csharp\r\n// Create the Tokenizer.\r\nstring modelUrl = @\"https://huggingface.co/hf-internal-testing/llama-llamaTokenizer/resolve/main/llamaTokenizer.model\";\r\nusing Stream remoteStream = File.OpenRead(modelUrl);\r\nTokenizer llamaTokenizer = Tokenizer.CreateLlama(remoteStream);\r\n\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/7257","RelatedDescription":"Closed issue \"How To use Phi-3.5-mini tokenizer\" (#7257)"},{"Id":"2612812795","IsPullRequest":true,"CreatedAt":"2024-10-26T23:07:20","Actor":"tarekgh","Number":"7280","RawContent":null,"Title":"Address the feedback regarding Bert tokenizer","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/7280","RelatedDescription":"Closed or merged PR \"Address the feedback regarding Bert tokenizer\" (#7280)"},{"Id":"2616188188","IsPullRequest":false,"CreatedAt":"2024-10-26T22:43:25","Actor":"tarekgh","Number":"7281","RawContent":null,"Title":"WordPiece and Bert Tokenizer Design review","State":"open","Body":"## Proposal \n\nThe proposal omitted the overridden properties and method that is defined in the abstraction we already reviewed before.\n\n### *WordPiece Tokenizer*\n\n```C#\nnamespace Microsoft.ML.Tokenizers\n{\n    public partial class WordPieceTokenizer : Tokenizer\n    {\n        public static WordPieceTokenizer Create(\n                        string vocabFilePath,\n                        PreTokenizer? preTokenizer = null,\n                        Normalizer? normalizer = null,\n                        IReadOnlyDictionary<string, int>? specialTokens = null,\n                        string unknownToken = \"[UNK]\",\n                        string continuingSubwordPrefix = DefaultContinuingSubwordPrefix,\n                        int maxInputCharsPerWord = DefaultMaxInputCharsPerWord)\n\n        public static WordPieceTokenizer Create(\n                        Stream vocabStream,\n                        PreTokenizer? preTokenizer = null,\n                        Normalizer? normalizer = null,\n                        IReadOnlyDictionary<string, int>? specialTokens = null,\n                        string unknownToken = \"[UNK]\",\n                        string continuingSubwordPrefix = DefaultContinuingSubwordPrefix,\n                        int maxInputCharsPerWord = DefaultMaxInputCharsPerWord)\n\n        public static async Task<WordPieceTokenizer> CreateAsync(\n                        Stream vocabStream,\n                        PreTokenizer? preTokenizer = null,\n                        Normalizer? normalizer = null,\n                        IReadOnlyDictionary<string, int>? specialTokens = null,\n                        string unknownToken = \"[UNK]\",\n                        string continuingSubwordPrefix = DefaultContinuingSubwordPrefix,\n                        int maxInputCharsPerWord = DefaultMaxInputCharsPerWord,\n                        CancellationToken cancellationToken = default)\n\n        /// <summary>\n        /// Gets the unknown token.\n        /// A token that is not in the vocabulary cannot be converted to an ID and is set to be this token instead.\n        /// </summary>\n        public string UnknownToken { get; }\n\n        /// <summary>\n        /// Gets the unknown token ID.\n        /// A token that is not in the vocabulary cannot be converted to an ID and is set to be this token instead.\n        /// </summary>\n        public int UnknownTokenId { get; }\n\n        /// <summary>\n        /// Gets the prefix to use for sub-words that are not the first part of a word.\n        /// </summary>\n        public string ContinuingSubwordPrefix { get; }\n\n        /// <summary>\n        /// Gets the maximum number of characters to authorize in a single word.\n        /// </summary>\n        public int MaxInputCharsPerWord { get; }\n\n        /// <summary>\n        /// Gets the special tokens and their corresponding ids.\n        /// </summary>\n        public IReadOnlyDictionary<string, int>? SpecialTokens { get; }\n\n        /// <summary>\n        /// Decode the given ids, back to a String.\n        /// </summary>\n        /// <param name=\"ids\">The list of ids that we want to decode.</param>\n        /// <param name=\"skipSpecialTokens\">Indicate whether to skip the special tokens during the decoding.</param>\n        /// <returns>The decoded string.</returns>\n        public string Decode(IEnumerable<int> ids, bool skipSpecialTokens)\n\n        public OperationStatus Decode(IEnumerable<int> ids, Span<char> destination, bool skipSpecialTokens, \n                   out int idsConsumed, out int charsWritten)\n    }\n}\n```\n\n### *Bert Tokenizer*\n\n```C#\nnamespace Microsoft.ML.Tokenizers\n{\n    public sealed partial class BertTokenizer : WordPieceTokenizer\n    {\n        public static BertTokenizer Create(\n                    string vocabFilePath,\n                    bool doLowerCase = true,\n                    bool doBasicTokenization = true,\n                    bool splitOnSpecialTokens = true,\n                    string unknownToken = \"[UNK]\",\n                    string sepToken = \"[SEP]\",\n                    string padToken = \"[PAD]\",\n                    string clsToken = \"[CLS]\",\n                    string maskToken = \"[MASK]\",\n                    bool tokenizeChineseChars = true,\n                    bool stripAccents = false)    \n\n        public static BertTokenizer Create(\n                    Stream vocabStream,\n                    bool doLowerCase = true,\n                    bool doBasicTokenization = true,\n                    bool splitOnSpecialTokens = true,\n                    string unknownToken = \"[UNK]\",\n                    string sepToken = \"[SEP]\",\n                    string padToken = \"[PAD]\",\n                    string clsToken = \"[CLS]\",\n                    string maskToken = \"[MASK]\",\n                    bool tokenizeChineseChars = true,\n                    bool stripAccents = false)\n\n        public static async Task<BertTokenizer> CreateAsync(\n                    Stream vocabStream,\n                    bool doLowerCase = true,\n                    bool doBasicTokenization = true,\n                    bool splitOnSpecialTokens = true,\n                    string unknownToken = \"[UNK]\",\n                    string sepToken = \"[SEP]\",\n                    string padToken = \"[PAD]\",\n                    string clsToken = \"[CLS]\",\n                    string maskToken = \"[MASK]\",\n                    bool tokenizeChineseChars = true,\n                    bool stripAccents = false)\n\n        /// <summary>\n        /// Gets a value indicating whether the tokenizer should lowercase the input text.\n        /// </summary>\n        public bool DoLowerCase { get; }\n\n        /// <summary>\n        /// Gets a value indicating whether the tokenizer should do basic tokenization. Like clean text, normalize it, lowercasing, etc.\n        /// </summary>\n        public bool DoBasicTokenization { get; }\n\n        /// <summary>\n        /// Gets a value indicating whether the tokenizer should split on the special tokens or treat special tokens as normal text.\n        /// </summary>\n        public bool SplitOnSpecialTokens { get; }\n\n        /// <summary>\n        /// Gets the separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for sequence classification \n        /// or for a text and a question for question answering.\n        /// It is also used as the last token of a sequence built with special tokens.\n        /// </summary>\n        public string SepToken { get; }\n\n        /// <summary>\n        /// Gets the separator token Id\n        /// </summary>\n        public int SepTokenId { get; }\n\n        /// <summary>\n        /// Gets the token used for padding, for example when batching sequences of different lengths\n        /// </summary>\n        public string PadToken { get; }\n\n        /// <summary>\n        /// Gets padding token Id\n        /// </summary>\n        public int PadTokenId { get; }\n\n        /// <summary>\n        /// Gets the classifier token which is used when doing sequence classification (classification of the whole sequence \n        /// instead of per-token classification).\n        /// It is the first token of the sequence when built with special tokens.\n        /// </summary>\n        public string ClsToken { get; }\n\n        /// <summary>\n        /// Gets the classifier token Id\n        /// </summary>\n        public int ClsTokenId { get; }\n\n        /// <summary>\n        /// Gets the mask token used for masking values. This is the token used when training this model with masked language modeling.\n        /// This is the token which the model will try to predict.\n        /// </summary>\n        public string MaskToken { get; }\n\n        /// <summary>\n        /// Gets the mask token Id\n        /// </summary>\n        public int MaskTokenId { get; }\n\n        /// <summary>\n        /// Gets a value indicating whether the tokenizer should split the Chinese characters into tokens.\n        /// </summary>\n        public bool TokenizeChineseChars { get; }\n\n        /// <summary>\n        /// Gets a value indicating whether the tokenizer should strip accents characters.\n        /// </summary>\n        public bool StripAccents { get; }\n\n        public IReadOnlyList<int> EncodeToIds(string text, bool addSpecialTokens, \n                      bool considerPreTokenization = true, bool considerNormalization = true)\n\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addSpecialTokens, \n                      bool considerPreTokenization = true, bool considerNormalization = true)\n\n        public IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, bool addSpecialTokens, out string? normalizedText, \n                       out int charsConsumed, bool considerPreTokenization = true, bool considerNormalization = true)\n\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, bool addSpecialTokens, \n                        out string? normalizedText,  out int charsConsumed, bool considerPreTokenization = true, bool considerNormalization = true)\n\n        /// <summary>\n        /// Build model inputs from a sequence or a pair of sequences for sequence classification tasks by concatenating and \n        /// adding special tokens. A BERT sequence has the following format:\n        ///     - single sequence: `[CLS] tokenIds0 [SEP]`\n        ///     - pair of sequences: `[CLS] tokenIds0 [SEP] tokenIds1 [SEP]`\n        /// </summary>\n        /// <param name=\"tokenIds0\">List of IDs to which the special tokens will be added.</param>\n        /// <param name=\"tokenIds1\">Optional second list of IDs for sequence pairs.</param>\n        /// <returns>The list of IDs with special tokens added.</returns>\n        /// <exception cref=\"ArgumentNullException\">When <paramref name=\"tokenIds0\"/> is null.</exception>\n        public IReadOnlyList<int> BuildInputsWithSpecialTokens(IEnumerable<int> tokenIds0, IEnumerable<int>? tokenIds1 = null)\n\n        public OperationStatus BuildInputsWithSpecialTokens(IEnumerable<int> tokenIds0, Span<int> buffer, out int written, \n                             IEnumerable<int>? tokenIds1 = null)\n\n        /// <summary>\n        /// Retrieve sequence tokens mask from a IDs list.\n        /// </summary>\n        /// <param name=\"tokenIds0\">List of IDs.</param>\n        /// <param name=\"tokenIds1\">Optional second list of IDs for sequence pairs.</param>\n        /// <param name=\"alreadyHasSpecialTokens\">Indicate whether or not the token list is already formatted with special tokens \n        /// for the model.</param>\n        /// <returns>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</returns>\n        /// <exception cref=\"ArgumentNullException\"></exception>\n        public IReadOnlyList<int> GetSpecialTokensMask(IEnumerable<int> tokenIds0, IEnumerable<int>? tokenIds1 = null, \n                    bool alreadyHasSpecialTokens = false)\n\n        public OperationStatus GetSpecialTokensMask(IEnumerable<int> tokenIds0, Span<int> buffer, out int written, \n                     IEnumerable<int>? tokenIds1 = null, bool alreadyHasSpecialTokens = false)\n\n        /// <summary>\n        /// Create a mask from the two sequences passed to be used in a sequence-pair classification task. \n        /// A BERT sequence pair mask has the following format:\n        ///         0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n        ///         | first sequence    | second sequence |\n        /// If <paramref name=\"tokenIds1\"/> is null, this method only returns the first portion of the type ids (0s).\n        /// </summary>\n        /// <param name=\"tokenIds0\">List of token IDs for the first sequence.</param>\n        /// <param name=\"tokenIds1\">Optional list of token IDs for the second sequence.</param>\n        /// <returns>List of token type IDs according to the given sequence(s).</returns>\n        /// <exception cref=\"ArgumentNullException\">When <paramref name=\"tokenIds0\"/> is null.</exception>\n        public IReadOnlyList<int> CreateTokenTypeIdsFromSequences(IEnumerable<int> tokenIds0, IEnumerable<int>? tokenIds1 = null)\n\n        public OperationStatus CreateTokenTypeIdsFromSequences(IEnumerable<int> tokenIds0, Span<int> buffer, out int written, \n                         IEnumerable<int>? tokenIds1 = null)\n    }\n}\n```\n\n### *PreTokenizer Factory methods*\n\n```C#\nnamespace Microsoft.ML.Tokenizers\n{\n    public abstract partial class PreTokenizer\n    {\n        // @\"\\w+|[\\p{P}]\"\n        public static PreTokenizer CreateWhiteSpaceOrPunctuationPreTokenizer(IReadOnlyDictionary<string, int>? specialTokensEncoder = null)\n\n        // @\"\\w+|[^\\w\\s]+\"\n        public static PreTokenizer CreateWordOrNonWordPreTokenizer(IReadOnlyDictionary<string, int>? specialTokensEncoder = null)\n\n        // @\"\\S+\"\n        public static PreTokenizer CreateWhiteSpacePreTokenizer(IReadOnlyDictionary<string, int>? specialTokensEncoder = null)\n\n    }\n}\n```\n\n","Url":"https://github.com/dotnet/machinelearning/issues/7281","RelatedDescription":"Open issue \"WordPiece and Bert Tokenizer Design review\" (#7281)"},{"Id":"2595307233","IsPullRequest":true,"CreatedAt":"2024-10-22T22:27:05","Actor":"michaelgsharp","Number":"7273","RawContent":null,"Title":"Adds in a way to add settings for the MLContext.","State":"closed","Body":"Adds in a way to add settings for the MLContext. This will allow us to add runtime options for things like OnnxRuntime and LightGBM, or additional advanced options we haven't exposed previously.","Url":"https://github.com/dotnet/machinelearning/pull/7273","RelatedDescription":"Closed or merged PR \"Adds in a way to add settings for the MLContext.\" (#7273)"},{"Id":"2606306465","IsPullRequest":true,"CreatedAt":"2024-10-22T20:46:20","Actor":"michaelgsharp","Number":"7279","RawContent":null,"Title":"fixing osx ci","State":"closed","Body":"Fixing OSX CI for cross-arm builds","Url":"https://github.com/dotnet/machinelearning/pull/7279","RelatedDescription":"Closed or merged PR \"fixing osx ci\" (#7279)"},{"Id":"2604163942","IsPullRequest":true,"CreatedAt":"2024-10-22T19:33:58","Actor":"tarekgh","Number":"7275","RawContent":null,"Title":"Introducing WordPiece and Bert tokenizers","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/7275","RelatedDescription":"Closed or merged PR \"Introducing WordPiece and Bert tokenizers\" (#7275)"},{"Id":"2605975205","IsPullRequest":true,"CreatedAt":"2024-10-22T16:55:12","Actor":"michaelgsharp","Number":"7278","RawContent":null,"Title":"fixing apple silicon official build","State":"closed","Body":"Fixes official build.","Url":"https://github.com/dotnet/machinelearning/pull/7278","RelatedDescription":"Closed or merged PR \"fixing apple silicon official build\" (#7278)"},{"Id":"2605837636","IsPullRequest":false,"CreatedAt":"2024-10-22T15:45:10","Actor":"alexcovington","Number":"7277","RawContent":null,"Title":"GetTempPath hot in some benchmarks in Microsoft.ML.PerformanceTests","State":"open","Body":"I have noticed that depending on the length of the temporary path, some benchmarks see a significant performance drop because of the need to convert from a Windows short name to a long name.\n\nIf I use the temporary directory path `C:\\tempdir\\`, the benchmark gives these performance results on my machine:\n\n```\nMicrosoft.ML.PerformanceTests.exe --filter '*CreatePredictionEngine' --envVars TEMP:C:\\tempdir TMP:C:\\tempdir\n\n\n| Method                 | Mean     | Error   | StdDev  | Extra Metric |\n|----------------------- |---------:|--------:|--------:|-------------:|\n| CreatePredictionEngine | 249.5 us | 4.80 us | 4.93 us |            - |\n```\n\nIf I use a long temporary directory `C:\\Users\\username\\AppData\\Local\\Temp\\ML.NET\\A Very Very Long File Name That Will Be Converted To A Short Path\\Temp` (which has the short name `C:\\Users\\username\\AppData\\Local\\Temp\\ML.NET\\AVERYV~1\\Temp`), the benchmark shows significantly worse performance on the same machine:\n\n```\n .\\Microsoft.ML.PerformanceTests.exe --filter '*CreatePredictionEngine' --envVars TEMP:C:\\Users\\username\\AppData\\Local\\Temp\\ML.NET\\AVERYV~1\\Temp TMP:C:\\Users\\username\\AppData\\Local\\Temp\\ML.NET\\AVERYV~1\\Temp\n\n| Method                 | Mean     | Error    | StdDev   | Extra Metric |\n|----------------------- |---------:|---------:|---------:|-------------:|\n| CreatePredictionEngine | 10.53 ms | 0.082 ms | 0.077 ms |            - |\n```\n\n\nLooking at this under PerfView, I see a lot of extra time spent [here](https://github.com/dotnet/machinelearning/blob/f385b06aa0aeb64000fc341ecc31e43057c1ee13/src/Microsoft.ML.Core/Environment/HostEnvironmentBase.cs#L330).\n\nI'm wondering if it is possible to cache this lookup? It seems misleading for the benchmark performance to fluctuate so much because of the temporary directory path.","Url":"https://github.com/dotnet/machinelearning/issues/7277","RelatedDescription":"Open issue \"GetTempPath hot in some benchmarks in Microsoft.ML.PerformanceTests\" (#7277)"},{"Id":"2604822504","IsPullRequest":false,"CreatedAt":"2024-10-22T09:21:34","Actor":"farsdewibs0n","Number":"7276","RawContent":null,"Title":"ImageClassificationTrainer stuck on downloading Downloading resnet_v2_50_299.meta","State":"open","Body":"**System Information (please complete the following information):**\n - OS & Version: Windows 10\n - ML.NET Version: Unkonwn (assuming the lastest version since I don't know how to check it and I just updated the library yesterday)\n - .NET Version: .NET 8.0\n\n**Describe the bug**\nUnable to train with data provided.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Select \"Image classification\"\n2. Use Local (CPU)\n3. Select Train Data folder\n4. Click \"Start Training\"\n5. See error\n\n**Expected behavior**\nVisual Studio should train a new model.\n\n**Screenshots, Code, Sample Projects**\n[MLModel1-WZ453D.txt](https://github.com/user-attachments/files/17473486/MLModel1-WZ453D.txt)\n\n\n**Additional context**\nLast time I tried (moths ago) this also happened when using Local (GPU).","Url":"https://github.com/dotnet/machinelearning/issues/7276","RelatedDescription":"Open issue \"ImageClassificationTrainer stuck on downloading Downloading resnet_v2_50_299.meta\" (#7276)"},{"Id":"2598541568","IsPullRequest":true,"CreatedAt":"2024-10-18T23:26:01","Actor":"carlossanlop","Number":"7274","RawContent":null,"Title":"Bump versions of maintenance-packages dependencies consumed in machin…","State":"open","Body":"dotnet/runtime depends on these packages that we are now publishing from dotnet/maintenance-packages:\r\n\r\n- Microsoft.Bcl.HashCode\r\n- System.Buffers\r\n- System.Memory\r\n- System.Runtime.CompilerServices.Unsafe\r\n\r\nBumping their versions to consume the new preview versions, available in the dotnet-libraries feed: https://dnceng.visualstudio.com/public/_artifacts/feed/dotnet-libraries\r\n\r\nRelated PRs:\r\n- runtime: https://github.com/dotnet/runtime/pull/108806\r\n- winforms: https://github.com/dotnet/winforms/pull/12313","Url":"https://github.com/dotnet/machinelearning/pull/7274","RelatedDescription":"Open PR \"Bump versions of maintenance-packages dependencies consumed in machin…\" (#7274)"},{"Id":"2592512571","IsPullRequest":false,"CreatedAt":"2024-10-17T21:16:24","Actor":"euju-ms","Number":"7271","RawContent":null,"Title":"Aot compatibility for ML.Tokenizers","State":"closed","Body":"**Is your feature request related to a problem? Please describe.**\nML.Tokenizers project is not [Aot compatible](https://learn.microsoft.com/en-us/dotnet/core/deploying/native-aot/?tabs=windows%2Cnet8).\n\n**Describe the solution you'd like**\nML.Tokenizers project needs to use [SourceGenerationContext](https://learn.microsoft.com/en-us/dotnet/standard/serialization/system-text-json/source-generation?pivots=dotnet-8-0).","Url":"https://github.com/dotnet/machinelearning/issues/7271","RelatedDescription":"Closed issue \"Aot compatibility for ML.Tokenizers\" (#7271)"},{"Id":"2592553605","IsPullRequest":true,"CreatedAt":"2024-10-17T21:16:23","Actor":"euju-ms","Number":"7272","RawContent":null,"Title":"Fixes #7271 AOT for ML.Tokenizers","State":"closed","Body":"Fixes https://github.com/dotnet/machinelearning/issues/7271\r\n\r\nThis PR makes ML.Tokenizers project [AOT compatible](https://learn.microsoft.com/en-us/dotnet/core/deploying/native-aot/?tabs=windows%2Cnet8). \r\n\r\nML.Tokenizers is made to use [SourceGenerationContext](https://learn.microsoft.com/en-us/dotnet/standard/serialization/system-text-json/source-generation?pivots=dotnet-8-0) for deserializing Json.\r\n\r\nI had to create a helper class `Vocabulary` in order to register a `JsonConverter` for it.\r\n\r\n\r\n**Before** the change, we have following Aot warnings on the calls to Json.Deserialize:\r\n```\r\n  Microsoft.ML.Tokenizers failed with 8 error(s) (2.6s)\r\n    C:\\Users\\euju\\RiderProjects\\ml\\src\\Microsoft.ML.Tokenizers\\Model\\EnglishRobertaTokenizer.cs(173,25): error IL3050: Using member 'System.Text.Json.JsonSerializer.Deserialize<TValue>(Stream, JsonSerializerOptions)' which has 'RequiresDynamicCodeAttribute' can break functionality when AOT compiling. JSON serialization and deserialization might require types that cannot be statically analyzed and might need runtime code generation. Use System.Text.Json source generation for native AOT applications.\r\n    C:\\Users\\euju\\RiderProjects\\ml\\src\\Microsoft.ML.Tokenizers\\Model\\EnglishRobertaTokenizer.cs(173,25): error IL2026: Using member 'System.Text.Json.JsonSerializer.Deserialize<TValue>(Stream, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed. Use the overload that takes a JsonTypeInfo or JsonSerializerContext, or make sure all of the required types are preserved.\r\n    C:\\Users\\euju\\RiderProjects\\ml\\src\\Microsoft.ML.Tokenizers\\Model\\BPETokenizer.cs(763,59): error IL3050: Using member 'System.Text.Json.JsonSerializer.DeserializeAsync<TValue>(Stream, JsonSerializerOptions, CancellationToken)' which has 'RequiresDynamicCodeAttribute' can break functionality when AOT compiling. JSON serialization and deserialization might require types that cannot be statically analyzed and might need runtime code generation. Use System.Text.Json source generation for native AOT applications.\r\n    C:\\Users\\euju\\RiderProjects\\ml\\src\\Microsoft.ML.Tokenizers\\Model\\BPETokenizer.cs(764,53): error IL3050: Using member 'System.Text.Json.JsonSerializer.Deserialize<TValue>(Stream, JsonSerializerOptions)' which has 'RequiresDynamicCodeAttribute' can break functionality when AOT compiling. JSON serialization and deserialization might require types that cannot be statically analyzed and might need runtime code generation. Use System.Text.Json source generation for native AOT applications.\r\n    C:\\Users\\euju\\RiderProjects\\ml\\src\\Microsoft.ML.Tokenizers\\Model\\BPETokenizer.cs(763,59): error IL2026: Using member 'System.Text.Json.JsonSerializer.DeserializeAsync<TValue>(Stream, JsonSerializerOptions, CancellationToken)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed. Use the overload that takes a JsonTypeInfo or JsonSerializerContext, or make sure all of the required types are preserved.\r\n    C:\\Users\\euju\\RiderProjects\\ml\\src\\Microsoft.ML.Tokenizers\\Model\\BPETokenizer.cs(764,53): error IL2026: Using member 'System.Text.Json.JsonSerializer.Deserialize<TValue>(Stream, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed. Use the overload that takes a JsonTypeInfo or JsonSerializerContext, or make sure all of the required types are preserved.\r\n    C:\\Users\\euju\\RiderProjects\\ml\\src\\Microsoft.ML.Tokenizers\\Model\\CodeGenTokenizer.cs(1771,25): error IL3050: Using member 'System.Text.Json.JsonSerializer.Deserialize<TValue>(Stream, JsonSerializerOptions)' which has 'RequiresDynamicCodeAttribute' can break functionality when AOT compiling. JSON serialization and deserialization might require types that cannot be statically analyzed and might need runtime code generation. Use System.Text.Json source generation for native AOT applications.\r\n    C:\\Users\\euju\\RiderProjects\\ml\\src\\Microsoft.ML.Tokenizers\\Model\\CodeGenTokenizer.cs(1771,25): error IL2026: Using member 'System.Text.Json.JsonSerializer.Deserialize<TValue>(Stream, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed. Use the overload that takes a JsonTypeInfo or JsonSerializerContext, or make sure all of the required types are preserved.\r\n```\r\n\r\n**After** the change, warnings are no more :)\r\n\r\n**Note** that `netstandard2.0` framework is not Aot compatible as trimming is only supported for .NET 6 and later. Therefore, in order to test the compatibility for Microsoft.ML.Tokenizers project, you need to set the TargetFramework to net8.0.\r\n\r\nThen you can add `<PublishAot>true</PublishAot>`.\r\n\r\ne.g. Microsoft.ML.Tokenizers.csproj\r\n```Xml\r\n  <PropertyGroup>\r\n    <TargetFramework>net8.0</TargetFramework>\r\n    <Nullable>enable</Nullable>\r\n    <IsPackable>true</IsPackable>\r\n    <PackageDescription>Microsoft.ML.Tokenizers contains the implmentation of the tokenization used in the NLP transforms.</PackageDescription>\r\n    <AllowUnsafeBlocks>true</AllowUnsafeBlocks>\r\n    <PublishAot>true</PublishAot>\r\n  </PropertyGroup>\r\n```\r\n\r\nThen building specifically for `Microsoft.ML.Tokenizers` should result in warnings if the project is not AOT compatible.\r\n\r\n\r\n\r\nWe are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [x] There's a descriptive title that will make sense to other developers some time from now. \r\n- [x] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [x] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [ ] You have included any necessary tests in the same PR.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7272","RelatedDescription":"Closed or merged PR \"Fixes #7271 AOT for ML.Tokenizers\" (#7272)"},{"Id":"2584159813","IsPullRequest":true,"CreatedAt":"2024-10-13T18:08:37","Actor":"LittleLittleCloud","Number":"7270","RawContent":null,"Title":"[GenAI] Introduce CausalLMPipelineChatClient for MEAI.IChatClient","State":"open","Body":"We are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [x] There's a descriptive title that will make sense to other developers some time from now. \r\n- [x] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [x] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [x] You have included any necessary tests in the same PR.\r\n\r\nThis PR implements `CausalLMPipelineChatClient<TTokenizer, TCausalLMModel>` class for MEAI.IChatClient's intergration.\r\n\r\nThis PR also refactors the AutoGen and semantic kernel intergretion for Phi3, which pulls out the prompt template parts into an individual `Phi3ChatTemplateBuilder` class.\r\n\r\nThis PR also update `STJ` to 8.0.5 to address a security warning for that package\r\n\r\nThe `CausalLMPipelineChatClient` is used as the abstract class for all `IChatClient` implementation in GenAI models. For now, the following implementations are available\r\n- Phi3CausalLMChatClient\r\n- Llama3CausalLMChatClient\r\n\r\nThe `MistralCausalLMChatClient` will come in the following PR with additional support for tool call.\r\n\r\n## Usage\r\nCheckout the `MEAI` folder under Microsoft.ML.GenAI.Samples` for examples. The following examples are added\r\n- Llama3_1\r\n- Phi3\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7270","RelatedDescription":"Open PR \"[GenAI] Introduce CausalLMPipelineChatClient for MEAI.IChatClient\" (#7270)"},{"Id":"2583158851","IsPullRequest":false,"CreatedAt":"2024-10-12T15:13:59","Actor":"thomasd3","Number":"7269","RawContent":null,"Title":"mlnet finds duplicate columns where they don't exist","State":"open","Body":"I use this command:\n\n```\nmlnet classification --dataset training.csv  --validation-dataset validating.csv --train-time 100 --label-col 0 --ignore-cols 1 2 3 -has-header true --name test\n```\n\nand I get this error:\n```\nStart Training\n[Source=TextLoader; Binding, Kind=Info] Duplicate name(s) specified - later columns will hide earlier ones\nSystem.ArgumentException: An item with the same key has already been added. Key: 22742.3                                                                                                                                                                         at System.Collections.Generic.Dictionary`2.TryInsert(TKey key, TValue value, InsertionBehavior behavior)                                                                                                                                                      at System.Collections.Generic.Dictionary`2.Add(TKey key, TValue value)\n```\n\nBut there are absolutely no duplicate names in the columns.\n\nThis is the (very long) header:\n```\nlabel_score_long_1005,label_score_short_1005,timestamp,interval,high0,high0_h1,high0_p03,high0_p05,high0_p10,high_raw0,high_raw0_h1,high_raw0_p03,high_raw0_p05,high_raw0_p10,low0,low0_h1,low0_p03,low0_p05,low0_p10,low_raw0,low_raw0_h1,low_raw0_p03,low_raw0_p05,low_raw0_p10,close0,close0_h1,close0_p03,close0_p05,close0_p10,close_raw0,close_raw0_h1,close_raw0_p03,close_raw0_p05,close_raw0_p10,volume0,volume0_h1,volume0_p03,volume0_p05,volume0_p10,volume_raw0,volume_raw0_h1,volume_raw0_p03,volume_raw0_p05,volume_raw0_p10,volume_ema0,volume_ema0_h1,volume_ema0_p03,volume_ema0_p05,volume_ema0_p10,adx0,adx0_h1,adx0_p03,adx0_p05,adx0_p10,adx_r0,adx_r0_h1,adx_r0_p03,adx_r0_p05,adx_r0_p10,adx_mdi0,adx_mdi0_h1,adx_mdi0_p03,adx_mdi0_p05,adx_mdi0_p10,adx_pdi0,adx_pdi0_h1,adx_pdi0_p03,adx_pdi0_p05,adx_pdi0_p10,adl_money_flow_multiplier0,adl_money_flow_multiplier0_h1,adl_money_flow_multiplier0_p03,adl_money_flow_multiplier0_p05,adl_money_flow_multiplier0_p10,ao0,ao0_h1,ao0_p03,ao0_p05,ao0_p10,aroon_down0,aroon_down0_h1,aroon_down0_p03,aroon_down0_p05,aroon_down0_p10,aroon_up0,aroon_up0_h1,aroon_up0_p03,aroon_up0_p05,aroon_up0_p10,aroon_oscillator0,aroon_oscillator0_h1,aroon_oscillator0_p03,aroon_oscillator0_p05,aroon_oscillator0_p10,atrp0,atrp0_h1,atrp0_p03,atrp0_p05,atrp0_p10,atr_buystop0,atr_buystop0_h1,atr_buystop0_p03,atr_buystop0_p05,atr_buystop0_p10,atr_sellstop0,atr_sellstop0_h1,atr_sellstop0_p03,atr_sellstop0_p05,atr_sellstop0_p10,bop0,bop0_h1,bop0_p03,bop0_p05,bop0_p10,cci0,cci0_h1,cci0_p03,cci0_p05,cci0_p10,chaikinmoney_flow_multiplier0,chaikinmoney_flow_multiplier0_h1,chaikinmoney_flow_multiplier0_p03,chaikinmoney_flow_multiplier0_p05,chaikinmoney_flow_multiplier0_p10,chand_exit0,chand_exit0_h1,chand_exit0_p03,chand_exit0_p05,chand_exit0_p10,choppiness0,choppiness0_h1,choppiness0_p03,choppiness0_p05,choppiness0_p10,cmf0,cmf0_h1,cmf0_p03,cmf0_p05,cmf0_p10,cmf_money_flow_multiplier0,cmf_money_flow_multiplier0_h1,cmf_money_flow_multiplier0_p03,cmf_money_flow_multiplier0_p05,cmf_money_flow_multiplier0_p10,cmo0,cmo0_h1,cmo0_p03,cmo0_p05,cmo0_p10,connor_rank0,connor_rank0_h1,connor_rank0_p03,connor_rank0_p05,connor_rank0_p10,connor_rsi0,connor_rsi0_h1,connor_rsi0_p03,connor_rsi0_p05,connor_rsi0_p10,connor_rsi_streak0,connor_rsi_streak0_h1,connor_rsi_streak0_p03,connor_rsi_streak0_p05,connor_rsi_streak0_p10,elderray_bearpower0,elderray_bearpower0_h1,elderray_bearpower0_p03,elderray_bearpower0_p05,elderray_bearpower0_p10,elderray_bullpower0,elderray_bullpower0_h1,elderray_bullpower0_p03,elderray_bullpower0_p05,elderray_bullpower0_p10,gator_lower0,gator_lower0_h1,gator_lower0_p03,gator_lower0_p05,gator_lower0_p10,gator_lower_is_expanding0,gator_lower_is_expanding0_h1,gator_lower_is_expanding0_p03,gator_lower_is_expanding0_p05,gator_lower_is_expanding0_p10,gator_upper0,gator_upper0_h1,gator_upper0_p03,gator_upper0_p05,gator_upper0_p10,gator_upper_is_expanding0,gator_upper_is_expanding0_h1,gator_upper_is_expanding0_p03,gator_upper_is_expanding0_p05,gator_upper_is_expanding0_p10,hurst_exponent0,hurst_exponent0_h1,hurst_exponent0_p03,hurst_exponent0_p05,hurst_exponent0_p10,hvz_score0,hvz_score0_h1,hvz_score0_p03,hvz_score0_p05,hvz_score0_p10,ichimoku_kijunsen0,ichimoku_kijunsen0_h1,ichimoku_kijunsen0_p03,ichimoku_kijunsen0_p05,ichimoku_kijunsen0_p10,ichimoku_senkou_span_a0,ichimoku_senkou_span_a0_h1,ichimoku_senkou_span_a0_p03,ichimoku_senkou_span_a0_p05,ichimoku_senkou_span_a0_p10,ichimoku_senkou_span_b0,ichimoku_senkou_span_b0_h1,ichimoku_senkou_span_b0_p03,ichimoku_senkou_span_b0_p05,ichimoku_senkou_span_b0_p10,ichimoku_tenkansen0,ichimoku_tenkansen0_h1,ichimoku_tenkansen0_p03,ichimoku_tenkansen0_p05,ichimoku_tenkansen0_p10,macd0,macd0_h1,macd0_p03,macd0_p05,macd0_p10,marubozu_match0,marubozu_match0_h1,marubozu_match0_p03,marubozu_match0_p05,marubozu_match0_p10,mfi0,mfi0_h1,mfi0_p03,mfi0_p05,mfi0_p10,parabolic_sar0,parabolic_sar0_h1,parabolic_sar0_p03,parabolic_sar0_p05,parabolic_sar0_p10,pivots_high_line0,pivots_high_line0_h1,pivots_high_line0_p03,pivots_high_line0_p05,pivots_high_line0_p10,pivots_high_trend0,pivots_high_trend0_h1,pivots_high_trend0_p03,pivots_high_trend0_p05,pivots_high_trend0_p10,pivots_low_line0,pivots_low_line0_h1,pivots_low_line0_p03,pivots_low_line0_p05,pivots_low_line0_p10,pivots_low_trend0,pivots_low_trend0_h1,pivots_low_trend0_p03,pivots_low_trend0_p05,pivots_low_trend0_p10,pmo0,pmo0_h1,pmo0_p03,pmo0_p05,pmo0_p10,pmo_signal0,pmo_signal0_h1,pmo_signal0_p03,pmo_signal0_p05,pmo_signal0_p10,pvo0,pvo0_h1,pvo0_p03,pvo0_p05,pvo0_p10,roc0,roc0_h1,roc0_p03,roc0_p05,roc0_p10,rsi0,rsi0_h1,rsi0_p03,rsi0_p05,rsi0_p10,schaff_stc0,schaff_stc0_h1,schaff_stc0_p03,schaff_stc0_p05,schaff_stc0_p10,smi0,smi0_h1,smi0_p03,smi0_p05,smi0_p10,srsi_signal0,srsi_signal0_h1,srsi_signal0_p03,srsi_signal0_p05,srsi_signal0_p10,srsi_stoch_rsi0,srsi_stoch_rsi0_h1,srsi_stoch_rsi0_p03,srsi_stoch_rsi0_p05,srsi_stoch_rsi0_p10,stoch_a0,stoch_a0_h1,stoch_a0_p03,stoch_a0_p05,stoch_a0_p10,stoch_a_zone0,stoch_a_zone0_h1,stoch_a_zone0_p03,stoch_a_zone0_p05,stoch_a_zone0_p10,stoch_a_cross_top0,stoch_a_cross_top0_h1,stoch_a_cross_top0_p03,stoch_a_cross_top0_p05,stoch_a_cross_top0_p10,stoch_a_cross_bottom0,stoch_a_cross_bottom0_h1,stoch_a_cross_bottom0_p03,stoch_a_cross_bottom0_p05,stoch_a_cross_bottom0_p10,stoch_b0,stoch_b0_h1,stoch_b0_p03,stoch_b0_p05,stoch_b0_p10,stoch_b_zone0,stoch_b_zone0_h1,stoch_b_zone0_p03,stoch_b_zone0_p05,stoch_b_zone0_p10,stoch_b_cross_top0,stoch_b_cross_top0_h1,stoch_b_cross_top0_p03,stoch_b_cross_top0_p05,stoch_b_cross_top0_p10,stoch_b_cross_bottom0,stoch_b_cross_bottom0_h1,stoch_b_cross_bottom0_p03,stoch_b_cross_bottom0_p05,stoch_b_cross_bottom0_p10,stoch_c0,stoch_c0_h1,stoch_c0_p03,stoch_c0_p05,stoch_c0_p10,stoch_c_zone0,stoch_c_zone0_h1,stoch_c_zone0_p03,stoch_c_zone0_p05,stoch_c_zone0_p10,stoch_c_cross_top0,stoch_c_cross_top0_h1,stoch_c_cross_top0_p03,stoch_c_cross_top0_p05,stoch_c_cross_top0_p10,stoch_c_cross_bottom0,stoch_c_cross_bottom0_h1,stoch_c_cross_bottom0_p03,stoch_c_cross_bottom0_p05,stoch_c_cross_bottom0_p10,stoch_d0,stoch_d0_h1,stoch_d0_p03,stoch_d0_p05,stoch_d0_p10,stoch_d_zone0,stoch_d_zone0_h1,stoch_d_zone0_p03,stoch_d_zone0_p05,stoch_d_zone0_p10,stoch_d_cross_top0,stoch_d_cross_top0_h1,stoch_d_cross_top0_p03,stoch_d_cross_top0_p05,stoch_d_cross_top0_p10,stoch_d_cross_bottom0,stoch_d_cross_bottom0_h1,stoch_d_cross_bottom0_p03,stoch_d_cross_bottom0_p05,stoch_d_cross_bottom0_p10,supertrend_lower0,supertrend_lower0_h1,supertrend_lower0_p03,supertrend_lower0_p05,supertrend_lower0_p10,supertrend_upper0,supertrend_upper0_h1,supertrend_upper0_p03,supertrend_upper0_p05,supertrend_upper0_p10,tsi_signal0,tsi_signal0_h1,tsi_signal0_p03,tsi_signal0_p05,tsi_signal0_p10,tsi0,tsi0_h1,tsi0_p03,tsi0_p05,tsi0_p10,ulcer_index0,ulcer_index0_h1,ulcer_index0_p03,ulcer_index0_p05,ulcer_index0_p10,uo_ultimate0,uo_ultimate0_h1,uo_ultimate0_p03,uo_ultimate0_p05,uo_ultimate0_p10,volstop_lower0,volstop_lower0_h1,volstop_lower0_p03,volstop_lower0_p05,volstop_lower0_p10,volstop_upper0,volstop_upper0_h1,volstop_upper0_p03,volstop_upper0_p05,volstop_upper0_p10,vortex_nvi0,vortex_nvi0_h1,vortex_nvi0_p03,vortex_nvi0_p05,vortex_nvi0_p10,vortex_pvi0,vortex_pvi0_h1,vortex_pvi0_p03,vortex_pvi0_p05,vortex_pvi0_p10,willfract_bear0,willfract_bear0_h1,willfract_bear0_p03,willfract_bear0_p05,willfract_bear0_p10,willfract_bull0,willfract_bull0_h1,willfract_bull0_p03,willfract_bull0_p05,willfract_bull0_p10,tgn_tunnel_high0,tgn_tunnel_high0_h1,tgn_tunnel_high0_p03,tgn_tunnel_high0_p05,tgn_tunnel_high0_p10,tgn_tunnel_low0,tgn_tunnel_low0_h1,tgn_tunnel_low0_p03,tgn_tunnel_low0_p05,tgn_tunnel_low0_p10,tgn_tunnel_high_price0,tgn_tunnel_high_price0_h1,tgn_tunnel_high_price0_p03,tgn_tunnel_high_price0_p05,tgn_tunnel_high_price0_p10,tgn_tunnel_low_price0,tgn_tunnel_low_price0_h1,tgn_tunnel_low_price0_p03,tgn_tunnel_low_price0_p05,tgn_tunnel_low_price0_p10,tgn_tunnel_high_price_change0,tgn_tunnel_high_price_change0_h1,tgn_tunnel_high_price_change0_p03,tgn_tunnel_high_price_change0_p05,tgn_tunnel_high_price_change0_p10,tgn_tunnel_low_price_change0,tgn_tunnel_low_price_change0_h1,tgn_tunnel_low_price_change0_p03,tgn_tunnel_low_price_change0_p05,tgn_tunnel_low_price_change0_p10,tgn_node_last_type0,tgn_node_last_type0_h1,tgn_node_last_type0_p03,tgn_node_last_type0_p05,tgn_node_last_type0_p10,tgn_node_price_hh0,tgn_node_price_hh0_h1,tgn_node_price_hh0_p03,tgn_node_price_hh0_p05,tgn_node_price_hh0_p10,tgn_node_price_lh0,tgn_node_price_lh0_h1,tgn_node_price_lh0_p03,tgn_node_price_lh0_p05,tgn_node_price_lh0_p10,tgn_node_price_hl0,tgn_node_price_hl0_h1,tgn_node_price_hl0_p03,tgn_node_price_hl0_p05,tgn_node_price_hl0_p10,tgn_node_price_ll0,tgn_node_price_ll0_h1,tgn_node_price_ll0_p03,tgn_node_price_ll0_p05,tgn_node_price_ll0_p10,tgn_gap_countAbove0,tgn_gap_countAbove0_h1,tgn_gap_countAbove0_p03,tgn_gap_countAbove0_p05,tgn_gap_countAbove0_p10,tgn_gap_countBelow0,tgn_gap_countBelow0_h1,tgn_gap_countBelow0_p03,tgn_gap_countBelow0_p05,tgn_gap_countBelow0_p10,tgn_gap_distanceClosestAbove0,tgn_gap_distanceClosestAbove0_h1,tgn_gap_distanceClosestAbove0_p03,tgn_gap_distanceClosestAbove0_p05,tgn_gap_distanceClosestAbove0_p10,tgn_gap_distanceClosestBelow0,tgn_gap_distanceClosestBelow0_h1,tgn_gap_distanceClosestBelow0_p03,tgn_gap_distanceClosestBelow0_p05,tgn_gap_distanceClosestBelow0_p10,tgn_gap_sizeClosestAbove0,tgn_gap_sizeClosestAbove0_h1,tgn_gap_sizeClosestAbove0_p03,tgn_gap_sizeClosestAbove0_p05,tgn_gap_sizeClosestAbove0_p10,tgn_gap_sizeClosestBelow0,tgn_gap_sizeClosestBelow0_h1,tgn_gap_sizeClosestBelow0_p03,tgn_gap_sizeClosestBelow0_p05,tgn_gap_sizeClosestBelow0_p10,tgn_gap_directionClosestAbove0,tgn_gap_directionClosestAbove0_h1,tgn_gap_directionClosestAbove0_p03,tgn_gap_directionClosestAbove0_p05,tgn_gap_directionClosestAbove0_p10,tgn_gap_directionClosestBelow0,tgn_gap_directionClosestBelow0_h1,tgn_gap_directionClosestBelow0_p03,tgn_gap_directionClosestBelow0_p05,tgn_gap_directionClosestBelow0_p10,tgn_gap_avgDistanceAbove0,tgn_gap_avgDistanceAbove0_h1,tgn_gap_avgDistanceAbove0_p03,tgn_gap_avgDistanceAbove0_p05,tgn_gap_avgDistanceAbove0_p10,tgn_gap_avgDistanceBelow0,tgn_gap_avgDistanceBelow0_h1,tgn_gap_avgDistanceBelow0_p03,tgn_gap_avgDistanceBelow0_p05,tgn_gap_avgDistanceBelow0_p10,tgn_LL_countAbove0,tgn_LL_countAbove0_h1,tgn_LL_countAbove0_p03,tgn_LL_countAbove0_p05,tgn_LL_countAbove0_p10,tgn_LL_countBelow0,tgn_LL_countBelow0_h1,tgn_LL_countBelow0_p03,tgn_LL_countBelow0_p05,tgn_LL_countBelow0_p10,tgn_ll_distanceClosestAbove0,tgn_ll_distanceClosestAbove0_h1,tgn_ll_distanceClosestAbove0_p03,tgn_ll_distanceClosestAbove0_p05,tgn_ll_distanceClosestAbove0_p10,tgn_ll_distanceClosestBelow0,tgn_ll_distanceClosestBelow0_h1,tgn_ll_distanceClosestBelow0_p03,tgn_ll_distanceClosestBelow0_p05,tgn_ll_distanceClosestBelow0_p10,tgn_ll_directionClosestAbove0,tgn_ll_directionClosestAbove0_h1,tgn_ll_directionClosestAbove0_p03,tgn_ll_directionClosestAbove0_p05,tgn_ll_directionClosestAbove0_p10,tgn_ll_directionClosestBelow0,tgn_ll_directionClosestBelow0_h1,tgn_ll_directionClosestBelow0_p03,tgn_ll_directionClosestBelow0_p05,tgn_ll_directionClosestBelow0_p10,tgn_ll_avgDistanceAbove0,tgn_ll_avgDistanceAbove0_h1,tgn_ll_avgDistanceAbove0_p03,tgn_ll_avgDistanceAbove0_p05,tgn_ll_avgDistanceAbove0_p10,tgn_ll_avgDistanceBelow0,tgn_ll_avgDistanceBelow0_h1,tgn_ll_avgDistanceBelow0_p03,tgn_ll_avgDistanceBelow0_p05,tgn_ll_avgDistanceBelow0_p10,tgn_coc_last_distance0,tgn_coc_last_distance0_h1,tgn_coc_last_distance0_p03,tgn_coc_last_distance0_p05,tgn_coc_last_distance0_p10,tgn_coc_last_direction0,tgn_coc_last_direction0_h1,tgn_coc_last_direction0_p03,tgn_coc_last_direction0_p05,tgn_coc_last_direction0_p10,tgn_coc_count_last_5_hours0,tgn_coc_count_last_5_hours0_h1,tgn_coc_count_last_5_hours0_p03,tgn_coc_count_last_5_hours0_p05,tgn_coc_count_last_5_hours0_p10,tgn_coc_count_last_10_hours0,tgn_coc_count_last_10_hours0_h1,tgn_coc_count_last_10_hours0_p03,tgn_coc_count_last_10_hours0_p05,tgn_coc_count_last_10_hours0_p10,tgn_coc_time_since_last_event0,tgn_coc_time_since_last_event0_h1,tgn_coc_time_since_last_event0_p03,tgn_coc_time_since_last_event0_p05,tgn_coc_time_since_last_event0_p10,tgn_coc_avg_interval_dist_between0,tgn_coc_avg_interval_dist_between0_h1,tgn_coc_avg_interval_dist_between0_p03,tgn_coc_avg_interval_dist_between0_p05,tgn_coc_avg_interval_dist_between0_p10,tgn_coc_avg_price_dist_between0,tgn_coc_avg_price_dist_between0_h1,tgn_coc_avg_price_dist_between0_p03,tgn_coc_avg_price_dist_between0_p05,tgn_coc_avg_price_dist_between0_p10,tgn_coc_long_count0,tgn_coc_long_count0_h1,tgn_coc_long_count0_p03,tgn_coc_long_count0_p05,tgn_coc_long_count0_p10,tgn_coc_short_count0,tgn_coc_short_count0_h1,tgn_coc_short_count0_p03,tgn_coc_short_count0_p05,tgn_coc_short_count0_p10,tgn_mss_last_distance0,tgn_mss_last_distance0_h1,tgn_mss_last_distance0_p03,tgn_mss_last_distance0_p05,tgn_mss_last_distance0_p10,tgn_mss_last_direction0,tgn_mss_last_direction0_h1,tgn_mss_last_direction0_p03,tgn_mss_last_direction0_p05,tgn_mss_last_direction0_p10,tgn_mss_count_last_5_hours0,tgn_mss_count_last_5_hours0_h1,tgn_mss_count_last_5_hours0_p03,tgn_mss_count_last_5_hours0_p05,tgn_mss_count_last_5_hours0_p10,tgn_mss_count_last_10_hours0,tgn_mss_count_last_10_hours0_h1,tgn_mss_count_last_10_hours0_p03,tgn_mss_count_last_10_hours0_p05,tgn_mss_count_last_10_hours0_p10,tgn_mss_time_since_last_event0,tgn_mss_time_since_last_event0_h1,tgn_mss_time_since_last_event0_p03,tgn_mss_time_since_last_event0_p05,tgn_mss_time_since_last_event0_p10,tgn_mss_avg_interval_dist_between0,tgn_mss_avg_interval_dist_between0_h1,tgn_mss_avg_interval_dist_between0_p03,tgn_mss_avg_interval_dist_between0_p05,tgn_mss_avg_interval_dist_between0_p10,tgn_mss_avg_price_dist_between0,tgn_mss_avg_price_dist_between0_h1,tgn_mss_avg_price_dist_between0_p03,tgn_mss_avg_price_dist_between0_p05,tgn_mss_avg_price_dist_between0_p10,tgn_mss_long_count0,tgn_mss_long_count0_h1,tgn_mss_long_count0_p03,tgn_mss_long_count0_p05,tgn_mss_long_count0_p10,tgn_mss_short_count0,tgn_mss_short_count0_h1,tgn_mss_short_count0_p03,tgn_mss_short_count0_p05,tgn_mss_short_count0_p10,tgn_fair_price0,tgn_fair_price0_h1,tgn_fair_price0_p03,tgn_fair_price0_p05,tgn_fair_price0_p10,tgn_tunnel_breakout0,tgn_tunnel_breakout0_h1,tgn_tunnel_breakout0_p03,tgn_tunnel_breakout0_p05,tgn_tunnel_breakout0_p10,tgn_tunnel_break_from_price0,tgn_tunnel_break_from_price0_h1,tgn_tunnel_break_from_price0_p03,tgn_tunnel_break_from_price0_p05,tgn_tunnel_break_from_price0_p10,tgn_compression_zone_breakout0,tgn_compression_zone_breakout0_h1,tgn_compression_zone_breakout0_p03,tgn_compression_zone_breakout0_p05,tgn_compression_zone_breakout0_p10,tgn_compression_zone_break_from_price0,tgn_compression_zone_break_from_price0_h1,tgn_compression_zone_break_from_price0_p03,tgn_compression_zone_break_from_price0_p05,tgn_compression_zone_break_from_price0_p10,tgn_in_zone0,tgn_in_zone0_h1,tgn_in_zone0_p03,tgn_in_zone0_p05,tgn_in_zone0_p10\n```\n\nwhen I don't specify that the file has a header, I get this error instead:\n\n```\nStart Training\nSystem.Exception: Train dataset and validate dataset schema is not compatible. Column(s) col0, col4, col5, col6, col7, col8, col9, col10, col11, col12, col13, col14, col15, col16, col17, col18, col19, col20, col21, col22, col23, col24, col25, col26, col27, col28, col29, col30, col31, col32, col33, col34, col35, col36, col37, col38, col39, col40, col41, col42, col43, col44, col45, col46, col47, col48, col49, col50, col51, col52, col53, col54, col55, col56, col57, col58, col59, col60, col61, col62, col63, col64, col65, col66, col67, col68, col69, col70, col71, col72, col73, col74, col75, col76, col77, col78, col79, col80, col81, col82, col83, col84, col85, col86, col87, col88, col89, col90, col91, col92, col93, col94, col95, col96, col97, col98, col99, col100, col101, col102, col103, col104, col105, col106, col107, col108, col109, col110, col111, col112, col113, col114, col115, col116, col117, col118, col119, col120, col121, col122, col123, col124, col125, col126, col127, col128, col129, col130, col131, col132, col133, col134, col135, col136, col137, col138, col139, col140, col141, col142, col143, col144, col145, col146, col147, col148, col149, col150, col151, col152, col153, col154, col155, col156, col157, col158, col159, col160, col161, col162, col163, col164, col165, col166, col167, col168, col169, col170, col171, col172, col173, col174, col175, col176, col177, col178, col179, col180, col181, col182, col183, col184, col185, col186, col187, col188, col189, col190, col191, col192, col193, col194, col195, col196, col197, col198, col199, col200, col201, col202, col203, col204, col205, col206, col207, col208, col209, col210, col211, col212, col213, col214, col215, col216, col217, col218, col219, col220, col221, col222, col223, col224, col225, col226, col227, col228, col229, col230, col231, col232, col233, col234, col235, col236, col237, col238, col239, col240, col241, col242, col243, col244, col245, col246, col247, col248, col249, col250, col251, col252, col253, col254, col255, col256, col257, col258, col259, col260, col261, col262, col263, col264, col265, col266, col267, col268, col269, col270, col271, col272, col273, col274, col275, col276, col277, col278, col279, col280, col281, col282, col283, col284, col285, col286, col287, col288, col289, col290, col291, col292, col293, col294, col295, col296, col297, col298, col299, col300, col301, col302, col303, col304, col305, col306, col307, col308, col309, col310, col311, col312, col313, col314, col315, col316, col317, col318, col319, col320, col321, col322, col323, col324, col325, col326, col327, col328, col329, col330, col331, col332, col333, col334, col335, col336, col337, col338, col339, col340, col341, col342, col343, col344, col345, col346, col347, col348, col349, col350, col351, col352, col353, col354, col355, col356, col357, col358, col359, col360, col361, col362, col363, col364, col365, col366, col367, col368, col369, col370, col371, col372, col373, col374, col375, col376, col377, col378, col379, col380, col381, col382, col383, col384, col385, col386, col387, col388, col389, col390, col391, col392, col393, col394, col395, col396, col397, col398, col399, col400, col401, col402, col403, col404, col405, col406, col407, col408, col409, col410, col411, col412, col413, col414, col415, col416, col417, col418, col419, col420, col421, col422, col423, col424, col425, col426, col427, col428, col429, col430, col431, col432, col433, col434, col435, col436, col437, col438, col439, col440, col441, col442, col443, col444, col445, col446, col447, col448, col449, col450, col451, col452, col453, col454, col455, col456, col457, col458, col459, col460, col461, col462, col463, col464, col465, col466, col467, col468, col469, col470, col471, col472, col473, col474, col475, col476, col477, col478, col479, col480, col481, col482, col483, col484, col485, col486, col487, col488, col489, col490, col491, col492, col493, col494, col495, col496, col497, col498, col499, col500, col501, col502, col503, col504, col505, col506, col507, col508, col509, col510, col511, col512, col513, col514, col515, col516, col517, col518, col519, col520, col521, col522, col523, col524, col525, col526, col527, col528, col529, col530, col531, col532, col533, col534, col535, col536, col537, col538, col539, col540, col541, col542, col543, col544, col545, col546, col547, col548, col549, col550, col551, col552, col553, col554, col555, col556, col557, col558, col559, col560, col561, col562, col563, col564, col565, col566, col567, col568, col569, col570, col571, col572, col573, col574, col575, col576, col577, col578, col579, col580, col581, col582, col583, col584, col585, col586, col587, col588, col589, col590, col591, col592, col593, col594, col595, col596, col597, col598, col599, col600, col601, col602, col603, col604, col605, col606, col607, col608, col609, col610, col611, col612, col613, col614, col615, col616, col617, col618, col619, col620, col621, col622, col623, col624, col625, col626, col627, col628, col629, col630, col631, col632, col633, col634, col635, col636, col637, col638, col639, col640, col641, col642, col643, col644, col645, col646, col647, col648, col649, col650, col651, col652, col653, col654, col655, col656, col657, col658, col659, col660, col661, col662, col663, col664, col665, col666, col667, col668, col669, col670, col671, col672, col673, col674, col675, col676, col677, col678, col679, col680, col681, col682, col683, col684, col685, col686, col687, col688, col689, col690, col691, col692, col693, col694, col695, col696, col697, col698, col699, col700, col701, col702, col703, col704, col705, col706, col707, col708, col709, col710, col711, col712, col713 are not found in validate dataset.\n   at Microsoft.ML.ModelBuilder.AutoMLEngine.GetValidateDatasetLoaderOption(ITrainingConfiguration config, IEnumerable`1 validateColumnInfo, Char decimalSeparator, String delimiter, Boolean hasHeader) in /_/src/Microsoft.ML.ModelBuilder.AutoMLService/AutoMLEngineService/AutoMLEngine.cs:line 307\n   at Microsoft.ML.ModelBuilder.AutoMLEngine.GetValidateDatasetTextLoaderOptionAsync(ITrainingConfiguration config) in /_/src/Microsoft.ML.ModelBuilder.AutoMLService/AutoMLEngineService/AutoMLEngine.cs:line 398\n   at Microsoft.ML.ModelBuilder.AutoMLEngine.StartTrainingAsync(ITrainingConfiguration config, PathConfiguration pathConfig, CancellationToken userCancellationToken) in /_/src/Microsoft.ML.ModelBuilder.AutoMLService/AutoMLEngineService/AutoMLEngine.cs:line 106\n\n```\n","Url":"https://github.com/dotnet/machinelearning/issues/7269","RelatedDescription":"Open issue \"mlnet finds duplicate columns where they don't exist\" (#7269)"},{"Id":"2574492091","IsPullRequest":true,"CreatedAt":"2024-10-11T23:06:23","Actor":"tarekgh","Number":"7264","RawContent":null,"Title":"Misc Changes","State":"closed","Body":"The changes here include the following:\r\n\r\n- Support `o1` model labels in the Tiktoken tokenizer\r\n- Replace the usage of the Tuple in the `EncodedToken` with the `Range` and remove the `TorchSharp` `Range/Index` implementation\r\n- Rename the `SentencePieceBpeTokenizer` to allow adding more models to it in the future\r\n- Make the `Tokenizer.Decode` method return a non-nullable string\r\n- Add support for added tokens in the BPE tokenizer\r\n\r\nEvery commit in the PR is representing one of the listed changes. Reviewing every commit separately will make it easier to have a clear understanding of the changes.","Url":"https://github.com/dotnet/machinelearning/pull/7264","RelatedDescription":"Closed or merged PR \"Misc Changes\" (#7264)"},{"Id":"2579529412","IsPullRequest":false,"CreatedAt":"2024-10-10T18:18:51","Actor":"tarekgh","Number":"7268","RawContent":null,"Title":"[Tracking] Clean up item related to Tokenizers","State":"open","Body":"In the PR https://github.com/dotnet/machinelearning/pull/7264 we have added dependency on the `Microsoft.Bcl.Memory` which was not released as GA yet. This issue is tracking the tracking to clean up the following two items:\n\n- `Microsoft.Bcl.Memory` will produce a warning in any project targeting net6. As we still have some projects targeting net6, we had to add the property `<SuppressTfmSupportBuildWarnings>true</SuppressTfmSupportBuildWarnings>` in the projects to avoid the warning. When we do the retargeting/upgrade such projects, we need to remove the added property.\n- As `Microsoft.Bcl.Memory` not released and we are referencing the preview version of this package, we need to change that after the package became GA.","Url":"https://github.com/dotnet/machinelearning/issues/7268","RelatedDescription":"Open issue \"[Tracking] Clean up item related to Tokenizers\" (#7268)"},{"Id":"2579485994","IsPullRequest":true,"CreatedAt":"2024-10-10T17:54:28","Actor":"dotnet-maestro[bot]","Number":"7267","RawContent":null,"Title":"[release/3.0] Update dependencies from dotnet/arcade","State":"open","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:45c6fd49-3a4f-4675-f3da-08dc0c527e17)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: 45c6fd49-3a4f-4675-f3da-08dc0c527e17\r\n- **Build**: 20241016.1\r\n- **Date Produced**: October 16, 2024 4:52:26 PM UTC\r\n- **Commit**: f7fb1fec01b91be69e4dcc5290a0bff3f28e214f\r\n- **Branch**: refs/heads/release/8.0\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 8.0.0-beta.24508.1 to 8.0.0-beta.24516.1][2]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 8.0.0-beta.24508.1 to 8.0.0-beta.24516.1][2]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 8.0.0-beta.24508.1 to 8.0.0-beta.24516.1][2]\r\n  - **Microsoft.DotNet.SignTool**: [from 8.0.0-beta.24508.1 to 8.0.0-beta.24516.1][2]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 8.0.0-beta.24508.1 to 8.0.0-beta.24516.1][2]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 8.0.0-beta.24508.1 to 8.0.0-beta.24516.1][2]\r\n\r\n[2]: https://github.com/dotnet/arcade/compare/e5b13e0543...f7fb1fec01\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:45c6fd49-3a4f-4675-f3da-08dc0c527e17)\r\n\r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7267","RelatedDescription":"Open PR \"[release/3.0] Update dependencies from dotnet/arcade\" (#7267)"},{"Id":"2579455275","IsPullRequest":true,"CreatedAt":"2024-10-10T17:39:15","Actor":"dotnet-maestro[bot]","Number":"7266","RawContent":null,"Title":"[main] Update dependencies from dotnet/arcade","State":"open","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: c692823c-b896-437f-4f57-08dc434cc8f6\r\n- **Build**: 20241027.1\r\n- **Date Produced**: October 28, 2024 3:08:39 AM UTC\r\n- **Commit**: bee0a0f7b1e68e88c63261e127beee2ed06c6d13\r\n- **Branch**: refs/heads/main\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 10.0.0-beta.24504.4 to 10.0.0-beta.24527.1][4]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 10.0.0-beta.24504.4 to 10.0.0-beta.24527.1][4]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 10.0.0-beta.24504.4 to 10.0.0-beta.24527.1][4]\r\n  - **Microsoft.DotNet.SignTool**: [from 10.0.0-beta.24504.4 to 10.0.0-beta.24527.1][4]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 10.0.0-beta.24504.4 to 10.0.0-beta.24527.1][4]\r\n  - **Microsoft.DotNet.XliffTasks**: [from 10.0.0-beta.24504.4 to 10.0.0-beta.24527.1][4]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 10.0.0-beta.24504.4 to 10.0.0-beta.24527.1][4]\r\n\r\n[4]: https://github.com/dotnet/arcade/compare/f209a925b1...bee0a0f7b1\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7266","RelatedDescription":"Open PR \"[main] Update dependencies from dotnet/arcade\" (#7266)"},{"Id":"2577356926","IsPullRequest":false,"CreatedAt":"2024-10-10T02:24:00","Actor":"asyura","Number":"7265","RawContent":null,"Title":"Monotone constraint still not support for LightGBM 3.0.1","State":"open","Body":"I readed this #1651 and #2330, but monotone constraint still not support for LightGBM 3.0.1\n\nany idea?\n\nthanks\n","Url":"https://github.com/dotnet/machinelearning/issues/7265","RelatedDescription":"Open issue \"Monotone constraint still not support for LightGBM 3.0.1\" (#7265)"},{"Id":"2562584478","IsPullRequest":true,"CreatedAt":"2024-10-09T17:07:21","Actor":"michaelgsharp","Number":"7254","RawContent":null,"Title":"Load onnx model from Stream of bytes","State":"closed","Body":"Fixes #6591 by adding an overload API to allow the ONNX model to be passed in as a Stream of bytes.","Url":"https://github.com/dotnet/machinelearning/pull/7254","RelatedDescription":"Closed or merged PR \"Load onnx model from Stream of bytes\" (#7254)"},{"Id":"2560059313","IsPullRequest":true,"CreatedAt":"2024-10-07T21:33:42","Actor":"ericstj","Number":"7253","RawContent":null,"Title":"Update wording in LDA docs","State":"closed","Body":"Update the wording mentioning countries.  This is actually a quote, but it need not mention `countries`  - the actual source of this quote has since been updated.  \r\nhttps://en.wikipedia.org/w/index.php?title=Earth_Day&oldid=824511329 (which was pulled from an old version of earthday.org).\r\n\r\nI didn't bother updating the values in the comment - I'm not sure if they need to be accurate to get the point across.  @michaelgsharp if you think they should be updated please let me know and help me recompute them.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7253","RelatedDescription":"Closed or merged PR \"Update wording in LDA docs\" (#7253)"},{"Id":"2564792599","IsPullRequest":true,"CreatedAt":"2024-10-07T19:39:15","Actor":"stephentoub","Number":"7255","RawContent":null,"Title":"Update tiktoken regexes","State":"closed","Body":"This updates two of the regexes to match the changes made in https://github.com/openai/tiktoken/commit/9f7f69d62d6052dcc2fd54357df6ae9ae2590518.\r\n\r\nOn .NET Core, these changes are mostly nops, as the main thing they're doing is changing some loops to be atomic, and the auto-atomicity logic in the regex optimizer was already noticing that could be done and doing it automatically. On .NET Framework, it's a bigger deal, as those loops will now be atomic where they weren't previously.\r\n\r\nIf nothing else, it keeps the regexes in sync with the reference implementation.","Url":"https://github.com/dotnet/machinelearning/pull/7255","RelatedDescription":"Closed or merged PR \"Update tiktoken regexes\" (#7255)"},{"Id":"2571146644","IsPullRequest":false,"CreatedAt":"2024-10-07T18:22:55","Actor":"luisquintanilla","Number":"7263","RawContent":null,"Title":"Enable custom token counting","State":"open","Body":"Currently, Tokenizer returns counts strictly based on tokens.\r\n\r\nHowever, there are scenarios where library authors may want / need to implement their own custom token counting function. \r\n\r\nOne such scenario is providing token counts for image inputs. In such cases, AI service providers provide an arbitrary way of calculating cost based on a fixed token count. \r\n\r\n| Provider | Cost calculations | Does tokenization |  Link |\r\n| --- | --- | --- | --- |\r\n| OpenAI | Fixed | No | https://platform.openai.com/docs/guides/vision/calculating-costs | \r\n| Claude | Fixed | No | https://docs.anthropic.com/en/docs/build-with-claude/vision#calculate-image-costs |\r\n| Gemini | Fixed | No | https://ai.google.dev/gemini-api/docs/tokens?lang=python#multimodal-tokens | \r\n| Cohere | N/A | N/A | https://docs.cohere.com/docs/tokens | \r\n| Mistral | N/A | N/A | https://docs.mistral.ai/guides/tokenization/#tokens-count | \r\n","Url":"https://github.com/dotnet/machinelearning/issues/7263","RelatedDescription":"Open issue \"Enable custom token counting\" (#7263)"},{"Id":"2568882983","IsPullRequest":true,"CreatedAt":"2024-10-06T20:34:06","Actor":"vthemelis","Number":"7262","RawContent":null,"Title":"Add support for Arrow Timestamp and Date32 in DataFrame","State":"open","Body":"Fixes: https://github.com/dotnet/machinelearning/issues/7260 \r\nFixes: https://github.com/dotnet/machinelearning/issues/7261\r\n\r\nTimestamp contains UTC information so it makes sense to capture it in `DateTimeOffset` rather than `DateTime`.\r\n\r\nAlso, capture `Date32` as a `DateOnly` if we're in .NET 6 (`DateOnly` is missing from .NET standard at this point).\r\n\r\n\r\n\r\n--------------------------------------------------\r\nWe are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [x] There's a descriptive title that will make sense to other developers some time from now. \r\n- [x] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [x] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [x] You have included any necessary tests in the same PR.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7262","RelatedDescription":"Open PR \"Add support for Arrow Timestamp and Date32 in DataFrame\" (#7262)"},{"Id":"2568882915","IsPullRequest":false,"CreatedAt":"2024-10-06T20:33:57","Actor":"vthemelis","Number":"7261","RawContent":null,"Title":"Decode Date32 arrow type in DataFrame","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nAt the moment, it looks like it's not possible to read `RecordBatch`es of `Date32`s:\r\n\r\n```csharp\r\nusing Apache.Arrow;\r\nusing Microsoft.Data.Analysis;\r\n\r\nvar batch = new RecordBatch.Builder()\r\n    .Append(\"DateColumn\", false, new Date32Array.Builder().AppendRange(Enumerable.Repeat(DateTime.Now, 10)).Build())\r\n    .Build();\r\n\r\nDataFrame.FromArrowRecordBatch(batch);\r\n```\r\n\r\ngives:\r\n```\r\nUnhandled exception. System.NotImplementedException: date32\r\n   at Microsoft.Data.Analysis.DataFrame.AppendDataFrameColumnFromArrowArray(Field field, IArrowArray arrowArray, DataFrame ret, String fieldNamePrefix)\r\n   at Microsoft.Data.Analysis.DataFrame.FromArrowRecordBatch(RecordBatch recordBatch)\r\n   at Program.<Main>$(String[] args)\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/7261","RelatedDescription":"Open issue \"Decode Date32 arrow type in DataFrame\" (#7261)"},{"Id":"2568861973","IsPullRequest":false,"CreatedAt":"2024-10-06T19:51:50","Actor":"vthemelis","Number":"7260","RawContent":null,"Title":"Decode and roundtrip Timestamp fields from Arrow into DataFrame.","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nAt the moment, it looks like it's not possible to read `RecordBatch`es of `Timestamp`s:\r\n\r\n```csharp\r\nusing Apache.Arrow;\r\nusing Microsoft.Data.Analysis;\r\n\r\nvar batch = new RecordBatch.Builder()\r\n    .Append(\"TimestampColumn\", false, new TimestampArray.Builder().AppendRange(Enumerable.Repeat(DateTimeOffset.Now, 10)).Build())\r\n    .Build();\r\n\r\nDataFrame.FromArrowRecordBatch(batch);\r\n```\r\n\r\ngives:\r\n```\r\nUnhandled exception. System.NotImplementedException: timestamp\r\n   at Microsoft.Data.Analysis.DataFrame.AppendDataFrameColumnFromArrowArray(Field field, IArrowArray arrowArray, DataFrame ret, String fieldNamePrefix)\r\n   at Microsoft.Data.Analysis.DataFrame.FromArrowRecordBatch(RecordBatch recordBatch)\r\n   at Program.<Main>$(String[] args)\r\n```\r\n\r\n**Describe the solution you'd like**\r\nThe above should pass and read the `RecordBatch` into a `DateTimeOffset` type.\r\n\r\nTried with:\r\n```bash\r\ndotnet add package Microsoft.Data.Analysis --version 0.21.1\r\n```\r\n\r\n\r\n-----------------------------------------------------\r\nWith the current pre-release version\r\n```bash\r\ndotnet add package Microsoft.Data.Analysis --version 0.22.0-preview.24378.1\r\n```\r\n\r\nthe above passes but doesn't roundtrip:\r\n```csharp\r\nusing Apache.Arrow;\r\nusing Microsoft.Data.Analysis;\r\n\r\nvar batch = new RecordBatch.Builder()\r\n    .Append(\"TimestampColumn\", false, new TimestampArray.Builder().AppendRange(Enumerable.Repeat(DateTimeOffset.Now, 10)).Build())\r\n    .Build();\r\n\r\nvar df = DataFrame.FromArrowRecordBatch(batch);\r\n\r\nvar newBatch = df.ToArrowRecordBatches();\r\n\r\nConsole.WriteLine($\"Original datatype: {batch.Schema.GetFieldByIndex(0).DataType}\");\r\nConsole.WriteLine($\"Final datatype: {newBatch.First().Schema.GetFieldByIndex(0).DataType}\");\r\n```\r\n\r\n```\r\n> dotnet run\r\nOriginal datatype: Apache.Arrow.Types.TimestampType\r\nFinal datatype: Apache.Arrow.Types.Date64Type\r\n```\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7260","RelatedDescription":"Open issue \"Decode and roundtrip Timestamp fields from Arrow into DataFrame.\" (#7260)"},{"Id":"2568746330","IsPullRequest":false,"CreatedAt":"2024-10-06T16:33:38","Actor":"superichmann","Number":"7259","RawContent":null,"Title":"sending no seed to mlcontext does not actually randomize stuff","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: windows 10\r\n - ML.NET Version: 4.0.0-preview.24271.1\r\n - .NET Version: 8.0.8\r\n\r\n**Describe the bug**\r\ninitializing mlcontext without any seed (`new MLContext()`) and training on the same data does not actually results in different models created by `Regression.Trainers.FastForest()` or `Regression.Trainers.LightGbm()`.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. initialize mlcontext without a seed.\r\n2. create a model on data with FastForest.\r\n3. initialize a different mlcontext without a seed.\r\n4. create a model on the same exact data with FastForest.\r\n5. Compare the predictions of both models (in my case, I compared 23645 predictions)\r\n\r\nrepeat the process with lgbm.\r\n\r\n**Expected behavior**\r\nAs I see it, first FastForest predictions should be different then the second FastForest predictions. same in lightgbm.\r\n\r\nEven the slightest change in randomness for the bootstrapped dataset selection should end up in different results.\r\n\r\nIt seems like the FastForest or LightGbm under ml.net are not so random.. :{\r\n\r\n**Further Research** **PLZ READ ME TOO**\r\nI played with LightGbm on python and I was able to introduce randomness into it with feat these params:\t\t`'feature_fraction': 0.2,'seed': rand_num`. removing one of them removes also the randomness in the results, see code:\r\n```\r\nimport lightgbm as lgb\r\nimport pandas as pd\r\nimport numpy as np\r\nnp.random.seed(42)\r\nnum_train_samples = 1000\r\nnum_test_samples = 10\r\nnum_features = 10\r\nX = np.random.rand(num_train_samples + num_test_samples, num_features)\r\ny = np.random.uniform(0, 2, num_train_samples + num_test_samples)\r\ny = y + np.random.normal(0, 0.2, num_train_samples + num_test_samples)\r\nX_train, X_test = X[:num_train_samples], X[num_train_samples:]\r\ny_train, y_test = y[:num_train_samples], y[num_train_samples:]\r\nparams1 = {\r\n    'objective': 'regression',\r\n    'verbose': -1,\r\n    'feature_fraction': 0.2,\r\n    'seed': 42\r\n}\r\nparams2 = {\r\n    'objective': 'regression',\r\n    'verbose': -1,\r\n    'feature_fraction': 0.2,\r\n    'seed': 43\r\n}\r\nmodel1 = lgb.train(params1, lgb.Dataset(X_train, y_train), num_boost_round=1000)  # Increase boosting rounds\r\nmodel2 = lgb.train(params2, lgb.Dataset(X_train, y_train), num_boost_round=1000)\r\ny_pred1 = model1.predict(X_test)\r\ny_pred2 = model2.predict(X_test)\r\nresults = pd.DataFrame({'true_value': y_test, 'model1_pred': y_pred1, 'model2_pred': y_pred2})\r\nprint(results)\r\n\r\n```\r\n**WorkArounds**\r\nA workaround for lightgbm would be to add `FeatureFraction` into the params.\r\nsending `Seed` as a part of `FastForestRegressionTrainer.Options` is a workaround for FastForest.","Url":"https://github.com/dotnet/machinelearning/issues/7259","RelatedDescription":"Open issue \"sending no seed to mlcontext does not actually randomize stuff\" (#7259)"},{"Id":"2568599425","IsPullRequest":false,"CreatedAt":"2024-10-06T11:01:48","Actor":"superichmann","Number":"7258","RawContent":null,"Title":"expose LearningRate","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nno\r\n\r\n**Describe the solution you'd like**\r\nexpose LearningRate on TreeOptions and FastForestRegressionTrainer.Options\r\n\r\n**Describe alternatives you've considered**\r\nmove to python\r\n\r\n**Additional context**\r\nmaybe expose all params of lgbm and ff plz?\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7258","RelatedDescription":"Open issue \"expose LearningRate\" (#7258)"},{"Id":"2565205062","IsPullRequest":false,"CreatedAt":"2024-10-04T22:05:20","Actor":"ericstj","Number":"7256","RawContent":null,"Title":"Microsoft.ML.Tokenizers.Tests.TiktokenTests.TestTokenizerUsingExternalVocab failing to download gpt2.tiktoken","State":"closed","Body":"### Build\n\nhttps://dev.azure.com/dnceng-public/public/_build/results?buildId=823668\n\n### Build leg reported\n\nMicrosoft.ML.Tokenizers.Tests.TiktokenTests.TestTokenizerUsingExternalVocab\n\n### Pull Request\n\nhttps://github.com/dotnet/machinelearning/pull/7252\n\n### Known issue core information\n\nFill out the known issue JSON section by following the [step by step documentation on how to create a known issue](https://github.com/dotnet/arcade/blob/main/Documentation/Projects/Build%20Analysis/KnownIssueJsonStepByStep.md#how-to-create-a-known-issue-step-by-step)\r\n\r\n```json\r\n {\r\n    \"ErrorMessage\" : [\"Microsoft.ML.Tokenizers.Tests.TiktokenTests.TestTokenizerUsingExternalVocab\", \"File not found\"],\r\n    \"BuildRetry\": false,\r\n    \"ErrorPattern\": \"\",\r\n    \"ExcludeConsoleLog\": false\r\n }\r\n ```\r\n\r\n @dotnet/dnceng\r\n\r\n<!-- DO NOT DELETE -->\r\n<!-- For internal use only; put release notes here. -->\r\n<!-- For guidance on writing good release notes, please see documentation here: https://dev.azure.com/dnceng/internal/_wiki/wikis/DNCEng%20Services%20Wiki/983/ReleaseNotesGuidance -->\r\n<!-- Additionally, please specify the note category below. -->\r\n### Release Note Category\r\n- [ ] Feature changes/additions \r\n- [ ] Bug fixes\r\n- [x] Internal Infrastructure Improvements\r\n### Release Note Description\r\n\n\n### Additional information about the issue reported\n\nIt looks to me like this file no longer exists:\r\nhttps://github.com/dotnet/machinelearning/blob/be1e428d41b5936903172855f7f30861ca7eb49a/test/Microsoft.ML.Tokenizers.Tests/TitokenTests.cs#L100C136-L100C149\r\n<!-- Known issue validation start -->\r\n ### Known issue validation\r\n**Build: :mag_right:** https://dev.azure.com/dnceng-public/public/_build/results?buildId=823668\r\n**Error message validated:** `[Microsoft.ML.Tokenizers.Tests.TiktokenTests.TestTokenizerUsingExternalVocab File not found`]\r\n**Result validation:** :white_check_mark: Known issue matched with the provided build.\r\n**Validation performed at:** 10/4/2024 12:37:23 AM UTC\r\n<!-- Known issue validation end -->\r\n<!--Known issue error report start -->\r\n\r\n### Report\r\n\r\n|Build|Definition|Test|Pull Request|\r\n|---|---|---|---|\r\n|[823668](https://dev.azure.com/dnceng-public/public/_build/results?buildId=823668)|dotnet/machinelearning|[Microsoft.ML.Tokenizers.Tests.WorkItemExecution](https://dev.azure.com/dnceng-public/public/_build/results?buildId=823668&view=ms.vss-test-web.build-test-results-tab&runId=21398774&resultId=101001)|dotnet/machinelearning#7252|\r\n#### Summary\r\n|24-Hour Hit Count|7-Day Hit Count|1-Month Count|\r\n|---|---|---|\r\n|0|1|1|\r\n<!--Known issue error report end -->","Url":"https://github.com/dotnet/machinelearning/issues/7256","RelatedDescription":"Closed issue \"Microsoft.ML.Tokenizers.Tests.TiktokenTests.TestTokenizerUsingExternalVocab failing to download gpt2.tiktoken\" (#7256)"}],"ResultType":"GitHubIssue"}},"RunOn":"2024-10-31T03:30:18.9851198Z","RunDurationInMilliseconds":478}