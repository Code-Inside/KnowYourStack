{"Data":{"GitHub":{"Issues":[{"Id":"2257816675","IsPullRequest":true,"CreatedAt":"2024-05-02T18:58:56","Actor":"tarekgh","Number":"7139","RawContent":null,"Title":"Introducing CodeGen Tokenizer","State":"closed","Body":"This change is implementing the [CodeGen](https://huggingface.co/Salesforce/codegen-350M-mono/tree/main) which also support the [Phi-2](https://huggingface.co/microsoft/phi-2/tree/main) tokenizer.","Url":"https://github.com/dotnet/machinelearning/pull/7139","RelatedDescription":"Closed or merged PR \"Introducing CodeGen Tokenizer\" (#7139)"},{"Id":"2273370726","IsPullRequest":false,"CreatedAt":"2024-05-01T11:40:39","Actor":"superichmann","Number":"7146","RawContent":null,"Title":"Modify IDataView in AutoML Experiment After Transform and Before Evaluate","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nno\r\n\r\n**Describe the solution you'd like**\r\nAdd the option to modify the idataview (such as in preFeaturizer) but a \"postFeaturizer\" which will transform the idataview after the Transform has occurred on it inside the experiment and before the evaluation metrics are calculated.\r\n\r\n**Describe alternatives you've considered**\r\nCreate my own trial runner, if possible and this feature is not planned in automl please provide me with a start code :]\r\n\r\n**Additional context**\r\nSome use cases require alteration of the idataview based on the Score column which is not present before Transform is called.\r\nAnother solution would be to add the possibility to call a custom evaluate function based on LINQ","Url":"https://github.com/dotnet/machinelearning/issues/7146","RelatedDescription":"Open issue \"Modify IDataView in AutoML Experiment After Transform and Before Evaluate\" (#7146)"},{"Id":"2273166524","IsPullRequest":false,"CreatedAt":"2024-05-01T08:47:48","Actor":"sportbilly21","Number":"7145","RawContent":null,"Title":"Dll version of Microsoft.ML.OnnxRuntime.dll is 0.0.0.0","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: either Windows 10 or 11 \r\n - ML.NET Version: versions 1.7 and 3.0\r\n - .NET Version: .netframework 4.7\r\n\r\n**Describe the bug**\r\nAfter building the solution the version of the Microsoft.ML.OnnxRuntime.dll is 0.0.0.0\r\nThis is not causing any issues if you are running the application through the Visual Studio.\r\nBut we create an installer with Wix for installation in production PC\r\nWhen you install the software in Windows, the OS and wix due to the version of the dll being zero, they think the dll is corrupted and the installation cannot continue as the installer tries to recover the corrupted dll. \r\n\r\nWe have a work around by changing the version of the above dll but from my understanding dlls should not have 0 as a version\r\n\r\n\r\n\r\n\r\n![image](https://github.com/dotnet/machinelearning/assets/60097348/7e596786-3ff8-4750-acf1-51f2a168f3e9)\r\n\r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7145","RelatedDescription":"Open issue \"Dll version of Microsoft.ML.OnnxRuntime.dll is 0.0.0.0\" (#7145)"},{"Id":"2267992526","IsPullRequest":false,"CreatedAt":"2024-04-29T02:12:59","Actor":"tarekgh","Number":"7144","RawContent":null,"Title":"Tokenizers Library Design","State":"open","Body":"LLM tokenizers are a crucial component in Large Language Models (LLMs) like GPT-3 or BERT. They are responsible for the tokenization process, which involves breaking down natural language text into smaller, manageable pieces called tokens. These tokens can be words, characters, sub-words, numbers, or symbols, and they allow the LLM to process and understand the text.\r\n\r\nThis issue presents the APIs proposed for the Microsoft.ML.Tokenizers library, intended for design review. The design introduces an abstract class named `Tokenizer`, which defines the primary interfaces for all supported tokenizers. Additionally, the Tokenizer class includes a factory method for creating various types of tokenizers.\r\n\r\nThe Tokenizer can be optionally configured with normalizers, which are used to normalize the text before processing it. Normalization can take various forms such as uppercasing, lowercasing, [Unicode Normalization](https://www.unicode.org/reports/tr15/), and removing or inserting specific characters from the input text. The normalization feature is optional for the tokenizer, and it is left to the discretion of either the tokenizer or the user to decide whether to utilize any normalizers.\r\n\r\nPre-tokenization is an additional component that the tokenizer can be configured with, aimed at splitting the input text into smaller units prior to processing. While pre-tokenization is also an optional feature, it is commonly utilized in most tokenizers. Many pre-tokenizers employ regex for this purpose.\r\n\r\nThe typical sequence of operations for the Tokenizer involves:\r\n\r\n- Normalizing the input text if a normalizer is configured.\r\n- Pre-tokenizing the input or normalized text to segment it into smaller units.\r\n- Encoding each unit of text, potentially dividing it into smaller tokens and generating string tokens, IDs for the tokens, and/or offsets that map each token to a portion of the input or normalized text.\r\n\r\nTokenizers offer the following functionalities:\r\n\r\n- Encoding the input text into IDs, which can be utilized as input for Language Models. This operation is referred to as `EncodeToIds` in the proposed design.\r\n- Counting the tokens within the input text, aiding in calculating the quota allowed for processing at any given time. This operation is named `CountTokens` in the proposed design.\r\n- Full encoding, providing detailed results such as string tokens, IDs, and offsets mapping the tokens to parts of the input text. This operation is labeled as `Encode` in the proposed design.\r\n- Given a maximum token count, the tokenizer can determine how far into the input text tokens can be produced, either from the beginning or the end. These operations are denoted as `IndexOfTokenCount` and `LastIndexOfTokenCount`.\r\n- Decoding the generated IDs back into text. This operation is named `Decode` in the proposed design.\r\n- Establishing mappings between string tokens and IDs. These operations are termed `MapTokenToId` and `MapIdToToken` in the proposed design.\r\n\r\nTokenizers typically rely on vocabulary files, which are provided to the tokenizer during instantiation. Users commonly pass these vocabularies as either a file or a stream to the tokenizer constructor. Vocabulary files can vary in format, such as JSON, plain text, protobuf, and more. Each tokenizer determines the specific formats of files it can be instantiated with.\r\n\r\n# Usage Example:\r\n\r\n### Create BPE tokenizer using the constructor\r\n\r\n```C#\r\n    Tokenizer tokenizer = new Bpe(vocabStream: vocabStream, , mergesStream: mergesStream, normalizer: null, preTokenizer: WhiteSpace.Instance);\r\n```\r\n\r\n### Create Tiktoken tokenizer using factory method:\r\n\r\n```C#\r\n    Dictionary<string, int> specialTokens = new Dictionary<string, int> { { IMStart, 100264}, { IMEnd, 100265}, };\r\n    Tokenizer tokenizer = Tokenizer.CreateTiktokenForModel(\"gpt-4\", specialTokens);\r\n```\r\n\r\n### Encode to Ids:\r\n\r\n```C#\r\n    IReadOnlyList<int> encoded = tokenizer.EncodeToIds(\"Hello World\");\r\n```\r\n\r\n### Count Tokens\r\n\r\n```C#\r\n    int idsCount = tokenizer.CountTokens(\"Hello World\");\r\n```\r\n\r\n### Ful Encoding:\r\n\r\n```C#\r\n    // APIs return any information related to the input or normalized text will usually out normalizedString which can be null if there is no normalization performed.\r\n    // Token contain the string token, the token ID, and the offset of the token mapped to the input or normalized text.\r\n    IReadOnlyList<Token> result = tokenizer.Encode(text, out string? normalizedString);\r\n```\r\n\r\n### Count tokens up to max token count:\r\n\r\n```C#\r\n    int length = tokenizer.IndexOfTokenCount(text, maxTokenCount: 10, out string? normalizedString, out int tokenCount);\r\n    \r\n    int index = tokenizer.LastIndexOfTokenCount(text, maxTokenCount: 3, out normalizedString, out tokenCount)\r\n```\r\n\r\n### Decoding Ids back to string\r\n\r\n```C#\r\nstring decodedText = tokenizer.Decode(idsArray);\r\n```\r\n\r\n### Map string token to Id and vice versa\r\n\r\n```C#\r\nint? id = tokenizer.MapTokenToId(\"Hello\");\r\n\r\nstring? token = MapIdToToken(tokenId);\r\n```\r\n\r\n# Proposal:\r\n\r\n### Namespace\r\n\r\n```C#\r\nnamespace Microsoft.ML.Tokenizers\r\n```\r\n\r\n### Tokenizer Abstraction\r\n\r\n```C#\r\n    public abstract partial class Tokenizer\r\n    {\r\n        protected Tokenizer() { }\r\n\r\n        public virtual Normalizer? Normalizer { get { throw null; } }\r\n\r\n        public virtual PreTokenizer? PreTokenizer { get { throw null; } }\r\n\r\n        public virtual IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedText, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedText, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual int LastIndexOfTokenCount(string text, int maxTokenCount, out string? processedText, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? processedText, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual string? Decode(IEnumerable<int> ids) { throw null; }\r\n\r\n        public virtual int? MapTokenToId(string token) { throw null; }\r\n        public abstract int? MapTokenToId(ReadOnlySpan<char> token);\r\n\r\n        public abstract string? MapIdToToken(int? id);\r\n\r\n       //\r\n       // Factory methods\r\n       // \r\n\r\n        public static Task<Tokenizer> CreateTiktokenAsync(Stream vocabStream, PreTokenizer? preTokenizer, Normalizer? normalizer, IReadOnlyDictionary<string, int> specialTokens = null, \r\n                                                                                                  int cacheSize = 8192, Threading.CancellationToken cancellationToken = null) { throw null; }\r\n\r\n        public static Task<Tokenizer> CreateTiktokenAsync(string vocabFilePath, PreTokenizer? preTokenizer, Normalizer? normalizer, IReadOnlyDictionary<string, int> specialTokensEncoder = null, \r\n                                                                                                  int cacheSize = 8192, Threading.CancellationToken cancellationToken = null) { throw null; }\r\n\r\n        public static Tokenizer CreateTiktokenForEncoding(string encodingName, IReadOnlyDictionary<string, int> extraSpecialTokens = null, Normalizer? normalizer = null) { throw null; }\r\n\r\n        public static Tokenizer CreateTiktokenForModel(string modelName, IReadOnlyDictionary<string, int> extraSpecialTokens = null, Normalizer? normalizer = null) { throw null; }\r\n\r\n        public static Tokenizer CreateTiktokenForModel(string modelName, Stream vocabStream, IReadOnlyDictionary<string, int> extraSpecialTokens = null, \r\n                                                                                                    int cacheSize = 8192, Normalizer? normalizer = null) { throw null; }\r\n\r\n        public static Task<Tokenizer> CreateTiktokenForModelAsync(string modelName, Stream vocabStream, IReadOnlyDictionary<string, int> extraSpecialTokens = null, \r\n                                                                                                   int cacheSize = 8192, Normalizer? normalizer = null, Threading.CancellationToken cancellationToken = null) { throw null; }\r\n\r\n        public static Tokenizer CreateLlama(Stream modelStream, bool addBeginOfSentence = true, bool addEndOfSentence = false) { throw null; }\r\n\r\n        public static Tokenizer CreateCodeGen(Stream vocabStream, Stream mergesStream, bool addPrefixSpace = false, bool addBeginOfSentence = false, bool addEndOfSentence = false) { throw null; }\r\n\r\n        public static Tokenizer CreatePhi2(Stream vocabStream, Stream mergesStream, bool addPrefixSpace = false, bool addBeginOfSentence = false, bool addEndOfSentence = false) { throw null; }\r\n    }\r\n```\r\n\r\n### Normalization abstraction \r\n\r\n```C#\r\n    public abstract partial class Normalizer\r\n    {\r\n        protected Normalizer() { }\r\n\r\n        public abstract string Normalize(string original);\r\n        public abstract string Normalize(ReadOnlySpan<char> original);\r\n    }\r\n\r\n```\r\n\r\n### Pre-tokenization abstraction \r\n\r\n```C#\r\n    public abstract partial class PreTokenizer\r\n    {\r\n        protected PreTokenizer() { }\r\n\r\n        public abstract IEnumerable<(int, int)> PreTokenize(string text);\r\n        public abstract IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text);\r\n    }\r\n```\r\n\r\n### Token class \r\n\r\n```C#\r\n   // returned from Tokenizer.Encode(...)\r\n   \r\n    public readonly struct Token\r\n    {\r\n        public Token(int id, string value, (int, int) offset) { }\r\n\r\n        public int Id { get { throw null; } }\r\n\r\n        public (int Index, int Length) Offset { get { throw null; } }\r\n\r\n        public string Value { get { throw null; } }\r\n    }\r\n```\r\n\r\n### Concrete Normalizers \r\n\r\n```C#\r\n    public sealed partial class LowerCaseNormalizer : Normalizer\r\n    {\r\n        public override string Normalize(ReadOnlySpan<char> original) { throw null; }\r\n        public override string Normalize(string original) { throw null; }\r\n    }\r\n\r\n    public sealed partial class UpperCaseNormalizer : Normalizer\r\n    {\r\n        public override string Normalize(ReadOnlySpan<char> original) { throw null; }\r\n\r\n        public override string Normalize(string original) { throw null; }\r\n    }\r\n    \r\n    public sealed partial class SentencePieceNormalizer : Normalizer\r\n    {\r\n        public SentencePieceNormalizer(bool removeExtraWhiteSpaces, bool addDummyPrefix, bool escapeWhiteSpaces, bool treatWhitespaceAsSuffix) { }\r\n        public bool AddDummyPrefix { get { throw null; } }\r\n        public bool EscapeWhiteSpaces { get { throw null; } }\r\n        public bool RemoveExtraWhiteSpaces { get { throw null; } }\r\n        public bool TreatWhitespaceAsSuffix { get { throw null; } }\r\n\r\n        public override string Normalize(ReadOnlySpan<char> original) { throw null; }\r\n        public override string Normalize(string original) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Pre-tokenizers\r\n\r\n```C#\r\n    public sealed partial class TiktokenPreTokenizer : PreTokenizer\r\n    {\r\n        public TiktokenPreTokenizer(Text.RegularExpressions.Regex regex, IReadOnlyDictionary<string, int> specialTokensEncoder) { }\r\n\r\n        public override IEnumerable<(int, int)> PreTokenize(string text) { throw null; }\r\n        public override IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text) { throw null; }\r\n    }\r\n\r\n    public sealed partial class WhiteSpace : PreTokenizer\r\n    {\r\n        public static WhiteSpace Instance { get { throw null; } }\r\n\r\n        public override IEnumerable<(int, int)> PreTokenize(string text) { throw null; }\r\n        public override IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text) { throw null; }\r\n    }\r\n\r\n    public sealed partial class RobertaPreTokenizer : PreTokenizer\r\n    {\r\n        public static RobertaPreTokenizer Instance { get { throw null; } }\r\n\r\n        public override IEnumerable<(int, int)> PreTokenize(string text) { throw null; }\r\n        public override IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - Bpe\r\n\r\n```C#\r\n    public sealed partial class Bpe : Tokenizer\r\n    {\r\n        public Bpe(string vocabFile, string? mergesFile, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, string? unknownToken = null, \r\n                           string? continuingSubwordPrefix = null, string? endOfWordSuffix = null, bool? fuseUnknownTokens = false) { }\r\n\r\n        public Bpe(Stream vocabStream, Stream? mergesStream, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, string? unknownToken = null, \r\n                          string? continuingSubwordPrefix = null, string? endOfWordSuffix = null, bool? fuseUnknownTokens = false) { }\r\n\r\n        public string? ContinuingSubwordPrefix { get { throw null; } }\r\n\r\n        public string? EndOfWordSuffix { get { throw null; } }\r\n\r\n        public bool? FuseUnknownTokens { get { throw null; } }\r\n\r\n        public string? UnknownToken { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public string? Decode(IEnumerable<int> ids, bool considerSpecialTokens) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - Tiktoken \r\n\r\n```C#\r\n    public sealed partial class Tiktoken : Tokenizer\r\n    {\r\n        public Tiktoken(Stream vocabStream, PreTokenizer? preTokenizer, IReadOnlyDictionary<string, int> specialTokens = null, Normalizer? normalizer = null, int? cacheSize = 8192) { }\r\n\r\n        public Tiktoken(string vocabFilePath, PreTokenizer? preTokenizer, IReadOnlyDictionary<string, int> specialTokens = null, Normalizer? normalizer = null, int? cacheSize = 8192) { }\r\n\r\n        public IReadOnlyDictionary<int, ReadOnlyMemory<Byte>> Decoder { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<ReadOnlyMemory<Byte>, int> Encoder { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> SpecialTokens { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - EnglishRoberta\r\n\r\n```C#\r\n    public sealed partial class EnglishRoberta : Tokenizer\r\n    {\r\n        public EnglishRoberta(Stream vocabularyStream, Stream mergeStream, Stream highestOccurrenceMappingStream, PreTokenizer? preTokenizer, Normalizer? normalizer, bool filterUnsupportedChars, bool disposeStream) { }\r\n\r\n        public EnglishRoberta(Stream vocabularyStream, Stream mergeStream, Stream highestOccurrenceMappingStream, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, bool filterUnsupportedChars = true) { }\r\n\r\n        public EnglishRoberta(string vocabularyPath, string mergePath, string highestOccurrenceMappingPath, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, bool filterUnsupportedChars = true) { }\r\n\r\n        public bool FilterUnsupportedChars { get { throw null; } }\r\n\r\n        public int PadIndex { get { throw null; } }\r\n\r\n        public int SymbolsCount { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public int AddMaskSymbol(string mask = \"<mask>\") { throw null; }\r\n\r\n        public IReadOnlyList<int> ConvertIdsToOccurrenceRanks(IReadOnlyList<int> ids) { throw null; }\r\n\r\n        public IReadOnlyList<int> ConvertIdsToOccurrenceValues(IReadOnlyList<int> ids) { throw null; }\r\n\r\n        public IReadOnlyList<int> ConvertOccurrenceRanksToIds(IReadOnlyList<int> ranks) { throw null; }\r\n\r\n        public bool IsSupportedChar(char ch) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - CodeGen \r\n\r\n```C#\r\n    public sealed partial class CodeGen : Tokenizer\r\n    {\r\n        public CodeGen(string vocabularyPath, string mergePath, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, IReadOnlyDictionary<string, int> addedTokens = null, \r\n                                     bool? addPrefixSpace = false, bool? addBeginningOfSentence = false, bool? addEndOfSentence = false, string? unknownToken = \"<|endoftext|>\", \r\n                                     string? beginningOfSentenceToken = \"<|endoftext|>\", string? endOfSentenceToken = \"<|endoftext|>\") { }\r\n\r\n        public CodeGen(Stream vocabularyStream, Stream mergeStream, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, IReadOnlyDictionary<string, int> addedTokens = null, \r\n                                    bool? addPrefixSpace = false, bool? addBeginningOfSentence = false, bool? addEndOfSentence = false, string? unknownToken = \"<|endoftext|>\", \r\n                                    string? beginningOfSentenceToken = \"<|endoftext|>\", string? endOfSentenceToken = \"<|endoftext|>\") { }\r\n\r\n        public bool AddBeginningOfSentence { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> AddedTokens { get { throw null; } }\r\n\r\n        public bool AddEndOfSentence { get { throw null; } }\r\n\r\n        public bool AddPrefixSpace { get { throw null; } }\r\n\r\n        public int? BeginningOfSentenceId { get { throw null; } }\r\n\r\n        public string? BeginningOfSentenceToken { get { throw null; } }\r\n\r\n        public int? EndOfSentenceId { get { throw null; } }\r\n\r\n        public string? EndOfSentenceToken { get { throw null; } }\r\n\r\n        public string? UnknownToken { get { throw null; } }\r\n\r\n        public int? UnknownTokenId { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                               out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                               out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int CountTokens(ReadOnlySpan<char> text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int CountTokens(string text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        \r\n        public int IndexOfTokenCount(string text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                               out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int LastIndexOfTokenCount(string text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                 out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        \r\n        public IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                 bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<Token> Encode(string text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                  bool considerPreTokenization = true,  bool considerNormalization = true) { throw null; }\r\n\r\n        public string? Decode(IEnumerable<int> ids, bool hasPrefixSpace, bool considerSpecialTokens) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - SentencePieceBpe\r\n\r\n```C#\r\n    public sealed partial class SentencePieceBpe : Tokenizer\r\n    {\r\n        internal SentencePieceBpe() { }\r\n\r\n        public bool AddBeginningOfSentence { get { throw null; } }\r\n\r\n        public bool AddDummyPrefix { get { throw null; } }\r\n\r\n        public bool AddEndOfSentence { get { throw null; } }\r\n\r\n        public int BeginningOfSentenceId { get { throw null; } }\r\n\r\n        public string BeginningOfSentenceToken { get { throw null; } }\r\n\r\n        public bool ByteFallback { get { throw null; } }\r\n\r\n        public int EndOfSentenceId { get { throw null; } }\r\n\r\n        public string EndOfSentenceToken { get { throw null; } }\r\n\r\n        public bool EscapeWhiteSpaces { get { throw null; } }\r\n\r\n\r\n        public bool TreatWhitespaceAsSuffix { get { throw null; } }\r\n\r\n        public int UnknownId { get { throw null; } }\r\n\r\n        public string UnknownToken { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public int CountTokens(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int CountTokens(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerNormalization, out string? normalizedString, out int textLength, int maxTokenCount = int.MaxValue) { throw null; }\r\n\r\n        public int CountTokens(string text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerNormalization, \r\n                                                            out string? normalizedString, out int textLength, int maxTokenCount = int.MaxValue) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                             out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                             out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int IndexOfTokenCount(string text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                            out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int IndexOfTokenCount(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                             out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, bool addBeginningOfSentence, bool addEndOfSentence, bool considerNormalization, \r\n                                                              out string? normalizedString, out int tokenCount) { throw null; }\r\n\r\n        public IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/7144","RelatedDescription":"Open issue \"Tokenizers Library Design\" (#7144)"},{"Id":"2266915088","IsPullRequest":false,"CreatedAt":"2024-04-27T08:37:12","Actor":"r-Larch","Number":"7143","RawContent":null,"Title":"[Tokenizers] Question regarding performance","State":"open","Body":"Hi, thanks for the effort put into the Microsoft.ML.Tokenizers!\r\n\r\nI'm the author of the last performance improvements in `SharpToken` library.\r\nSince MLTokenizers are faster now than SharpToken I looked into the sources to understand where this performance comes from.\r\n\r\n**Now I have a question (out of curiosity)**\r\n\r\nWhy is it required to copy a `ReadOnlySpan<char>` to a buffer, when the rest of the code just uses `ReadOnlySpan<char>` again?\r\n\r\n**TiktokenPreTokenizer.cs** line: 104\r\nhttps://github.com/dotnet/machinelearning/blob/72cfdf611a510ba0570170a708ddcc1a1928f329/src/Microsoft.ML.Tokenizers/PreTokenizer/TiktokenPreTokenizer.cs#L95-L107\r\n\r\n**PreTokenizer.cs** line: 74\r\nhttps://github.com/dotnet/machinelearning/blob/72cfdf611a510ba0570170a708ddcc1a1928f329/src/Microsoft.ML.Tokenizers/PreTokenizer/PreTokenizer.cs#L43-L54","Url":"https://github.com/dotnet/machinelearning/issues/7143","RelatedDescription":"Open issue \"[Tokenizers] Question regarding performance\" (#7143)"},{"Id":"2266549113","IsPullRequest":true,"CreatedAt":"2024-04-26T21:50:38","Actor":"sevenzees","Number":"7142","RawContent":null,"Title":"Allow developers to supply their own function to infer column data types from data while loading CSVs","State":"open","Body":"Fixes #7141\r\n\r\nCurrently when you use `LoadCsv` or `LoadCsvFromString` without supplying data types for each column, the code will try to guess the data types based on the data in the CSV file. This is good, but the problem is that the default type inference code only considers `bool`, `float`, `DateTime`, and `string` for column types. Sometimes the user may need another data type, such as `int`, `long`, or `double` (see [issue 6347](https://github.com/dotnet/machinelearning/issues/6347) for an example where someone had a problem with the `float` data type that was chosen by default) but not know the structure of the data ahead of time.\r\n\r\nI would like to be able to pass in my own custom type inference logic to override the default `GuessKind` implementation that is given in the library right now. If no custom guess type function is provided to the `LoadCsv` or `LoadCsvFromString` methods, then the code should work the same as it does today.","Url":"https://github.com/dotnet/machinelearning/pull/7142","RelatedDescription":"Open PR \"Allow developers to supply their own function to infer column data types from data while loading CSVs\" (#7142)"},{"Id":"2266545151","IsPullRequest":false,"CreatedAt":"2024-04-26T21:46:25","Actor":"sevenzees","Number":"7141","RawContent":null,"Title":"Allow developers to supply their own function to infer column data types from data while loading CSVs","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nCurrently when you use `LoadCsv` or `LoadCsvFromString` without supplying data types for each column, the code will try to guess the data types based on the data in the CSV file. This is good, but the problem is that the default type inference code only considers `bool`, `float`, `DateTime`, and `string` for column types. Sometimes the user may need another data type, such as `int`, `long`, or `double` (see [issue 6347](https://github.com/dotnet/machinelearning/issues/6347) for an example where someone had a problem with the `float` data type that was chosen by default) but not know the structure of the data ahead of time.\r\n\r\n**Describe the solution you'd like**\r\nI would like to be able to pass in my own custom type inference logic to override the default `GuessKind` implementation that is given in the library right now. If no custom guess type function is provided to the `LoadCsv` or `LoadCsvFromString` methods, then the code should work the same as it does today.\r\n\r\n**Describe alternatives you've considered**\r\nThe alternative is to call `LoadCsv`/`LoadCsvFromString` with `dataTypes` set to an array filled with `typeof(string)` for each column in your data, and then run your logic on the `DataFrame` with all string type columns to convert the columns to the data types that make sense for each column based on the data that is in each column.\r\n\r\n**Additional context**\r\nI already have implemented a fix for this issue that I would like to merge in with a pull request.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7141","RelatedDescription":"Open issue \"Allow developers to supply their own function to infer column data types from data while loading CSVs\" (#7141)"},{"Id":"2259274067","IsPullRequest":false,"CreatedAt":"2024-04-23T16:17:50","Actor":"chrisevans9629","Number":"7140","RawContent":null,"Title":"Get Loss During Training for Visualization (Learning Curve Graph)","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nI need a way to visualize how my model is learning during training, which is a comparison between training loss and test loss.\r\n\r\n**Describe the solution you'd like**\r\nAn event handler that enables the ability to extract loss during training.\r\n\r\n**Describe alternatives you've considered**\r\nRunning the model for x epochs, evaluating the model, then retraining the model in a loop.  This unfortunately does not work for all models, such as LightGbm that can't be retrained.\r\n\r\n```csharp\r\nvar kfold = ctx.BinaryClassification.CrossValidate(training, estimator, param.kfold);\r\nvar bestModel = kfold.OrderByDescending(p => p.Metrics.Accuracy).Select(p => p.Model).First();\r\n\r\nvar testOutput = bestModel.Transform(test);\r\n\r\nvar metrics = ctx.BinaryClassification.Evaluate(testOutput);\r\n\r\n// This code doesn't work as estimator is IEstimator<ITransform> and bestModel is ITransform. Not sure how you would do this...\r\nestimator = bestModel;\r\n```\r\n\r\n**Additional context**\r\nUltimately, I am trying to analyze what the models I'm comparing are actually doing and so far I haven't found any documentation or any straightforward way to do it.\r\n![image](https://github.com/dotnet/machinelearning/assets/33850520/79ca7603-a42b-4746-a8b3-76e7f2c3aa2a)\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7140","RelatedDescription":"Open issue \"Get Loss During Training for Visualization (Learning Curve Graph)\" (#7140)"},{"Id":"2256524274","IsPullRequest":true,"CreatedAt":"2024-04-22T13:19:15","Actor":"dotnet-maestro[bot]","Number":"7138","RawContent":null,"Title":"[main] Update dependencies from dotnet/arcade","State":"open","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: c692823c-b896-437f-4f57-08dc434cc8f6\r\n- **Build**: 20240429.1\r\n- **Date Produced**: April 30, 2024 12:14:55 AM UTC\r\n- **Commit**: be933308b9024d798a9a22c0b8f3c8e3616ffbd8\r\n- **Branch**: refs/heads/main\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24229.1][3]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24229.1][3]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24229.1][3]\r\n  - **Microsoft.DotNet.SignTool**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24229.1][3]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24229.1][3]\r\n  - **Microsoft.DotNet.XliffTasks**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24229.1][3]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24229.1][3]\r\n\r\n[3]: https://github.com/dotnet/arcade/compare/87b015b938...be933308b9\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7138","RelatedDescription":"Open PR \"[main] Update dependencies from dotnet/arcade\" (#7138)"},{"Id":"2256140857","IsPullRequest":false,"CreatedAt":"2024-04-22T10:13:44","Actor":"matnatx","Number":"7137","RawContent":null,"Title":"Specify Categorical Features in LightGBM","State":"open","Body":"I'm trying to use LightGBM algorithm in ML.NET to train a model using a dataset.\r\n\r\nThe dataset has a number of categorical features such as \"gender\", \"race\",  \"country_of_birth\".\r\n\r\nI can convert/encode these categorical data as integers.\r\n\r\nMy question is how I can specify that these features are categorical and not (continuous variables), so that LightGBM can handle them as categorical data.\r\n\r\nI can use OneHotEncoding/OneHotHashEncoding, but I believe it's not the same.\r\n\r\nTIA","Url":"https://github.com/dotnet/machinelearning/issues/7137","RelatedDescription":"Open issue \"Specify Categorical Features in LightGBM\" (#7137)"},{"Id":"2255452625","IsPullRequest":true,"CreatedAt":"2024-04-22T02:31:25","Actor":"feiyun0112","Number":"7136","RawContent":null,"Title":"Accessing data by column after adding columns to a DataFrame returns error data","State":"open","Body":"fix #7135 \r\n\r\nDescribe the bug\r\nAccessing data by column after adding columns to a DataFrame returns error data\r\n\r\n````\r\nvar df = DataFrame.LoadCsvFromString(\"a1,a2\\n1,2\\n3,4\");\r\nvar dc0 = DataFrameColumn.Create(\"a0\", new int[] { 0, 0 });\r\ndf.Columns.Insert(0, dc0);\r\nvar dc1 = df[\"a1\"];\r\nConsole.WriteLine(dc1.ToString());\r\n````\r\n\r\n\r\nThis code expected print: a1: 1 3\r\nBut it print: a0: 0 0","Url":"https://github.com/dotnet/machinelearning/pull/7136","RelatedDescription":"Open PR \"Accessing data by column after adding columns to a DataFrame returns error data\" (#7136)"},{"Id":"2254553728","IsPullRequest":false,"CreatedAt":"2024-04-21T12:33:49","Actor":"CodingOctocat","Number":"7134","RawContent":null,"Title":"How to predict text type based on input text?","State":"closed","Body":"I'm a newbie and I read the tutorial [github-issue-classification](https://learn.microsoft.com/en-us/dotnet/machine-learning/tutorials/github-issue-classification), but I don't quite understand it, and it seems that my question should be `classification`.\r\n\r\nMy specific requirement is to develop a database query function for a single text box (search box), where the program automatically predicts that the input keyword belongs to a certain column of data in the database and filters the query against it. For example:\r\n\r\n- text box input: _KS 25-3LM_, the program should be predicted to be: **Model**\r\n- text box input: _A543148543143_, the program should be predicted to be: **CertificateNumber**\r\n\r\nHere is my data model where `ProductCategory`, `ProductName`, `Models`, `Enterprise`, `CertificateNumber`, `ReportNumbers` are the fields that I need to involve in the training, note that `Models`, `ReportNumbers` are list types. ReportNumbers` are list types.\r\n\r\nMy database uses Ef Core/SQLite for storage and has a record size of about 150,000 rows.\r\n\r\nCan you all help me to implement this feature? I want to learn and understand ML.NET with this example.\r\n\r\n```csharp\r\n[Table(\"FooSet\")]\r\n[Index(nameof(CertificateNumber), nameof(ReportNumbers), nameof(Models), nameof(Enterprise), nameof(ProductName))]\r\npublic partial record Foo\r\n{\r\n    private string _productCategory = \"\";\r\n\r\n    [Column(\"cert_exp\")]\r\n    public DateOnly CertificateExpires { get; set; }\r\n\r\n    [Column(\"cert_iss\")]\r\n    public DateOnly CertificateIssue { get; set; }\r\n\r\n    [Key]\r\n    [Column(\"cert_no\")]\r\n    public string CertificateNumber { get; set; }\r\n\r\n    [Column(\"ent\")]\r\n    public string Enterprise { get; set; }\r\n\r\n    [Column(\"models\")]\r\n    public List<string> Models { get; set; }\r\n\r\n    [Column(\"cat\")]\r\n    public string ProductCategory\r\n    {\r\n        get => _productCategory ?? \"\";\r\n        set => _productCategory = value ?? \"\";\r\n    }\r\n\r\n    [Column(\"name\")]\r\n    public string ProductName { get; set; }\r\n\r\n    [Column(\"rpt_nos\")]\r\n    public List<string> ReportNumbers { get; set; }\r\n\r\n    [Column(\"status\")]\r\n    public string Status { get; set; }\r\n\r\n    public Foo(string productCategory, string productName, List<string> models, string enterprise,\r\n        string certificateNumber, List<string> reportNumbers,\r\n        DateOnly certificateIssue, DateOnly certificateExpires, string status)\r\n    {\r\n        ProductCategory = productCategory;\r\n        ProductName = productName;\r\n        Models = models;\r\n        Enterprise = enterprise;\r\n        CertificateNumber = certificateNumber;\r\n        ReportNumbers = reportNumbers;\r\n\r\n        CertificateIssue = certificateIssue;\r\n        CertificateExpires = certificateExpires;\r\n        Status = status;\r\n    }\r\n}\r\n```\r\n\r\nrefer link: [machinelearning-samples/issues/1028](https://github.com/dotnet/machinelearning-samples/issues/1028)","Url":"https://github.com/dotnet/machinelearning/issues/7134","RelatedDescription":"Closed issue \"How to predict text type based on input text?\" (#7134)"},{"Id":"2254864513","IsPullRequest":false,"CreatedAt":"2024-04-21T02:26:35","Actor":"wildwind2000","Number":"7135","RawContent":null,"Title":"Accessing data by column after adding columns to a DataFrame returns error  data","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: Windows 11\r\n - ML.NET Version: 3.01\r\n - .NET Version: .NET 8.0\r\n\r\n**Describe the bug**\r\nAccessing data by column after adding columns to a DataFrame returns error data\r\n\r\n```code\r\nvar df = DataFrame.LoadCsvFromString(\"a1,a2\\n1,2\\n3,4\");\r\nvar dc0 = DataFrameColumn.Create(\"a0\", new int[] { 0, 0 });\r\ndf.Columns.Insert(0, dc0);\r\nvar dc1 = df[\"a1\"];\r\nConsole.WriteLine(dc1.ToString());\r\n```\r\nThis code expected print: a1: 1 3   \r\nBut it print: a0: 0 0\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7135","RelatedDescription":"Open issue \"Accessing data by column after adding columns to a DataFrame returns error  data\" (#7135)"},{"Id":"2247028859","IsPullRequest":false,"CreatedAt":"2024-04-19T22:56:52","Actor":"muhamedkarajic","Number":"7130","RawContent":null,"Title":"ML.NET can't add Evaluate logic into pipeline","State":"closed","Body":"**System Information (please complete the following information):**\r\n - OS & Version: macOS Monterey 12.6.6\r\n - ML.NET Version: ML.NET 3.0.1 (Had it also with 2.X)\r\n - .NET Version: .NET 6.0\r\n\r\n**Describe the bug**\r\nI want to split the training and test set and evaluate the model. Therefor I have created a function:\r\n```csharp\r\nprivate static void EvaluateModel(MLContext mlContext, ITransformer trainedModel, IDataView testData)\r\n{\r\n    var predictedData = trainedModel.Transform(testData);\r\n\r\n    var metrics = mlContext.BinaryClassification.EvaluateNonCalibrated(predictedData, \"target\", \"Score\", \"PredictedLabel\");\r\n\r\n    Console.WriteLine($\"Accurecy: {metrics.Accuracy: 0.###}\");\r\n    Console.WriteLine($\"---------------------------\");\r\n    Console.WriteLine($\"Confusion Matrix\");\r\n    Console.WriteLine(metrics.ConfusionMatrix.GetFormattedConfusionTable());\r\n    Console.WriteLine();\r\n}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run `mlnet classification --dataset \"./FILE_PATH/FILE_NAME.csv\" --label-col 11 --has-header true --train-time 60`\r\n2. Add to `SampleClassification.training.cs` the function `EvaluateModel`\r\n3. Adjust the already existing `Train` function by\r\n4. run `dotnet run`\r\n5. Error: `An unhandled exception of type 'System.ArgumentOutOfRangeException' occurred in Microsoft.ML.Data.dll: 'Schema mismatch for score column 'Score': expected Single, got Vector<Single, 2>'`\r\n\r\n**Expected behavior**\r\nI would expect the code to work since the Score is something which ML.NET creates. **It seems like it expects the score to be a sinle value while its a compex vector.**\r\n\r\n**Screenshots, Code, Sample Projects**\r\n\r\nHere is the adjusted train function:\r\n```csharp\r\npublic static void Train(string outputModelPath, string inputDataFilePath = RetrainFilePath, char separatorChar = RetrainSeparatorChar, bool hasHeader = RetrainHasHeader)\r\n{\r\n    var mlContext = new MLContext();\r\n\r\n    var data = LoadIDataViewFromFile(mlContext, inputDataFilePath, separatorChar, hasHeader);\r\n    var splitedData = mlContext.Data.TrainTestSplit(data, 0.2, null, 0);\r\n    var model = RetrainModel(mlContext, splitedData.TrainSet);\r\n    EvaluateModel(mlContext, model, data);\r\n    SaveModel(mlContext, model, data, outputModelPath);\r\n}\r\n```\r\n\r\n\r\n**Additional context**\r\nI ahve found that I am supposed to use`EvaluateNonCalibrated` instead of `Evaluate`. Have similar when using `Evaluate` its says that its missing Predictions. Error in that case `Probability column 'Probability' not found (Parameter 'schema')`. \r\n","Url":"https://github.com/dotnet/machinelearning/issues/7130","RelatedDescription":"Closed issue \"ML.NET can't add Evaluate logic into pipeline\" (#7130)"},{"Id":"2242316679","IsPullRequest":true,"CreatedAt":"2024-04-19T16:21:27","Actor":"tarekgh","Number":"7128","RawContent":null,"Title":"Tokenizer's APIs Update","State":"closed","Body":"Updating the Tokenizer's APIs:\r\n- Simplifying the APIs by merging the `Model` abstraction into the `Tokenizer` abstracted class.\r\n- Adding overloads to work with spans.\r\n- doing some clean up optimization and adding more tests.","Url":"https://github.com/dotnet/machinelearning/pull/7128","RelatedDescription":"Closed or merged PR \"Tokenizer's APIs Update\" (#7128)"},{"Id":"2249443247","IsPullRequest":true,"CreatedAt":"2024-04-18T00:29:47","Actor":"RussKie","Number":"7133","RawContent":null,"Title":"Update locker.yml","State":"closed","Body":"* Bump the Locker action version\r\nRefer to https://github.com/microsoft/vscode-github-triage-actions/pull/210\r\n\r\n* Restrict the Locker action to dotnet org","Url":"https://github.com/dotnet/machinelearning/pull/7133","RelatedDescription":"Closed or merged PR \"Update locker.yml\" (#7133)"},{"Id":"2248568194","IsPullRequest":true,"CreatedAt":"2024-04-18T00:10:16","Actor":"directhex","Number":"7132","RawContent":null,"Title":"[release/3.0] Don't use deprecated -pt images","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/7132","RelatedDescription":"Closed or merged PR \"[release/3.0] Don't use deprecated -pt images\" (#7132)"},{"Id":"2248565494","IsPullRequest":true,"CreatedAt":"2024-04-18T00:09:42","Actor":"directhex","Number":"7131","RawContent":null,"Title":"Don't use deprecated -pt images","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/7131","RelatedDescription":"Closed or merged PR \"Don't use deprecated -pt images\" (#7131)"},{"Id":"2242069777","IsPullRequest":false,"CreatedAt":"2024-04-17T19:56:53","Actor":"80LevelElf","Number":"7127","RawContent":null,"Title":"Regression FairLearn with AutoML?","State":"closed","Body":"At this moment we have a FairLearn extension method only for Binary Classification - SetBinaryClassificationMetricWithFairLearn method.\r\n\r\nBut during the code ML.net also have Regression Metric FairLearn support - https://github.com/dotnet/machinelearning/blob/main/src/Microsoft.ML.Fairlearn/Metrics/FairlearnMetricCatalog.cs\r\n\r\nIs it possible to use FairLearn with regression via AutoML?","Url":"https://github.com/dotnet/machinelearning/issues/7127","RelatedDescription":"Closed issue \"Regression FairLearn with AutoML?\" (#7127)"},{"Id":"2236137765","IsPullRequest":true,"CreatedAt":"2024-04-17T15:14:53","Actor":"dotnet-maestro[bot]","Number":"7125","RawContent":null,"Title":"[release/3.0] Update dependencies from dotnet/arcade","State":"closed","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:45c6fd49-3a4f-4675-f3da-08dc0c527e17)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: 45c6fd49-3a4f-4675-f3da-08dc0c527e17\r\n- **Build**: 20240404.3\r\n- **Date Produced**: April 4, 2024 5:15:15 PM UTC\r\n- **Commit**: 188340e12c0a372b1681ad6a5e72c608021efdba\r\n- **Branch**: refs/heads/release/8.0\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 8.0.0-beta.24161.7 to 8.0.0-beta.24204.3][1]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 8.0.0-beta.24161.7 to 8.0.0-beta.24204.3][1]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 8.0.0-beta.24161.7 to 8.0.0-beta.24204.3][1]\r\n  - **Microsoft.DotNet.SignTool**: [from 8.0.0-beta.24161.7 to 8.0.0-beta.24204.3][1]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 8.0.0-beta.24161.7 to 8.0.0-beta.24204.3][1]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 8.0.0-beta.24161.7 to 8.0.0-beta.24204.3][1]\r\n\r\n[1]: https://github.com/dotnet/arcade/compare/cd10e5d374...188340e12c\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:45c6fd49-3a4f-4675-f3da-08dc0c527e17)\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7125","RelatedDescription":"Closed or merged PR \"[release/3.0] Update dependencies from dotnet/arcade\" (#7125)"},{"Id":"2243822616","IsPullRequest":false,"CreatedAt":"2024-04-16T00:58:56","Actor":"yueyinqiu","Number":"7129","RawContent":null,"Title":"Is there an equivalent to pandas' get_dummies, in Microsoft.Data.Analysis?","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/issues/7129","RelatedDescription":"Closed issue \"Is there an equivalent to pandas' get_dummies, in Microsoft.Data.Analysis?\" (#7129)"},{"Id":"2237275563","IsPullRequest":false,"CreatedAt":"2024-04-11T09:17:22","Actor":"markbeij","Number":"7126","RawContent":null,"Title":"Create HTML from C# in Jupyter","State":"open","Body":"I hope someone here can help me with this.. I'm exploring the possibilities of data analysis using c# and see a lot of samples where you generate html from your Jupyter notebooks, like this blog from Microsoft: https://devblogs.microsoft.com/dotnet/an-introduction-to-dataframe/ by @pgovind\r\n\r\n```c#\r\nusing Microsoft.AspNetCore.Html;\r\nFormatter<DataFrame>.Register((df, writer) =>\r\n{\r\n    var headers = new List<IHtmlContent>();\r\n    headers.Add(th(i(\"index\")));\r\n    headers.AddRange(df.Columns.Select(c => (IHtmlContent) th(c.Name)));\r\n    var rows = new List<List<IHtmlContent>>();\r\n    var take = 20;\r\n    for (var i = 0; i < Math.Min(take, df.Rows.Count); i++)\r\n    {\r\n        var cells = new List<IHtmlContent>();\r\n        cells.Add(td(i));\r\n        foreach (var obj in df.Rows[i])\r\n        {\r\n            cells.Add(td(obj));\r\n        }\r\n        rows.Add(cells);\r\n    }\r\n\r\n    var t = table(\r\n        thead(\r\n            headers),\r\n        tbody(\r\n            rows.Select(\r\n                r => tr(r))));\r\n\r\n    writer.Write(t);\r\n}, \"text/html\");\r\n```\r\n\r\nIf I paste this in my notebook, I get\r\n`Error: (2,1): error CS0103: The name 'Formatter' does not exist in the current context`\r\n\r\nI also tried with a smaller example:\r\n\r\n```c#\r\nusing Microsoft.AspNetCore.Html;\r\nth(i(\"index\"))\r\n```\r\n\r\nBut I get \r\n`Error: (2,1): error CS0103: The name 'th' does not exist in the current context\r\n(2,4): error CS0103: The name 'i' does not exist in the current context`\r\n\r\nI'm using polyglot notebook in VS Code using C# Script.\r\nNot sure what more info is helpfull.","Url":"https://github.com/dotnet/machinelearning/issues/7126","RelatedDescription":"Open issue \"Create HTML from C# in Jupyter\" (#7126)"},{"Id":"2235172875","IsPullRequest":false,"CreatedAt":"2024-04-10T09:43:38","Actor":"Huochengyan","Number":"7124","RawContent":null,"Title":"How to gain progress when training for a long time","State":"open","Body":"`ITransformer trainedModel = trainingPipeline.Fit(trainingData);`\r\nHow to obtain progress when the time is particularly long? I see many answers are before 2020. Is it currently supported in version 3.0.1? Or other methods can detect the required duration","Url":"https://github.com/dotnet/machinelearning/issues/7124","RelatedDescription":"Open issue \"How to gain progress when training for a long time\" (#7124)"},{"Id":"2233175660","IsPullRequest":true,"CreatedAt":"2024-04-09T11:00:58","Actor":"asmirnov82","Number":"7123","RawContent":null,"Title":"Implement DataFrameColumn Apply and DropNulls methods","State":"open","Body":"Fixes #7107 as was asked in https://github.com/dotnet/machinelearning/issues/6144#issuecomment-2018430389\r\n\r\nAdditionaly:\r\n\r\n1) Fix incorrect IsNumeric method\r\n2) Fix error FillNulls crashes with NotImplemented exception on DataFrame with VBufferDataFrameColumn\r\n3) Improve speed and redesign API (legacy API marked as obsolete), internal methods are rewritten to use new API\r\n4) Add extra unit tests\r\n\r\nReasons for refactoring:\r\n\r\n1) Legacy implementation of ApplyElementwise is written not only to apply function on each element in the column, but also to be used for fast columns iteration, that's why it has rowIndex as one of it's arguments. This duplication of responsibilities is very confusing and absolutely not straightforward. More over, it is slow, as instead of only reading values, ApplyElementwise writes function results into into memory, also it converts read only columns to editable (and that again involves memory copy). So in some circumstances memory can be excessively copied even twice.\r\n2) Legacy method requires to provide function, that takes into account null values - this is not userfriendly. For working with Nulls there are already FillNulls and DropNulls methods. New method applies user function only to valuable values, this also leads to better performance\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7123","RelatedDescription":"Open PR \"Implement DataFrameColumn Apply and DropNulls methods\" (#7123)"},{"Id":"2232019976","IsPullRequest":false,"CreatedAt":"2024-04-08T20:24:17","Actor":"asmirnov82","Number":"7122","RawContent":null,"Title":"DataFrame IndexOufRange exception on attemp to call Apply method","State":"open","Body":"Test to reproduce:\r\n\r\n```cs\r\npublic void TestApplyMethod()\r\n{\r\n    PrimitiveDataFrameColumn<byte> column = new PrimitiveDataFrameColumn<byte>(\"Byte1\", int.MaxValue / 2 - 1);\r\n    PrimitiveDataFrameColumn<double> newColumn = column.Apply<double>(x => (double?)x);\r\n}\r\n```\r\nRoot cause is in Apply<TResult> method:\r\n\r\n```cs\r\npublic void Apply<TResult>(Func<T?, TResult?> func, PrimitiveColumnContainer<TResult> resultContainer)\r\n    where TResult : unmanaged\r\n{\r\n    for (int b = 0; b < Buffers.Count; b++)\r\n    {\r\n        var sourceBuffer = Buffers[b];\r\n        var sourceNullBitMap = NullBitMapBuffers[b].ReadOnlySpan;\r\n\r\n        Span<TResult> mutableResultBuffer = resultContainer.Buffers.GetOrCreateMutable(b).Span;\r\n        Span<byte> mutableResultNullBitMapBuffers = resultContainer.NullBitMapBuffers.GetOrCreateMutable(b).Span;\r\n\r\n        for (int i = 0; i < sourceBuffer.Length; i++)\r\n        {\r\n            bool isValid = BitUtility.IsValid(sourceNullBitMap, i);\r\n            TResult? value = func(isValid ? sourceBuffer[i] : null);\r\n            mutableResultBuffer[i] = value.GetValueOrDefault();\r\n            resultContainer.SetValidityBit(mutableResultNullBitMapBuffers, i, value != null);\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n`mutableResultBuffer` has TResult type of underlying data, so it's max length is 2Gb / sizeof(TResult)\r\n`sourceBuffer` has T type of underlying data, so it's max length is 2 Gb/ sizeof(T)\r\n\r\nwhen sizeof(TResult) > sizeof(T) and source buffer is large enough - it resulted in IndexOutOfRange exception","Url":"https://github.com/dotnet/machinelearning/issues/7122","RelatedDescription":"Open issue \"DataFrame IndexOufRange exception on attemp to call Apply method\" (#7122)"},{"Id":"2230049227","IsPullRequest":false,"CreatedAt":"2024-04-08T00:51:40","Actor":"NohaNLon","Number":"7121","RawContent":null,"Title":"trying to carry out the 'ML.NET in ten minutes' project, but the script for creating the machine learning model does not work on my computer","State":"open","Body":"\r\n - ML.NET Version: [e.g. ML.NET v1.5.5]\r\n - .NET Version: [e.g. .NET 5.0]\r\n\r\n**Describe the bug**\r\ntrying to carry out the 'ML.NET in ten minutes' project, but the script for creating the machine learning model does not work on my computer. I am using a Mac Book, and the error says that the required .NET is not installed in my computer (when I installed it five minutes prior).\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7121","RelatedDescription":"Open issue \"trying to carry out the 'ML.NET in ten minutes' project, but the script for creating the machine learning model does not work on my computer\" (#7121)"},{"Id":"2229275347","IsPullRequest":false,"CreatedAt":"2024-04-06T14:33:41","Actor":"lmxin123","Number":"7120","RawContent":null,"Title":"How to analyze speech by tensorflow.Net ","State":"open","Body":"              The limitation is part of the issue I guess, \"we\" state we do TF but do we.... \r\nDoing a search on the internet one can see nice samples using TF to analyse audio, text, price prediction housing  etc. The most interesting I guess as it allows comparison is if we would be able to demonstrate the [Iris sample](https://www.kaggle.com/pierrek20/multiclass-iris-prediction-with-tensorflow-keras)\r\n\r\n_Originally posted by @PeterPann23 in https://github.com/dotnet/machinelearning/issues/3770#issuecomment-495492045_\r\n            ","Url":"https://github.com/dotnet/machinelearning/issues/7120","RelatedDescription":"Open issue \"How to analyze speech by tensorflow.Net \" (#7120)"},{"Id":"2229252538","IsPullRequest":false,"CreatedAt":"2024-04-06T13:52:49","Actor":"lmxin123","Number":"7119","RawContent":null,"Title":"Urgent need for a speech recognition samples","State":"open","Body":"Urgent need for a speech recognition case.\r\nSupport understanding of environmental sounds, such as glass shattering sounds and knocking sounds.\r\nSupports recognizing profanity, insulting others, and other speech sounds.\r\nSupport for identifying anger emotions.","Url":"https://github.com/dotnet/machinelearning/issues/7119","RelatedDescription":"Open issue \"Urgent need for a speech recognition samples\" (#7119)"},{"Id":"2228782410","IsPullRequest":true,"CreatedAt":"2024-04-05T20:45:30","Actor":"asmirnov82","Number":"7118","RawContent":null,"Title":"Extend dataframe orderby method to allow defining preferred position for null values","State":"open","Body":"Fixes #7102 \r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7118","RelatedDescription":"Open PR \"Extend dataframe orderby method to allow defining preferred position for null values\" (#7118)"},{"Id":"2224087389","IsPullRequest":false,"CreatedAt":"2024-04-04T19:57:03","Actor":"jiafatom","Number":"7117","RawContent":null,"Title":"LightGbm does not exist in namespace Microsoft.ML.Trainers","State":"closed","Body":"**System Information (please complete the following information):**\r\n - OS & Version: Windows 11\r\n - ML.NET Version: 3.0.1\r\n - .NET Version: .Net Framework 4.7.1\r\n\r\n**Describe the bug**\r\nCreate a console app, in the cs file, try to \r\nusing Microsoft.ML.Trainers.LightGbm;\r\nHowever, it says LightGbm does not exist in namespace.\r\n![lightgbm](https://github.com/dotnet/machinelearning/assets/30608893/86eb27bc-5131-4619-a4b8-a86aa78b9749)\r\n\r\nThanks.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7117","RelatedDescription":"Closed issue \"LightGbm does not exist in namespace Microsoft.ML.Trainers\" (#7117)"}],"ResultType":"GitHubIssue"}},"RunOn":"2024-05-04T03:30:16.9212503Z","RunDurationInMilliseconds":392}