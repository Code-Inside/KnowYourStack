{"Data":{"GitHub":{"Issues":[{"Id":"2305068455","IsPullRequest":true,"CreatedAt":"2024-05-29T19:35:42","Actor":"dotnet-maestro[bot]","Number":"7156","RawContent":null,"Title":"[release/3.0] Update dependencies from dotnet/arcade","State":"closed","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:45c6fd49-3a4f-4675-f3da-08dc0c527e17)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: 45c6fd49-3a4f-4675-f3da-08dc0c527e17\r\n- **Build**: 20240516.3\r\n- **Date Produced**: May 16, 2024 10:54:40 PM UTC\r\n- **Commit**: e6f70c7dd528f05cd28cec2a179d58c22e91d9ac\r\n- **Branch**: refs/heads/release/8.0\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 8.0.0-beta.24204.3 to 8.0.0-beta.24266.3][1]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 8.0.0-beta.24204.3 to 8.0.0-beta.24266.3][1]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 8.0.0-beta.24204.3 to 8.0.0-beta.24266.3][1]\r\n  - **Microsoft.DotNet.SignTool**: [from 8.0.0-beta.24204.3 to 8.0.0-beta.24266.3][1]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 8.0.0-beta.24204.3 to 8.0.0-beta.24266.3][1]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 8.0.0-beta.24204.3 to 8.0.0-beta.24266.3][1]\r\n\r\n[1]: https://github.com/dotnet/arcade/compare/188340e12c...e6f70c7dd5\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:45c6fd49-3a4f-4675-f3da-08dc0c527e17)\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7156","RelatedDescription":"Closed or merged PR \"[release/3.0] Update dependencies from dotnet/arcade\" (#7156)"},{"Id":"2319190214","IsPullRequest":true,"CreatedAt":"2024-05-29T19:35:26","Actor":"dotnet-maestro[bot]","Number":"7161","RawContent":null,"Title":"[main] Update dependencies from dotnet/arcade","State":"closed","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: c692823c-b896-437f-4f57-08dc434cc8f6\r\n- **Build**: 20240522.5\r\n- **Date Produced**: May 23, 2024 6:03:20 AM UTC\r\n- **Commit**: 2001d73c8ff942331a73300ba61fa6164805b231\r\n- **Branch**: refs/heads/main\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 9.0.0-beta.24260.2 to 9.0.0-beta.24272.5][1]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 9.0.0-beta.24260.2 to 9.0.0-beta.24272.5][1]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 9.0.0-beta.24260.2 to 9.0.0-beta.24272.5][1]\r\n  - **Microsoft.DotNet.SignTool**: [from 9.0.0-beta.24260.2 to 9.0.0-beta.24272.5][1]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 9.0.0-beta.24260.2 to 9.0.0-beta.24272.5][1]\r\n  - **Microsoft.DotNet.XliffTasks**: [from 9.0.0-beta.24260.2 to 9.0.0-beta.24272.5][1]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 9.0.0-beta.24260.2 to 9.0.0-beta.24272.5][1]\r\n\r\n[1]: https://github.com/dotnet/arcade/compare/480401b003...2001d73c8f\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7161","RelatedDescription":"Closed or merged PR \"[main] Update dependencies from dotnet/arcade\" (#7161)"},{"Id":"2323990537","IsPullRequest":false,"CreatedAt":"2024-05-29T18:33:40","Actor":"pjsgsy","Number":"7164","RawContent":null,"Title":"Model builder training appears to leak data somehow into the training set","State":"open","Body":"Windwos 11\r\nML.Net 3.0.1\r\n.Net 4.8 \r\n\r\nWhen traingin a large csv my model would get consistently high results that I could not replicate in testing outside of model builder. I was letting model builder handle the trainign/validation split, though I tried all those options. Folds, 70/30, 80/20, etc. Always ended up >90% micro accuracy over training time if left, but never got even close when run in real time.  After many days - I today split the SAME data file into 2 different files, telling model builder the validation data is in that separate file, and hey presto, can;t train more than 45%...  This is better (for worse!). The 2 files are a 80/20 split - I just did it myself. Give model builder the whole file and tell it to do the 80/20 split, and it will train to >93% again.  Something in there is broken it seems! So little visibility for me into what is going on, I don't have much more to offer in terms of what. it would appear the validation data is somehow leaked into the training set.\r\n\r\nSeperate validation file\r\n![image](https://github.com/dotnet/machinelearning/assets/439341/2a681847-64b1-41c8-a7fa-e3c11dfc7835)\r\n\r\nCombined file letting model builder do the split will train to >0.93, same data and metrics.\r\n\r\nModel builder version is 17.18.2.2415501\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7164","RelatedDescription":"Open issue \"Model builder training appears to leak data somehow into the training set\" (#7164)"},{"Id":"2254864513","IsPullRequest":false,"CreatedAt":"2024-05-29T17:50:15","Actor":"wildwind2000","Number":"7135","RawContent":null,"Title":"Accessing data by column after adding columns to a DataFrame returns error  data","State":"closed","Body":"**System Information (please complete the following information):**\r\n - OS & Version: Windows 11\r\n - ML.NET Version: 3.01\r\n - .NET Version: .NET 8.0\r\n\r\n**Describe the bug**\r\nAccessing data by column after adding columns to a DataFrame returns error data\r\n\r\n```code\r\nvar df = DataFrame.LoadCsvFromString(\"a1,a2\\n1,2\\n3,4\");\r\nvar dc0 = DataFrameColumn.Create(\"a0\", new int[] { 0, 0 });\r\ndf.Columns.Insert(0, dc0);\r\nvar dc1 = df[\"a1\"];\r\nConsole.WriteLine(dc1.ToString());\r\n```\r\nThis code expected print: a1: 1 3   \r\nBut it print: a0: 0 0\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7135","RelatedDescription":"Closed issue \"Accessing data by column after adding columns to a DataFrame returns error  data\" (#7135)"},{"Id":"2255452625","IsPullRequest":true,"CreatedAt":"2024-05-29T17:50:14","Actor":"feiyun0112","Number":"7136","RawContent":null,"Title":"Accessing data by column after adding columns to a DataFrame returns error data","State":"closed","Body":"fix #7135 \r\n\r\nDescribe the bug\r\nAccessing data by column after adding columns to a DataFrame returns error data\r\n\r\n````\r\nvar df = DataFrame.LoadCsvFromString(\"a1,a2\\n1,2\\n3,4\");\r\nvar dc0 = DataFrameColumn.Create(\"a0\", new int[] { 0, 0 });\r\ndf.Columns.Insert(0, dc0);\r\nvar dc1 = df[\"a1\"];\r\nConsole.WriteLine(dc1.ToString());\r\n````\r\n\r\n\r\nThis code expected print: a1: 1 3\r\nBut it print: a0: 0 0","Url":"https://github.com/dotnet/machinelearning/pull/7136","RelatedDescription":"Closed or merged PR \"Accessing data by column after adding columns to a DataFrame returns error data\" (#7136)"},{"Id":"2279025739","IsPullRequest":true,"CreatedAt":"2024-05-29T17:18:08","Actor":"ravibaghel","Number":"7147","RawContent":null,"Title":"Added error handling, removed unwanted null check and enhanced readability","State":"closed","Body":"The most significant changes include the removal of the null check for \"args\" in the \"Main\" method, the addition of a \"try-catch\" block to handle exceptions during the execution of the \"sample\" method, and the modification of the final console output line to use string interpolation for better readability.\r\n\r\n1. The null check for \"args\" in the \"Main\" method has been removed, indicating that \"args\" is always expected to be an array, even if it's an empty one. This change simplifies the code and makes the assumption about the nature of \"args\" more explicit.\r\n\r\n2. A \"try-catch\" block has been added around the invocation of the \"sample\" method and the increment of \"samples\". This change improves the robustness of the code by handling potential exceptions that might occur during the execution of the \"sample\" method. If an exception is thrown, it is caught and an error message is printed to the console, providing useful information about the error.\r\n\r\n3. The final console output line has been updated to use string interpolation to display the number of samples that ran without any exception. This change enhances the readability of the code by using a more modern and readable way to format strings in C#.\r\n\r\nWe are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [ ] There's a descriptive title that will make sense to other developers some time from now. \r\n- [ ] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [ ] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [ ] You have included any necessary tests in the same PR.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7147","RelatedDescription":"Closed or merged PR \"Added error handling, removed unwanted null check and enhanced readability\" (#7147)"},{"Id":"2322959164","IsPullRequest":false,"CreatedAt":"2024-05-29T10:26:22","Actor":"aforoughi1","Number":"7163","RawContent":null,"Title":"Loading a LSTM Model Created in Torchsharp in ML.Net","State":"open","Body":"I can create/train/evaluate/save/load a multilayer LSTM model using Torchsharp but cannot run in ML.NET.\r\n\r\nThe main requirement is to export/import a Torchsharp model to ONNX and run it in ML.NET. However, Torchsharp doesn't implement torch.onnx.export method.  I'm stuck at a point and had to convert everything to pytorch just to export to ONNX. \r\n\r\n I also tried a custom pipeline and  trial runner to train and evaluate the model but I'm struggling with how these models can be saved/Loaded in ML.NET.    \r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7163","RelatedDescription":"Open issue \"Loading a LSTM Model Created in Torchsharp in ML.Net\" (#7163)"},{"Id":"2320948894","IsPullRequest":false,"CreatedAt":"2024-05-28T12:31:53","Actor":"agonzalezm","Number":"7162","RawContent":null,"Title":"Running AI inference of phi3 and other llms from c# using NPU + GPU in comming processors?","State":"open","Body":"Intel, AMD, Qualqomm, etc are getting powerful NPUs (+40TOPS) for inferencing.\r\n\r\nIs there any plan to incluide in ml.net functionality to be able to run and inference these models easily from C# offloading to NPU or GPU or both. Next Intel processors will have 40TOPS NPU and 60TOPS CPU/GPU.\r\n\r\nHow from C# can we easily make the most and inference using all of these TOPS coming from NPU + GPU?\r\n\r\nAll samples i see about this require using python etc, would be great to have all this available in .NET C# directly.\r\n\r\nMaybe including some C# wrapper around https://github.com/intel/intel-npu-acceleration-library but what about AMD and qualcomm?\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7162","RelatedDescription":"Open issue \"Running AI inference of phi3 and other llms from c# using NPU + GPU in comming processors?\" (#7162)"},{"Id":"2318862438","IsPullRequest":false,"CreatedAt":"2024-05-27T10:51:12","Actor":"alkampfergit","Number":"7160","RawContent":null,"Title":"Special Tokens handling seems to be incorrect (at least in my scenario where I'm creating Command R+ tiktoken file from specification)","State":"open","Body":"Windows 11, .NET 8 version of library 0.22.0-preview.24271.1\r\n\r\n**Describe the bug**\r\nI'm trying to use tiktoken class to implement Cohere Command R+ tokenizer from the vocabulary file that cohere produces at https://storage.googleapis.com/cohere-public/tokenizers/command-r-plus.json\r\n\r\n**To Reproduce**\r\nyou can find a playbook here https://github.com/alkampfergit/ai-playground/blob/develop/src/python/langchainVarious/Tokenization/dotnetcohere.dib Actually I downloaded the file, then extract vocabulary node and create tiktoken file, then parse the json file to grab special tokens.\r\n\r\n**Expected behavior**\r\nAfter I've parsed the commandR+ tokenizer specification if I tokenize a special token like <|YES_TOKEN|> it correctly recognize it as a a single token (upper part of the picture), but if I use in a sentence, like good<|YES_TOKEN|>good tokenization does not recognize the special token (lower part of the picture)\r\n\r\nSince I've created the .tiktoken file from the specification it is possible that I've done something wrong, other tokens seems to be recognized just good.\r\n\r\n**Screenshots, Code, Sample Projects**\r\n![image](https://github.com/dotnet/machinelearning/assets/358545/43ab3c71-0d69-4687-a0bb-736413ad3cb2)\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7160","RelatedDescription":"Open issue \"Special Tokens handling seems to be incorrect (at least in my scenario where I'm creating Command R+ tiktoken file from specification)\" (#7160)"},{"Id":"2313550032","IsPullRequest":true,"CreatedAt":"2024-05-23T18:07:05","Actor":"ericstj","Number":"7159","RawContent":null,"Title":"Update libmf submodule and reenable CodeQL on it","State":"open","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/7159","RelatedDescription":"Open PR \"Update libmf submodule and reenable CodeQL on it\" (#7159)"},{"Id":"2311876933","IsPullRequest":false,"CreatedAt":"2024-05-23T03:49:04","Actor":"LittleLittleCloud","Number":"7158","RawContent":null,"Title":"Support special tokens in sentence piece bpe","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nThe phi-3 uses llama2 tokenizer with a few special tokens like `<|user|>` and `<|system|>`. But currently there is no way to add special tokens to sentence piece bpe (the llama 2 tokenizer) in mlnet\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7158","RelatedDescription":"Open issue \"Support special tokens in sentence piece bpe\" (#7158)"},{"Id":"2301544637","IsPullRequest":false,"CreatedAt":"2024-05-21T15:32:10","Actor":"Xan-Kun","Number":"7154","RawContent":null,"Title":"Still no o200k_base support","State":"closed","Body":"**System Information (please complete the following information):**\r\n - OS & Version: all\r\n - OS & Version: all\r\n - ML.NET Version: all\r\n - .NET Version: all\r\n\r\n**Describe the bug**\r\nNo way to tokenize gpt-4o strings!\r\n\r\n**To Reproduce**\r\nTokenize a string for gpt-4o\r\n\r\n**Expected behavior**\r\nThe most recent models are supported.\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7154","RelatedDescription":"Closed issue \"Still no o200k_base support\" (#7154)"},{"Id":"2306994085","IsPullRequest":true,"CreatedAt":"2024-05-21T15:32:09","Actor":"tarekgh","Number":"7157","RawContent":null,"Title":"Support Gpt-4o tokenizer model","State":"closed","Body":"Support the new OpenAI `Gpt-4o` tokenizer model. \r\n\r\n```C#\r\n        Tokenizer GPT4o = Tokenizer.CreateTiktokenForModel(\"gpt-4o\");\r\n         text = \"<|endoftext|>Hello ‚≠ê World<|endofprompt|>\";\r\n        \r\n        IReadOnlyList<int> encoded = GPT4o.EncodeToIds(text);\r\n        int idsCount = GPT4o.CountTokens(text);\r\n```\r\n\r\n---\r\n***Notes***\r\n\r\n- We have embedded the new tokenizer vocabulary file `o200k_base.tiktoken` in the tokenizer assembly, just as we did with other Tiktoken models. After compression, the file size is `1,188,794 bytes`. We plan to move the tokenizer vocabulary files into their own packages in the future.\r\n- [OpenAI](https://github.com/openai/tiktoken/tree/main) didn't add tests yet for this new model. All tests added in this PR are manually generated. We can add more validation later when OpenAI enable this model in https://platform.openai.com/tokenizer.\r\n---\r\n\r\nCloses https://github.com/dotnet/machinelearning/issues/7154","Url":"https://github.com/dotnet/machinelearning/pull/7157","RelatedDescription":"Closed or merged PR \"Support Gpt-4o tokenizer model\" (#7157)"},{"Id":"2301548458","IsPullRequest":true,"CreatedAt":"2024-05-17T14:22:18","Actor":"ericstj","Number":"7155","RawContent":null,"Title":"Remove Codeql.SourceRoot","State":"closed","Body":"This was resulting in issues with bug reports and paths as well as with CodeQL honoring our exception config for submodules.\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7155","RelatedDescription":"Closed or merged PR \"Remove Codeql.SourceRoot\" (#7155)"},{"Id":"2300399375","IsPullRequest":false,"CreatedAt":"2024-05-16T13:23:53","Actor":"robalexclark","Number":"7153","RawContent":null,"Title":"Schema mismatch for label column ': expected Boolean, got Single ","State":"open","Body":"I have an AutoML experiment as follows:\r\n```\r\n     List<QuoteModel> quoteModels = new List<QuoteModel>();\r\n\r\n   //...get data from db and load into quoteModels\r\n\r\n     MLContext mlContext = new MLContext();\r\n\r\n     IDataView dataView = mlContext.Data.LoadFromEnumerable(quoteModels);\r\n     TrainTestData splitDataView = mlContext.Data.TrainTestSplit(dataView, testFraction: 0.8);\r\n\r\n     SweepablePipeline pipeline = mlContext.Auto().Featurizer(\r\n         dataView)\r\n         .Append(mlContext.Auto().BinaryClassification(labelColumnName: \"LeadWon\", featureColumnName: \"PreviouslyFlooded\"));\r\n\r\n     AutoMLExperiment experiment = mlContext.Auto().CreateExperiment();\r\n\r\n     experiment\r\n         .SetPipeline(pipeline)\r\n         .SetBinaryClassificationMetric(BinaryClassificationMetric.Accuracy, labelColumn: \"LeadWon\")\r\n         .SetTrainingTimeInSeconds(60)\r\n         .SetDataset(splitDataView);\r\n\r\n     TrialResult experimentResults = await experiment.RunAsync();\r\n```\r\n\r\nThe QuoteModel class is:\r\n\r\n```\r\n\r\npublic class QuoteModel\r\n{\r\n    public string PreviouslyFlooded { get; set; }\r\n\r\n    public Boolean LeadWon { get; set; }\r\n}\r\n\r\n```\r\n\r\nWhen I run the experiment it gives the error:\r\n\r\nSystem.ArgumentOutOfRangeException: 'Schema mismatch for label column 'LeadWon': expected Boolean, got Single \r\n\r\nIf you look at the QuoteModel class, the label column LeadWon is definately a boolean!!\r\n\r\nAny ideas why/how it can give this seemingly impossible error?\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7153","RelatedDescription":"Open issue \"Schema mismatch for label column ': expected Boolean, got Single \" (#7153)"},{"Id":"2299037834","IsPullRequest":false,"CreatedAt":"2024-05-15T23:43:29","Actor":"lucaspastorduran","Number":"7152","RawContent":null,"Title":"DataFrame.LoadCsv() Could not load file or assembly 'System.Runtime.CompilerServices.Unsafe, Version=4.0.4.1'","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: Windows 10 Enterprise\r\n - ML.NET Version: Microsoft.Data.Analysis 0.21.1\r\n - .NET Version: .Net Framework 4.8 (NET SDK 6.0.417)\r\n\r\n**Describe the bug**\r\nUsing the method DataFrame Data = DataFrame.LoadCsv(\"myFIlePath\") returns runtime exception:\r\n\"Could not load file or assembly 'System.Runtime.CompilerServices.Unsafe, Version=4.0.4.1, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' or one of its dependencies. The located assembly's manifest definition does not match the assembly reference. (Exception from HRESULT: 0x80131040)\" \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Create a DataFrame object from CSV file: DataFrame Data = DataFrame.LoadCsv(\"myFIlePath\")\r\n\r\n**Expected behavior**\r\nIt should parse the csv data into the DataFrame object.\r\n\r\n**Screenshots, Code, Sample Projects**\r\n![image](https://github.com/dotnet/machinelearning/assets/36445004/ee27bd6b-b99b-4aef-ad2d-3284a80d155a)\r\n\r\n**Additional context**\r\nIt is looking for the System.Runtime.CompilerServices.Unsafe, Version=4.0.4.1, which is contained in the nuget package 'System.Runtime.CompilerServices.Unsafe 4.5.3'.\r\nHowever, the ML version 0.21.1 requires to install System.Runtime.CompilerServices.Unsafe with version >= 6.0 (which has a higer version than the one is looking for).\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7152","RelatedDescription":"Open issue \"DataFrame.LoadCsv() Could not load file or assembly 'System.Runtime.CompilerServices.Unsafe, Version=4.0.4.1'\" (#7152)"},{"Id":"2292807929","IsPullRequest":true,"CreatedAt":"2024-05-13T16:54:01","Actor":"dotnet-maestro[bot]","Number":"7151","RawContent":null,"Title":"[main] Update dependencies from dotnet/arcade","State":"closed","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: c692823c-b896-437f-4f57-08dc434cc8f6\r\n- **Build**: 20240510.2\r\n- **Date Produced**: May 10, 2024 5:18:03 PM UTC\r\n- **Commit**: 480401b003bfd2eb989c315da5d6b99ad13a968c\r\n- **Branch**: refs/heads/main\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.SignTool**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.XliffTasks**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 9.0.0-beta.24253.1 to 9.0.0-beta.24260.2][1]\r\n\r\n[1]: https://github.com/dotnet/arcade/compare/020255bcf7...480401b003\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7151","RelatedDescription":"Closed or merged PR \"[main] Update dependencies from dotnet/arcade\" (#7151)"},{"Id":"2290412166","IsPullRequest":true,"CreatedAt":"2024-05-13T16:53:31","Actor":"ericstj","Number":"7150","RawContent":null,"Title":"Fix iterator type so that it matches boundary condition type","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/7150","RelatedDescription":"Closed or merged PR \"Fix iterator type so that it matches boundary condition type\" (#7150)"},{"Id":"2256524274","IsPullRequest":true,"CreatedAt":"2024-05-10T20:39:26","Actor":"dotnet-maestro[bot]","Number":"7138","RawContent":null,"Title":"[main] Update dependencies from dotnet/arcade","State":"closed","Body":"This pull request updates the following dependencies\r\n\r\n[marker]: <> (Begin:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n## From https://github.com/dotnet/arcade\r\n- **Subscription**: c692823c-b896-437f-4f57-08dc434cc8f6\r\n- **Build**: 20240503.1\r\n- **Date Produced**: May 3, 2024 9:02:59 AM UTC\r\n- **Commit**: 020255bcf7d0b8beed7de05338d97396982ae527\r\n- **Branch**: refs/heads/main\r\n\r\n[DependencyUpdate]: <> (Begin)\r\n\r\n- **Updates**:\r\n  - **Microsoft.DotNet.Arcade.Sdk**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.Build.Tasks.Feed**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.Helix.Sdk**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.SignTool**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.SwaggerGenerator.MSBuild**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.XliffTasks**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n  - **Microsoft.DotNet.XUnitExtensions**: [from 9.0.0-beta.24212.4 to 9.0.0-beta.24253.1][4]\r\n\r\n[4]: https://github.com/dotnet/arcade/compare/87b015b938...020255bcf7\r\n\r\n[DependencyUpdate]: <> (End)\r\n\r\n\r\n[marker]: <> (End:c692823c-b896-437f-4f57-08dc434cc8f6)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/7138","RelatedDescription":"Closed or merged PR \"[main] Update dependencies from dotnet/arcade\" (#7138)"},{"Id":"2288110461","IsPullRequest":true,"CreatedAt":"2024-05-10T15:14:43","Actor":"directhex","Number":"7149","RawContent":null,"Title":"Try enabling TSA scan during build","State":"closed","Body":"The method for TSA scanning has changed over time, this ought to do the trick","Url":"https://github.com/dotnet/machinelearning/pull/7149","RelatedDescription":"Closed or merged PR \"Try enabling TSA scan during build\" (#7149)"},{"Id":"2288024884","IsPullRequest":false,"CreatedAt":"2024-05-09T16:16:09","Actor":"winscripter","Number":"7148","RawContent":null,"Title":"Is it possible to use ML.NET for image processing (such as remove background)?","State":"open","Body":"Hello,\r\n\r\nDoes ML.NET support image processing, such as removing background or\r\nmaking a specific item different color, using a set of images? If so, is there\r\na documented example for image processing with ML.NET?\r\n\r\nThanks.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7148","RelatedDescription":"Open issue \"Is it possible to use ML.NET for image processing (such as remove background)?\" (#7148)"},{"Id":"2257816675","IsPullRequest":true,"CreatedAt":"2024-05-02T18:58:56","Actor":"tarekgh","Number":"7139","RawContent":null,"Title":"Introducing CodeGen Tokenizer","State":"closed","Body":"This change is implementing the [CodeGen](https://huggingface.co/Salesforce/codegen-350M-mono/tree/main) which also support the [Phi-2](https://huggingface.co/microsoft/phi-2/tree/main) tokenizer.","Url":"https://github.com/dotnet/machinelearning/pull/7139","RelatedDescription":"Closed or merged PR \"Introducing CodeGen Tokenizer\" (#7139)"},{"Id":"2273370726","IsPullRequest":false,"CreatedAt":"2024-05-01T11:40:39","Actor":"superichmann","Number":"7146","RawContent":null,"Title":"Modify IDataView in AutoML Experiment After Transform and Before Evaluate","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nno\r\n\r\n**Describe the solution you'd like**\r\nAdd the option to modify the idataview (such as in preFeaturizer) but a \"postFeaturizer\" which will transform the idataview after the Transform has occurred on it inside the experiment and before the evaluation metrics are calculated.\r\n\r\n**Describe alternatives you've considered**\r\nCreate my own trial runner, if possible and this feature is not planned in automl please provide me with a start code :]\r\n\r\n**Additional context**\r\nSome use cases require alteration of the idataview based on the Score column which is not present before Transform is called.\r\nAnother solution would be to add the possibility to call a custom evaluate function based on LINQ","Url":"https://github.com/dotnet/machinelearning/issues/7146","RelatedDescription":"Open issue \"Modify IDataView in AutoML Experiment After Transform and Before Evaluate\" (#7146)"},{"Id":"2273166524","IsPullRequest":false,"CreatedAt":"2024-05-01T08:47:48","Actor":"sportbilly21","Number":"7145","RawContent":null,"Title":"Dll version of Microsoft.ML.OnnxRuntime.dll is 0.0.0.0","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: either Windows 10 or 11 \r\n - ML.NET Version: versions 1.7 and 3.0\r\n - .NET Version: .netframework 4.7\r\n\r\n**Describe the bug**\r\nAfter building the solution the version of the Microsoft.ML.OnnxRuntime.dll is 0.0.0.0\r\nThis is not causing any issues if you are running the application through the Visual Studio.\r\nBut we create an installer with Wix for installation in production PC\r\nWhen you install the software in Windows, the OS and wix due to the version of the dll being zero, they think the dll is corrupted and the installation cannot continue as the installer tries to recover the corrupted dll. \r\n\r\nWe have a work around by changing the version of the above dll but from my understanding dlls should not have 0 as a version\r\n\r\n\r\n\r\n\r\n![image](https://github.com/dotnet/machinelearning/assets/60097348/7e596786-3ff8-4750-acf1-51f2a168f3e9)\r\n\r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7145","RelatedDescription":"Open issue \"Dll version of Microsoft.ML.OnnxRuntime.dll is 0.0.0.0\" (#7145)"},{"Id":"2267992526","IsPullRequest":false,"CreatedAt":"2024-04-29T02:12:59","Actor":"tarekgh","Number":"7144","RawContent":null,"Title":"Tokenizers Library Design","State":"open","Body":"LLM tokenizers are a crucial component in Large Language Models (LLMs) like GPT-3 or BERT. They are responsible for the tokenization process, which involves breaking down natural language text into smaller, manageable pieces called tokens. These tokens can be words, characters, sub-words, numbers, or symbols, and they allow the LLM to process and understand the text.\r\n\r\nThis issue presents the APIs proposed for the Microsoft.ML.Tokenizers library, intended for design review. The design introduces an abstract class named `Tokenizer`, which defines the primary interfaces for all supported tokenizers. Additionally, the Tokenizer class includes a factory method for creating various types of tokenizers.\r\n\r\nThe Tokenizer can be optionally configured with normalizers, which are used to normalize the text before processing it. Normalization can take various forms such as uppercasing, lowercasing, [Unicode Normalization](https://www.unicode.org/reports/tr15/), and removing or inserting specific characters from the input text. The normalization feature is optional for the tokenizer, and it is left to the discretion of either the tokenizer or the user to decide whether to utilize any normalizers.\r\n\r\nPre-tokenization is an additional component that the tokenizer can be configured with, aimed at splitting the input text into smaller units prior to processing. While pre-tokenization is also an optional feature, it is commonly utilized in most tokenizers. Many pre-tokenizers employ regex for this purpose.\r\n\r\nThe typical sequence of operations for the Tokenizer involves:\r\n\r\n- Normalizing the input text if a normalizer is configured.\r\n- Pre-tokenizing the input or normalized text to segment it into smaller units.\r\n- Encoding each unit of text, potentially dividing it into smaller tokens and generating string tokens, IDs for the tokens, and/or offsets that map each token to a portion of the input or normalized text.\r\n\r\nTokenizers offer the following functionalities:\r\n\r\n- Encoding the input text into IDs, which can be utilized as input for Language Models. This operation is referred to as `EncodeToIds` in the proposed design.\r\n- Counting the tokens within the input text, aiding in calculating the quota allowed for processing at any given time. This operation is named `CountTokens` in the proposed design.\r\n- Full encoding, providing detailed results such as string tokens, IDs, and offsets mapping the tokens to parts of the input text. This operation is labeled as `Encode` in the proposed design.\r\n- Given a maximum token count, the tokenizer can determine how far into the input text tokens can be produced, either from the beginning or the end. These operations are denoted as `IndexOfTokenCount` and `LastIndexOfTokenCount`.\r\n- Decoding the generated IDs back into text. This operation is named `Decode` in the proposed design.\r\n- Establishing mappings between string tokens and IDs. These operations are termed `MapTokenToId` and `MapIdToToken` in the proposed design.\r\n\r\nTokenizers typically rely on vocabulary files, which are provided to the tokenizer during instantiation. Users commonly pass these vocabularies as either a file or a stream to the tokenizer constructor. Vocabulary files can vary in format, such as JSON, plain text, protobuf, and more. Each tokenizer determines the specific formats of files it can be instantiated with.\r\n\r\n# Usage Example:\r\n\r\n### Create BPE tokenizer using the constructor\r\n\r\n```C#\r\n    Tokenizer tokenizer = new Bpe(vocabStream: vocabStream, , mergesStream: mergesStream, normalizer: null, preTokenizer: WhiteSpace.Instance);\r\n```\r\n\r\n### Create Tiktoken tokenizer using factory method:\r\n\r\n```C#\r\n    Dictionary<string, int> specialTokens = new Dictionary<string, int> { { IMStart, 100264}, { IMEnd, 100265}, };\r\n    Tokenizer tokenizer = Tokenizer.CreateTiktokenForModel(\"gpt-4\", specialTokens);\r\n```\r\n\r\n### Encode to Ids:\r\n\r\n```C#\r\n    IReadOnlyList<int> encoded = tokenizer.EncodeToIds(\"Hello World\");\r\n```\r\n\r\n### Count Tokens\r\n\r\n```C#\r\n    int idsCount = tokenizer.CountTokens(\"Hello World\");\r\n```\r\n\r\n### Ful Encoding:\r\n\r\n```C#\r\n    // APIs return any information related to the input or normalized text will usually out normalizedString which can be null if there is no normalization performed.\r\n    // Token contain the string token, the token ID, and the offset of the token mapped to the input or normalized text.\r\n    IReadOnlyList<Token> result = tokenizer.Encode(text, out string? normalizedString);\r\n```\r\n\r\n### Count tokens up to max token count:\r\n\r\n```C#\r\n    int length = tokenizer.IndexOfTokenCount(text, maxTokenCount: 10, out string? normalizedString, out int tokenCount);\r\n    \r\n    int index = tokenizer.LastIndexOfTokenCount(text, maxTokenCount: 3, out normalizedString, out tokenCount)\r\n```\r\n\r\n### Decoding Ids back to string\r\n\r\n```C#\r\nstring decodedText = tokenizer.Decode(idsArray);\r\n```\r\n\r\n### Map string token to Id and vice versa\r\n\r\n```C#\r\nint? id = tokenizer.MapTokenToId(\"Hello\");\r\n\r\nstring? token = MapIdToToken(tokenId);\r\n```\r\n\r\n# Proposal:\r\n\r\n### Namespace\r\n\r\n```C#\r\nnamespace Microsoft.ML.Tokenizers\r\n```\r\n\r\n### Tokenizer Abstraction\r\n\r\n```C#\r\n    public abstract partial class Tokenizer\r\n    {\r\n        protected Tokenizer() { }\r\n\r\n        public virtual Normalizer? Normalizer { get { throw null; } }\r\n\r\n        public virtual PreTokenizer? PreTokenizer { get { throw null; } }\r\n\r\n        public virtual IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedText, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedText, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual int LastIndexOfTokenCount(string text, int maxTokenCount, out string? processedText, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public abstract int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? processedText, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true);\r\n\r\n        public virtual string? Decode(IEnumerable<int> ids) { throw null; }\r\n\r\n        public virtual int? MapTokenToId(string token) { throw null; }\r\n        public abstract int? MapTokenToId(ReadOnlySpan<char> token);\r\n\r\n        public abstract string? MapIdToToken(int? id);\r\n\r\n       //\r\n       // Factory methods\r\n       // \r\n\r\n        public static Task<Tokenizer> CreateTiktokenAsync(Stream vocabStream, PreTokenizer? preTokenizer, Normalizer? normalizer, IReadOnlyDictionary<string, int> specialTokens = null, \r\n                                                                                                  int cacheSize = 8192, Threading.CancellationToken cancellationToken = null) { throw null; }\r\n\r\n        public static Task<Tokenizer> CreateTiktokenAsync(string vocabFilePath, PreTokenizer? preTokenizer, Normalizer? normalizer, IReadOnlyDictionary<string, int> specialTokensEncoder = null, \r\n                                                                                                  int cacheSize = 8192, Threading.CancellationToken cancellationToken = null) { throw null; }\r\n\r\n        public static Tokenizer CreateTiktokenForEncoding(string encodingName, IReadOnlyDictionary<string, int> extraSpecialTokens = null, Normalizer? normalizer = null) { throw null; }\r\n\r\n        public static Tokenizer CreateTiktokenForModel(string modelName, IReadOnlyDictionary<string, int> extraSpecialTokens = null, Normalizer? normalizer = null) { throw null; }\r\n\r\n        public static Tokenizer CreateTiktokenForModel(string modelName, Stream vocabStream, IReadOnlyDictionary<string, int> extraSpecialTokens = null, \r\n                                                                                                    int cacheSize = 8192, Normalizer? normalizer = null) { throw null; }\r\n\r\n        public static Task<Tokenizer> CreateTiktokenForModelAsync(string modelName, Stream vocabStream, IReadOnlyDictionary<string, int> extraSpecialTokens = null, \r\n                                                                                                   int cacheSize = 8192, Normalizer? normalizer = null, Threading.CancellationToken cancellationToken = null) { throw null; }\r\n\r\n        public static Tokenizer CreateLlama(Stream modelStream, bool addBeginOfSentence = true, bool addEndOfSentence = false) { throw null; }\r\n\r\n        public static Tokenizer CreateCodeGen(Stream vocabStream, Stream mergesStream, bool addPrefixSpace = false, bool addBeginOfSentence = false, bool addEndOfSentence = false) { throw null; }\r\n\r\n        public static Tokenizer CreatePhi2(Stream vocabStream, Stream mergesStream, bool addPrefixSpace = false, bool addBeginOfSentence = false, bool addEndOfSentence = false) { throw null; }\r\n    }\r\n```\r\n\r\n### Normalization abstraction \r\n\r\n```C#\r\n    public abstract partial class Normalizer\r\n    {\r\n        protected Normalizer() { }\r\n\r\n        public abstract string Normalize(string original);\r\n        public abstract string Normalize(ReadOnlySpan<char> original);\r\n    }\r\n\r\n```\r\n\r\n### Pre-tokenization abstraction \r\n\r\n```C#\r\n    public abstract partial class PreTokenizer\r\n    {\r\n        protected PreTokenizer() { }\r\n\r\n        public abstract IEnumerable<(int, int)> PreTokenize(string text);\r\n        public abstract IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text);\r\n    }\r\n```\r\n\r\n### Token class \r\n\r\n```C#\r\n   // returned from Tokenizer.Encode(...)\r\n   \r\n    public readonly struct Token\r\n    {\r\n        public Token(int id, string value, (int, int) offset) { }\r\n\r\n        public int Id { get { throw null; } }\r\n\r\n        public (int Index, int Length) Offset { get { throw null; } }\r\n\r\n        public string Value { get { throw null; } }\r\n    }\r\n```\r\n\r\n### Concrete Normalizers \r\n\r\n```C#\r\n    public sealed partial class LowerCaseNormalizer : Normalizer\r\n    {\r\n        public override string Normalize(ReadOnlySpan<char> original) { throw null; }\r\n        public override string Normalize(string original) { throw null; }\r\n    }\r\n\r\n    public sealed partial class UpperCaseNormalizer : Normalizer\r\n    {\r\n        public override string Normalize(ReadOnlySpan<char> original) { throw null; }\r\n\r\n        public override string Normalize(string original) { throw null; }\r\n    }\r\n    \r\n    public sealed partial class SentencePieceNormalizer : Normalizer\r\n    {\r\n        public SentencePieceNormalizer(bool removeExtraWhiteSpaces, bool addDummyPrefix, bool escapeWhiteSpaces, bool treatWhitespaceAsSuffix) { }\r\n        public bool AddDummyPrefix { get { throw null; } }\r\n        public bool EscapeWhiteSpaces { get { throw null; } }\r\n        public bool RemoveExtraWhiteSpaces { get { throw null; } }\r\n        public bool TreatWhitespaceAsSuffix { get { throw null; } }\r\n\r\n        public override string Normalize(ReadOnlySpan<char> original) { throw null; }\r\n        public override string Normalize(string original) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Pre-tokenizers\r\n\r\n```C#\r\n    public sealed partial class TiktokenPreTokenizer : PreTokenizer\r\n    {\r\n        public TiktokenPreTokenizer(Text.RegularExpressions.Regex regex, IReadOnlyDictionary<string, int> specialTokensEncoder) { }\r\n\r\n        public override IEnumerable<(int, int)> PreTokenize(string text) { throw null; }\r\n        public override IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text) { throw null; }\r\n    }\r\n\r\n    public sealed partial class WhiteSpace : PreTokenizer\r\n    {\r\n        public static WhiteSpace Instance { get { throw null; } }\r\n\r\n        public override IEnumerable<(int, int)> PreTokenize(string text) { throw null; }\r\n        public override IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text) { throw null; }\r\n    }\r\n\r\n    public sealed partial class RobertaPreTokenizer : PreTokenizer\r\n    {\r\n        public static RobertaPreTokenizer Instance { get { throw null; } }\r\n\r\n        public override IEnumerable<(int, int)> PreTokenize(string text) { throw null; }\r\n        public override IEnumerable<(int, int)> PreTokenize(ReadOnlySpan<char> text) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - Bpe\r\n\r\n```C#\r\n    public sealed partial class Bpe : Tokenizer\r\n    {\r\n        public Bpe(string vocabFile, string? mergesFile, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, string? unknownToken = null, \r\n                           string? continuingSubwordPrefix = null, string? endOfWordSuffix = null, bool? fuseUnknownTokens = false) { }\r\n\r\n        public Bpe(Stream vocabStream, Stream? mergesStream, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, string? unknownToken = null, \r\n                          string? continuingSubwordPrefix = null, string? endOfWordSuffix = null, bool? fuseUnknownTokens = false) { }\r\n\r\n        public string? ContinuingSubwordPrefix { get { throw null; } }\r\n\r\n        public string? EndOfWordSuffix { get { throw null; } }\r\n\r\n        public bool? FuseUnknownTokens { get { throw null; } }\r\n\r\n        public string? UnknownToken { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public string? Decode(IEnumerable<int> ids, bool considerSpecialTokens) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - Tiktoken \r\n\r\n```C#\r\n    public sealed partial class Tiktoken : Tokenizer\r\n    {\r\n        public Tiktoken(Stream vocabStream, PreTokenizer? preTokenizer, IReadOnlyDictionary<string, int> specialTokens = null, Normalizer? normalizer = null, int? cacheSize = 8192) { }\r\n\r\n        public Tiktoken(string vocabFilePath, PreTokenizer? preTokenizer, IReadOnlyDictionary<string, int> specialTokens = null, Normalizer? normalizer = null, int? cacheSize = 8192) { }\r\n\r\n        public IReadOnlyDictionary<int, ReadOnlyMemory<Byte>> Decoder { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<ReadOnlyMemory<Byte>, int> Encoder { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> SpecialTokens { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - EnglishRoberta\r\n\r\n```C#\r\n    public sealed partial class EnglishRoberta : Tokenizer\r\n    {\r\n        public EnglishRoberta(Stream vocabularyStream, Stream mergeStream, Stream highestOccurrenceMappingStream, PreTokenizer? preTokenizer, Normalizer? normalizer, bool filterUnsupportedChars, bool disposeStream) { }\r\n\r\n        public EnglishRoberta(Stream vocabularyStream, Stream mergeStream, Stream highestOccurrenceMappingStream, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, bool filterUnsupportedChars = true) { }\r\n\r\n        public EnglishRoberta(string vocabularyPath, string mergePath, string highestOccurrenceMappingPath, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, bool filterUnsupportedChars = true) { }\r\n\r\n        public bool FilterUnsupportedChars { get { throw null; } }\r\n\r\n        public int PadIndex { get { throw null; } }\r\n\r\n        public int SymbolsCount { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public int AddMaskSymbol(string mask = \"<mask>\") { throw null; }\r\n\r\n        public IReadOnlyList<int> ConvertIdsToOccurrenceRanks(IReadOnlyList<int> ids) { throw null; }\r\n\r\n        public IReadOnlyList<int> ConvertIdsToOccurrenceValues(IReadOnlyList<int> ids) { throw null; }\r\n\r\n        public IReadOnlyList<int> ConvertOccurrenceRanksToIds(IReadOnlyList<int> ranks) { throw null; }\r\n\r\n        public bool IsSupportedChar(char ch) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - CodeGen \r\n\r\n```C#\r\n    public sealed partial class CodeGen : Tokenizer\r\n    {\r\n        public CodeGen(string vocabularyPath, string mergePath, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, IReadOnlyDictionary<string, int> addedTokens = null, \r\n                                     bool? addPrefixSpace = false, bool? addBeginningOfSentence = false, bool? addEndOfSentence = false, string? unknownToken = \"<|endoftext|>\", \r\n                                     string? beginningOfSentenceToken = \"<|endoftext|>\", string? endOfSentenceToken = \"<|endoftext|>\") { }\r\n\r\n        public CodeGen(Stream vocabularyStream, Stream mergeStream, PreTokenizer? preTokenizer = null, Normalizer? normalizer = null, IReadOnlyDictionary<string, int> addedTokens = null, \r\n                                    bool? addPrefixSpace = false, bool? addBeginningOfSentence = false, bool? addEndOfSentence = false, string? unknownToken = \"<|endoftext|>\", \r\n                                    string? beginningOfSentenceToken = \"<|endoftext|>\", string? endOfSentenceToken = \"<|endoftext|>\") { }\r\n\r\n        public bool AddBeginningOfSentence { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> AddedTokens { get { throw null; } }\r\n\r\n        public bool AddEndOfSentence { get { throw null; } }\r\n\r\n        public bool AddPrefixSpace { get { throw null; } }\r\n\r\n        public int? BeginningOfSentenceId { get { throw null; } }\r\n\r\n        public string? BeginningOfSentenceToken { get { throw null; } }\r\n\r\n        public int? EndOfSentenceId { get { throw null; } }\r\n\r\n        public string? EndOfSentenceToken { get { throw null; } }\r\n\r\n        public string? UnknownToken { get { throw null; } }\r\n\r\n        public int? UnknownTokenId { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                               out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                               out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int CountTokens(ReadOnlySpan<char> text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int CountTokens(string text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        \r\n        public int IndexOfTokenCount(string text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                               out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int LastIndexOfTokenCount(string text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                 out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        \r\n        public IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                 bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<Token> Encode(string text, bool addPrefixSpace, bool addBeginningOfSentence, bool addEndOfSentence, out string? normalizedString, \r\n                                                                  bool considerPreTokenization = true,  bool considerNormalization = true) { throw null; }\r\n\r\n        public string? Decode(IEnumerable<int> ids, bool hasPrefixSpace, bool considerSpecialTokens) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```\r\n\r\n### Concrete Tokenizer - SentencePiece\r\n\r\n```C#\r\n    public sealed partial class SentencePiece : Tokenizer\r\n    {\r\n        internal SentencePiece() { }\r\n\r\n        public bool AddBeginningOfSentence { get { throw null; } }\r\n\r\n        public bool AddDummyPrefix { get { throw null; } }\r\n\r\n        public bool AddEndOfSentence { get { throw null; } }\r\n\r\n        public int BeginningOfSentenceId { get { throw null; } }\r\n\r\n        public string BeginningOfSentenceToken { get { throw null; } }\r\n\r\n        public bool ByteFallback { get { throw null; } }\r\n\r\n        public int EndOfSentenceId { get { throw null; } }\r\n\r\n        public string EndOfSentenceToken { get { throw null; } }\r\n\r\n        public bool EscapeWhiteSpaces { get { throw null; } }\r\n\r\n\r\n        public bool TreatWhitespaceAsSuffix { get { throw null; } }\r\n\r\n        public int UnknownId { get { throw null; } }\r\n\r\n        public string UnknownToken { get { throw null; } }\r\n\r\n        public IReadOnlyDictionary<string, int> Vocab { get { throw null; } }\r\n\r\n        public int CountTokens(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int CountTokens(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerNormalization, out string? normalizedString, out int textLength, int maxTokenCount = int.MaxValue) { throw null; }\r\n\r\n        public int CountTokens(string text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, bool considerNormalization, \r\n                                                            out string? normalizedString, out int textLength, int maxTokenCount = int.MaxValue) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(string text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                             out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                             out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int IndexOfTokenCount(string text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                            out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int IndexOfTokenCount(ReadOnlySpan<char> text, bool addBeginningOfSentence, bool addEndOfSentence, int maxTokenCount, out string? normalizedString, \r\n                                                             out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, bool addBeginningOfSentence, bool addEndOfSentence, bool considerNormalization, \r\n                                                              out string? normalizedString, out int tokenCount) { throw null; }\r\n\r\n        public IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool addBeginningOfSentence, bool addEndOfSentence, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n\r\n        public override Normalizer? Normalizer { get { throw null; } }\r\n        public override PreTokenizer? PreTokenizer { get { throw null; } }\r\n        public override int CountTokens(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int CountTokens(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? Decode(IEnumerable<int> ids) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(ReadOnlySpan<char> text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<Token> Encode(string text, out string? normalizedString, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override IReadOnlyList<int> EncodeToIds(string text, int maxTokenCount, out string? normalizedString, out int textLength, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int IndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(ReadOnlySpan<char> text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override int LastIndexOfTokenCount(string text, int maxTokenCount, out string? normalizedString, out int tokenCount, bool considerPreTokenization = true, bool considerNormalization = true) { throw null; }\r\n        public override string? MapIdToToken(int? id) { throw null; }\r\n        public override int? MapTokenToId(ReadOnlySpan<char> token) { throw null; }\r\n    }\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/7144","RelatedDescription":"Open issue \"Tokenizers Library Design\" (#7144)"},{"Id":"2266915088","IsPullRequest":false,"CreatedAt":"2024-04-27T08:37:12","Actor":"r-Larch","Number":"7143","RawContent":null,"Title":"[Tokenizers] Question regarding performance","State":"open","Body":"Hi, thanks for the effort put into the Microsoft.ML.Tokenizers!\r\n\r\nI'm the author of the last performance improvements in `SharpToken` library.\r\nSince MLTokenizers are faster now than SharpToken I looked into the sources to understand where this performance comes from.\r\n\r\n**Now I have a question (out of curiosity)**\r\n\r\nWhy is it required to copy a `ReadOnlySpan<char>` to a buffer, when the rest of the code just uses `ReadOnlySpan<char>` again?\r\n\r\n**TiktokenPreTokenizer.cs** line: 104\r\nhttps://github.com/dotnet/machinelearning/blob/72cfdf611a510ba0570170a708ddcc1a1928f329/src/Microsoft.ML.Tokenizers/PreTokenizer/TiktokenPreTokenizer.cs#L95-L107\r\n\r\n**PreTokenizer.cs** line: 74\r\nhttps://github.com/dotnet/machinelearning/blob/72cfdf611a510ba0570170a708ddcc1a1928f329/src/Microsoft.ML.Tokenizers/PreTokenizer/PreTokenizer.cs#L43-L54","Url":"https://github.com/dotnet/machinelearning/issues/7143","RelatedDescription":"Open issue \"[Tokenizers] Question regarding performance\" (#7143)"},{"Id":"2266549113","IsPullRequest":true,"CreatedAt":"2024-04-26T21:50:38","Actor":"sevenzees","Number":"7142","RawContent":null,"Title":"Allow developers to supply their own function to infer column data types from data while loading CSVs","State":"open","Body":"Fixes #7141\r\n\r\nCurrently when you use `LoadCsv` or `LoadCsvFromString` without supplying data types for each column, the code will try to guess the data types based on the data in the CSV file. This is good, but the problem is that the default type inference code only considers `bool`, `float`, `DateTime`, and `string` for column types. Sometimes the user may need another data type, such as `int`, `long`, or `double` (see [issue 6347](https://github.com/dotnet/machinelearning/issues/6347) for an example where someone had a problem with the `float` data type that was chosen by default) but not know the structure of the data ahead of time.\r\n\r\nI would like to be able to pass in my own custom type inference logic to override the default `GuessKind` implementation that is given in the library right now. If no custom guess type function is provided to the `LoadCsv` or `LoadCsvFromString` methods, then the code should work the same as it does today.","Url":"https://github.com/dotnet/machinelearning/pull/7142","RelatedDescription":"Open PR \"Allow developers to supply their own function to infer column data types from data while loading CSVs\" (#7142)"},{"Id":"2266545151","IsPullRequest":false,"CreatedAt":"2024-04-26T21:46:25","Actor":"sevenzees","Number":"7141","RawContent":null,"Title":"Allow developers to supply their own function to infer column data types from data while loading CSVs","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nCurrently when you use `LoadCsv` or `LoadCsvFromString` without supplying data types for each column, the code will try to guess the data types based on the data in the CSV file. This is good, but the problem is that the default type inference code only considers `bool`, `float`, `DateTime`, and `string` for column types. Sometimes the user may need another data type, such as `int`, `long`, or `double` (see [issue 6347](https://github.com/dotnet/machinelearning/issues/6347) for an example where someone had a problem with the `float` data type that was chosen by default) but not know the structure of the data ahead of time.\r\n\r\n**Describe the solution you'd like**\r\nI would like to be able to pass in my own custom type inference logic to override the default `GuessKind` implementation that is given in the library right now. If no custom guess type function is provided to the `LoadCsv` or `LoadCsvFromString` methods, then the code should work the same as it does today.\r\n\r\n**Describe alternatives you've considered**\r\nThe alternative is to call `LoadCsv`/`LoadCsvFromString` with `dataTypes` set to an array filled with `typeof(string)` for each column in your data, and then run your logic on the `DataFrame` with all string type columns to convert the columns to the data types that make sense for each column based on the data that is in each column.\r\n\r\n**Additional context**\r\nI already have implemented a fix for this issue that I would like to merge in with a pull request.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7141","RelatedDescription":"Open issue \"Allow developers to supply their own function to infer column data types from data while loading CSVs\" (#7141)"},{"Id":"2259274067","IsPullRequest":false,"CreatedAt":"2024-04-23T16:17:50","Actor":"chrisevans9629","Number":"7140","RawContent":null,"Title":"Get Loss During Training for Visualization (Learning Curve Graph)","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nI need a way to visualize how my model is learning during training, which is a comparison between training loss and test loss.\r\n\r\n**Describe the solution you'd like**\r\nAn event handler that enables the ability to extract loss during training.\r\n\r\n**Describe alternatives you've considered**\r\nRunning the model for x epochs, evaluating the model, then retraining the model in a loop.  This unfortunately does not work for all models, such as LightGbm that can't be retrained.\r\n\r\n```csharp\r\nvar kfold = ctx.BinaryClassification.CrossValidate(training, estimator, param.kfold);\r\nvar bestModel = kfold.OrderByDescending(p => p.Metrics.Accuracy).Select(p => p.Model).First();\r\n\r\nvar testOutput = bestModel.Transform(test);\r\n\r\nvar metrics = ctx.BinaryClassification.Evaluate(testOutput);\r\n\r\n// This code doesn't work as estimator is IEstimator<ITransform> and bestModel is ITransform. Not sure how you would do this...\r\nestimator = bestModel;\r\n```\r\n\r\n**Additional context**\r\nUltimately, I am trying to analyze what the models I'm comparing are actually doing and so far I haven't found any documentation or any straightforward way to do it.\r\n![image](https://github.com/dotnet/machinelearning/assets/33850520/79ca7603-a42b-4746-a8b3-76e7f2c3aa2a)\r\n","Url":"https://github.com/dotnet/machinelearning/issues/7140","RelatedDescription":"Open issue \"Get Loss During Training for Visualization (Learning Curve Graph)\" (#7140)"},{"Id":"2256140857","IsPullRequest":false,"CreatedAt":"2024-04-22T10:13:44","Actor":"matnatx","Number":"7137","RawContent":null,"Title":"Specify Categorical Features in LightGBM","State":"open","Body":"I'm trying to use LightGBM algorithm in ML.NET to train a model using a dataset.\r\n\r\nThe dataset has a number of categorical features such as \"gender\", \"race\",  \"country_of_birth\".\r\n\r\nI can convert/encode these categorical data as integers.\r\n\r\nMy question is how I can specify that these features are categorical and not (continuous variables), so that LightGBM can handle them as categorical data.\r\n\r\nI can use OneHotEncoding/OneHotHashEncoding, but I believe it's not the same.\r\n\r\nTIA","Url":"https://github.com/dotnet/machinelearning/issues/7137","RelatedDescription":"Open issue \"Specify Categorical Features in LightGBM\" (#7137)"}],"ResultType":"GitHubIssue"}},"RunOn":"2024-05-30T03:30:16.0731638Z","RunDurationInMilliseconds":423}