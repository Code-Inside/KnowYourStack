{"Data":{"GitHub":{"Issues":[{"Id":"1865835159","IsPullRequest":false,"CreatedAt":"2023-09-02T04:06:56","Actor":"pi-curst","Number":"6806","RawContent":null,"Title":"Append Datafarmes","State":"closed","Body":"**System Information (please complete the following information):**\r\n - .NET Version: [e.g. .NET 6.0.301]\r\n\r\n**Describe the bug**\r\n\r\nHi, Appending Dataframes (DataFrame.Append Method) doesn't seem to append based on column names, but appends based on the position. Is this normal?\r\n\r\n**To Reproduce**\r\n\r\n`var data = new DataFrame(); \r\n\r\nvar col1 = new StringDataFrameColumn(\"ColumnA\", new string[] { \"a\", \"b\", \"c\", \"d\", \"e\" });\r\nvar col2 = new Int32DataFrameColumn(\"ColumnB\", new int[] { 1, 2, 3, 4, 5 });\r\nvar col3 = new Int32DataFrameColumn(\"ColumnC\", new int[] { 10, 20, 30, 40, 50 });\r\nvar col4 = new StringDataFrameColumn(\"ColumnA\", new string[] { \"f\", \"g\", \"c\", \"d\", \"e\" });\r\nvar col5 = new Int32DataFrameColumn(\"ColumnB\", new int[] { 6, 7, 3, 4, 5 });\r\nvar col6 = new Int32DataFrameColumn(\"ColumnC\", new int[] { 100, 200, 300, 400, 500 });\r\n\r\nvar dataFrame1 = new DataFrame(col1, col2, col3);\r\nvar dataFrame2 = new DataFrame(col4, col6, col5);\r\n\r\nvar dataFrames = new List<DataFrame> { dataFrame1, dataFrame2 };\r\nvar resultDataFrame = dataFrame1.Append(dataFrame2.Rows);`\r\n\r\n\r\n**Exhibited behavior**\r\n\r\nindex | ColumnA | ColumnB | ColumnC\r\n-- | -- | -- | --\r\n0 | a | 1 | 10\r\n1 | b | 2 | 20\r\n2 | c | 3 | 30\r\n3 | d | 4 | 40\r\n4 | e | 5 | 50\r\n5 | f | 100 | 6\r\n6 | g | 200 | 7\r\n7 | c | 300 | 3\r\n8 | d | 400 | 4\r\n9 | e | 500 | 5\r\n\r\n\r\n**Expected behavior**\r\n\r\nindex | ColumnA | ColumnB | ColumnC\r\n-- | -- | -- | --\r\n0 | a | 1 | 10\r\n1 | b | 2 | 20\r\n2 | c | 3 | 30\r\n3 | d | 4 | 40\r\n4 | e | 5 | 50\r\n5 | f | 6 | 100\r\n6 | g | 7 | 200\r\n7 | c | 3 | 300\r\n8 | d | 4 | 400\r\n9 | e | 5 | 500\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6806","RelatedDescription":"Closed issue \"Append Datafarmes\" (#6806)"},{"Id":"1877864231","IsPullRequest":true,"CreatedAt":"2023-09-02T04:06:55","Actor":"asmirnov82","Number":"6808","RawContent":null,"Title":"Append dataframe rows based on column names","State":"closed","Body":"Fixes #6806 \r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/6808","RelatedDescription":"Closed or merged PR \"Append dataframe rows based on column names\" (#6808)"},{"Id":"1831274496","IsPullRequest":true,"CreatedAt":"2023-09-01T03:36:47","Actor":"asmirnov82","Number":"6782","RawContent":null,"Title":"Allow to define CultureInfo for parsing values on reading DataFrame from csv","State":"closed","Body":"Fixes #5652 \r\n","Url":"https://github.com/dotnet/machinelearning/pull/6782","RelatedDescription":"Closed or merged PR \"Allow to define CultureInfo for parsing values on reading DataFrame from csv\" (#6782)"},{"Id":"1845863129","IsPullRequest":true,"CreatedAt":"2023-09-01T03:36:33","Actor":"Lehonti","Number":"6792","RawContent":null,"Title":"File-scoped namespaces in files under `Prediction` (`Microsoft.ML.Core`)","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/6792","RelatedDescription":"Closed or merged PR \"File-scoped namespaces in files under `Prediction` (`Microsoft.ML.Core`)\" (#6792)"},{"Id":"1875097039","IsPullRequest":false,"CreatedAt":"2023-08-31T09:02:58","Actor":"CodedBeard","Number":"6807","RawContent":null,"Title":"GC pause caused by Tuple<int, int> on model load","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nMy app is quite sensitive to GC pauses, so I've been doing allocation profiling. I noticed that a lot of GC pauses are being caused by the usage of `Tuple<int, int>` within the `Attention` class when the model is loaded.\r\n\r\nhttps://github.com/dotnet/machinelearning/blob/aaf226c7e7c359edf27e663362e928e02c8b9d0f/src/Microsoft.ML.TorchSharp/AutoFormerV2/Attention.cs#L72-L79\r\n\r\n**Describe the solution you'd like**\r\nIn my own fork, I've switched to using `ValueTuple`, which has removed the allocations and thus GC pauses\r\n\r\n**Describe alternatives you've considered**\r\nI don't think there's an easier/cleaner fix for this.\r\n\r\n**Additional context**\r\nUsing `Microsoft.ML.TorchSharp` version `0.21.0-preview.23266.6`, I observed 240k allocations of `Tuple<int, int>` during model loading, coming in just after `Tensor`.\r\n\r\n<img width=\"364\" alt=\"ml-allocations\" src=\"https://github.com/dotnet/machinelearning/assets/4446559/532508f8-9bdb-48dc-82da-9ff0be517fb2\">\r\n\r\nAfter switching to `ValueTuple` on my own fork, these allocations were removed, resulting in a significant reduction in CG pauses during model loading.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6807","RelatedDescription":"Open issue \"GC pause caused by Tuple<int, int> on model load\" (#6807)"},{"Id":"1845851888","IsPullRequest":true,"CreatedAt":"2023-08-31T04:21:46","Actor":"Lehonti","Number":"6789","RawContent":null,"Title":"File-scoped namespaces in files under `Data` (`Microsoft.ML.Core`)","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/6789","RelatedDescription":"Closed or merged PR \"File-scoped namespaces in files under `Data` (`Microsoft.ML.Core`)\" (#6789)"},{"Id":"1845847250","IsPullRequest":true,"CreatedAt":"2023-08-31T04:21:20","Actor":"Lehonti","Number":"6788","RawContent":null,"Title":"File-scoped namespaces in files under `ComponentModel` (`Microsoft.ML.Core`)","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/6788","RelatedDescription":"Closed or merged PR \"File-scoped namespaces in files under `ComponentModel` (`Microsoft.ML.Core`)\" (#6788)"},{"Id":"1864410786","IsPullRequest":false,"CreatedAt":"2023-08-27T22:18:38","Actor":"l3aalteshuva","Number":"6804","RawContent":null,"Title":"ITransform","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/issues/6804","RelatedDescription":"Closed issue \"ITransform\" (#6804)"},{"Id":"1845853909","IsPullRequest":true,"CreatedAt":"2023-08-25T21:58:36","Actor":"Lehonti","Number":"6790","RawContent":null,"Title":"File-scoped namespaces in files under `EntryPoints` (`Microsoft.ML.Core`)","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/6790","RelatedDescription":"Closed or merged PR \"File-scoped namespaces in files under `EntryPoints` (`Microsoft.ML.Core`)\" (#6790)"},{"Id":"1845856421","IsPullRequest":true,"CreatedAt":"2023-08-25T21:58:05","Actor":"Lehonti","Number":"6791","RawContent":null,"Title":"File-scoped namespaces in files under `Environment` (`Microsoft.ML.Core`)","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/6791","RelatedDescription":"Closed or merged PR \"File-scoped namespaces in files under `Environment` (`Microsoft.ML.Core`)\" (#6791)"},{"Id":"1840363721","IsPullRequest":true,"CreatedAt":"2023-08-25T17:57:36","Actor":"zewditu","Number":"6785","RawContent":null,"Title":"Add TargetType to Type_convert","State":"closed","Body":"We are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [ ] There's a descriptive title that will make sense to other developers some time from now. \r\n- [ ] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [ ] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [ ] You have included any necessary tests in the same PR.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/6785","RelatedDescription":"Closed or merged PR \"Add TargetType to Type_convert\" (#6785)"},{"Id":"1865657667","IsPullRequest":true,"CreatedAt":"2023-08-24T19:56:15","Actor":"michaelgsharp","Number":"6805","RawContent":null,"Title":"removed deprecated yosemite brew","State":"closed","Body":"Removed the deprecated reference to yosemite in the mac os brew file.","Url":"https://github.com/dotnet/machinelearning/pull/6805","RelatedDescription":"Closed or merged PR \"removed deprecated yosemite brew\" (#6805)"},{"Id":"1864090498","IsPullRequest":false,"CreatedAt":"2023-08-23T22:17:19","Actor":"saibaldas","Number":"6803","RawContent":null,"Title":"Using Roberta-base fine-tuned for boolq exported to ONNX in ML.NET ","State":"open","Body":"Roberta-base fine-tuned for the boolq dataset uses the tokenizer encode_plus to encode both the question and context. When exported to ONNX , the inputs are \r\n\r\nname: input_ids\r\ntensor: int64[batch_size,sequence_length]\r\n\r\nname: attention_mask\r\ntensor: int64[batch_size,sequence_length]\r\n\r\nPlease suggest the steps to run the ONNX for inference in ML.NET . The BertTokenizer can't be used as the tokenizer is BPE\r\nTried to use the https://gist.github.com/luisquintanilla/bc91de8668cfa7c3755b20329fadd027 without much success \r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6803","RelatedDescription":"Open issue \"Using Roberta-base fine-tuned for boolq exported to ONNX in ML.NET \" (#6803)"},{"Id":"1863686875","IsPullRequest":false,"CreatedAt":"2023-08-23T16:49:02","Actor":"torronen","Number":"6802","RawContent":null,"Title":"AutoML: consider adding precision in checkpoint file","State":"open","Body":"Nightly, VS 2022, .NET 7.0\r\n\r\nThe checkpoint file has columns with 3-digit precision. In some cases I see loss being exactly the same on every run in the checkpoint file. I would like to confirm it it is exactly (or almost) same prediction. Therefore I would like to see 5-digit values in this file. \r\n\r\nI think, in difficult problems it may also help the tuner to go to correct direction. \r\n\r\nI know it is a minor issue to go 0.01% level but I also think there is no downside to adding precision to this file.\r\n\r\n**Example**\r\nCost-frugal tuner. Are these results not improving because of the type of the data, or because the changes to params are too small? \r\n![image](https://github.com/dotnet/machinelearning/assets/26261427/aefb41af-5ca0-4171-ae65-57b8fdd8b1bd)\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6802","RelatedDescription":"Open issue \"AutoML: consider adding precision in checkpoint file\" (#6802)"},{"Id":"1862684347","IsPullRequest":false,"CreatedAt":"2023-08-23T06:52:19","Actor":"superichmann","Number":"6801","RawContent":null,"Title":"Predicted values are not at original scale after applying preFeaturizer","State":"open","Body":"When applying a preFeaturizer on the target column\r\n`var logTransformer = mlContext.Transforms.NormalizeMinMax(\"Label\",fixZero:true);`\r\nwithin a regression experiment, the predicted values\r\n`IDataView preds = result.Model.Transform(predictMe);`\r\nare in the wrong scale.\r\n\r\nOriginal values:\r\n10\r\n22\r\n33\r\n12\r\n6\r\n6\r\n5\r\n\r\nPredictions **without** preFeaturizer:\r\n8.593365\r\n19.133606\r\n19.094164\r\n11.576735\r\n11.040246\r\n10.156776\r\n9.546574\r\n\r\nPredictions **with** preFeaturizer:\r\n0.06661523\r\n0.14832252\r\n0.14801677\r\n0.08974213\r\n0.08558331\r\n0.0787347\r\n0.07400444\r\n\r\nas you can see, the predictions with the preFeaturizer applied are in a totally different scale.\r\n\r\nHow can I actually get the predicted values in the scale of the original target column and not in the scale of the applied transformer? ","Url":"https://github.com/dotnet/machinelearning/issues/6801","RelatedDescription":"Open issue \"Predicted values are not at original scale after applying preFeaturizer\" (#6801)"},{"Id":"1860312887","IsPullRequest":false,"CreatedAt":"2023-08-21T23:28:06","Actor":"MaxGrekhov","Number":"6800","RawContent":null,"Title":"BPE Tokenizer doesn't allow reading carriage return symbol","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: windows 11\r\n - ML.NET Version: [0.20.1](https://www.nuget.org/packages/Microsoft.ML.Tokenizers/0.20.1)\r\n - .NET Version: .NET 7\r\n\r\n**Describe the bug**\r\nBPE Tokenizer doesn't allow reading carriage return symbol.\r\nhttps://github.com/dotnet/machinelearning/blob/077a6b81966dc2c514572568917f36cb94e08ac4/src/Microsoft.ML.Tokenizers/Model/BPE.cs#L297\r\nReading text with ReadLines causes inability to read the carriage return symbol in merges.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n```csharp\r\nusing System.Text.Json;\r\nusing Microsoft.ML.Tokenizers;\r\n\r\nvar vocabFilePath = @\"vocab.json\";\r\nvar mergeFilePath = @\"merges.txt\";\r\ntry\r\n{\r\n    var tokenizer = new Tokenizer(new Bpe(vocabFilePath, mergeFilePath, \"<unk>\"));\r\n\r\n    var input = \" Test String \";\r\n\r\n    var tokenizerEncodedResult = tokenizer.Encode(input);\r\n    Console.WriteLine(JsonSerializer.Serialize(tokenizerEncodedResult.Ids));\r\n    var tokenizerDecodedResult = tokenizer.Decode(tokenizerEncodedResult.Ids);\r\n    Console.WriteLine(tokenizerDecodedResult);\r\n}\r\ncatch (Exception e)\r\n{\r\n    Console.WriteLine(e);\r\n}\r\n```\r\n\r\n```\r\nSystem.InvalidOperationException: Invalid merger file format at line: 3869\r\n   at Microsoft.ML.Tokenizers.Bpe.ConvertMergesToHashmap(String mergesFile)\r\n   at Microsoft.ML.Tokenizers.Bpe.ReadFile(String vocab, String merges)\r\n   at Microsoft.ML.Tokenizers.Bpe..ctor(String vocabFile, String mergesFile, String unknownToken, String continuingSubwordPrefix, String endOfWordSuffix)\r\n   at Program.<Main>$(String[] args) in D:\\dev\\personal\\ai\\Program.cs:line 8\r\n```\r\n\r\n**Expected behavior**\r\nBPE should process merges without exception\r\n\r\n**Simple solution**\r\nRead a json instead\r\n```csharp\r\n\r\ninternal static Vec<(string, string)> ConvertMergesToHashmap(string? mergesFile)\r\n{\r\n    if (mergesFile is null)\r\n    {\r\n        return new Vec<(string, string)>();\r\n    }\r\n\r\n    Vec<(string, string)> merges = new(1000);\r\n\r\n    int lineNumber = 0;\r\n    List<string> mergesList;\r\n    using (Stream stream = File.OpenRead(mergesFile))\r\n    {\r\n        mergesList = JsonSerializer.Deserialize<List<string>>(stream);\r\n    }\r\n    foreach (string line in mergesList)\r\n    {\r\n        lineNumber++;\r\n        if (line.StartsWith(\"#version\", StringComparison.Ordinal) || line.Length == 0)\r\n        {\r\n            continue;\r\n        }\r\n        int index = line.IndexOf(' ');\r\n        if (index < 0 || index == line.Length - 1 || line.IndexOf(' ', index + 1) >= 0)\r\n        {\r\n            throw new InvalidOperationException($\"Invalid merger file format at line: {lineNumber}\");\r\n        }\r\n        merges.Push((line.Substring(0, index), line.Substring(index + 1)));\r\n    }\r\n\r\n    return merges;\r\n}\r\n```\r\n\r\n**Additional context**\r\nI was able to fix the exception. However, BPE algorithm generates different results in comparison to google's sentencepiece that is widely used in ML.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6800","RelatedDescription":"Open issue \"BPE Tokenizer doesn't allow reading carriage return symbol\" (#6800)"},{"Id":"1857824134","IsPullRequest":false,"CreatedAt":"2023-08-19T18:04:58","Actor":"gsgou","Number":"6799","RawContent":null,"Title":"implement conv1d cross platform without a dependency on TorchSharp","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nI need to pre-process data before invoking a ONNX model in xamarin and .net maui.\r\nVery often this requires conv1d but TorchSharp isn't available on mobile.\r\nDo you plan to add some basic operations as conv1d in the lib soon?\r\n\r\n[conv1d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html)\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6799","RelatedDescription":"Open issue \"implement conv1d cross platform without a dependency on TorchSharp\" (#6799)"},{"Id":"1852898171","IsPullRequest":false,"CreatedAt":"2023-08-16T09:54:04","Actor":"eperegrine","Number":"6798","RawContent":null,"Title":"Clarification on the privacy of data","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\nIn the documentation it is stated that the training and use of models is done locally, however there is not explicit statement about the secuirty of that data - is there any analystics or usage of local training data. Would it be safe to train on commercially sensitive data (assuming responsible use of the model)?\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\nAn answer to my question and ideally an update to documentation addressing this concern\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6798","RelatedDescription":"Open issue \"Clarification on the privacy of data\" (#6798)"},{"Id":"1849918359","IsPullRequest":false,"CreatedAt":"2023-08-14T14:37:05","Actor":"superichmann","Number":"6797","RawContent":null,"Title":"Setting CategoricalColumnNames is not Actually Doing Anything","State":"open","Body":"**System Information (please complete the following information):**\r\n - OS & Version: Windows 10\r\n - ML.NET Version: Microsoft.ML, 2.0.1\r\nMicrosoft.ML.AutoML, 0.20.1\r\nBUT ALSO ON LATEST PreRelease from dotnet-libraries\r\n - .NET Version: 7.0\r\n\r\n**Describe the bug**\r\nColumnInformation CategoricalColumnNames suppose to instruct automl to consider specific columns as categories which in turn should increase precision on the training.\r\n\r\nIn the  past (couple of months ago) I did a test with setting columns to CategoricalColumnNames, it was some prerelease version from  dotnet-libraries and the training result actually was better and training time was much longer.\r\n\r\nToday, I have re-tested this and training time and score is exactly the same as not setting CategoricalColumnNames.\r\n\r\n**To Reproduce**\r\ndownload code (change extansion to ipynb) [ColumnInformationDoesNotWork.txt](https://github.com/dotnet/machinelearning/files/12335917/ColumnInformationDoesNotWork.txt)\r\nopen with vscode\r\ndownload train.csv from [here](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=train.csv)\r\nrun ipynb\r\nsee scores are similiar for both with CI and without.\r\n\r\n**Expected behavior**\r\nWhen adding CI the training should handle the data differently and as well produce a better score with longer training time.\r\n\r\n**Screenshots, Code, Sample Projects**\r\n[ColumnInformationDoesNotWork.txt](https://github.com/dotnet/machinelearning/files/12335917/ColumnInformationDoesNotWork.txt)\r\n\r\n**Additional context**\r\nI might be missing something in parameter initialization of the process, if so please instruct me on what exactly to set.\r\n\r\n**Code Snippets**\r\n```\r\nvar set = new RegressionExperimentSettings();\r\nset.MaxExperimentTimeInSeconds = 1; // Maxmodels bypass\r\nset.Trainers.Clear();\r\nset.Trainers.Add(RegressionTrainer.FastForest);\r\nRegressionExperiment experiment = mlContext.Auto().CreateRegressionExperiment(set);\r\nColumnInformation CI = new ColumnInformation();\r\nCI.CategoricalColumnNames.Add(\"family\");\r\nCI.CategoricalColumnNames.Add(\"store_nbr\");\r\nvar x1 = experiment.Execute(train,CI);\r\nvar score1 = x1.BestRun.ValidationMetrics.RSquared;\r\nConsole.WriteLine(\"Result with categoricals definitions: \" + score1);\r\n```\r\n\r\n```\r\nvar no = new RegressionExperimentSettings();\r\nno.MaxExperimentTimeInSeconds = 1; // Maxmodels bypass\r\nno.Trainers.Clear();\r\nno.Trainers.Add(RegressionTrainer.FastForest);\r\nCI.CategoricalColumnNames.Clear();\r\nRegressionExperiment experiment2 = mlContext.Auto().CreateRegressionExperiment(no);\r\nvar x2 = experiment2.Execute(train,CI);\r\nvar score2 = x2.BestRun.ValidationMetrics.RSquared;\r\nConsole.WriteLine(\"Result without categoricals definitions: \" + score2);\r\n```\r\n\r\nThanks!\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6797","RelatedDescription":"Open issue \"Setting CategoricalColumnNames is not Actually Doing Anything\" (#6797)"},{"Id":"1848551968","IsPullRequest":false,"CreatedAt":"2023-08-13T11:28:58","Actor":"torronen","Number":"6796","RawContent":null,"Title":"Add baseline metrics to trainer results","State":"open","Body":"Datasets are almost never perfectly balanced. That means that an impressive metric, let's say 80% binary classification accuracy, could be worse than selecting always the most common category.  Therefore, I would like to automatically calculate the baseline metrics, for example:\r\n - null accuracy\r\n - distribution of items according to categories\r\n - baseline metric for ranking of a random order\r\n\r\nI think ideally, I would be able to read the baseline metrics as easily as the training data metrics. I think it should be also promoted as during the currenty AI summer I see many developers quickly build AI-enabled apps without any or much  validation, i.e. whatever GPT models says is enough without validation or even real prompt engineering with comparison to ground truth; whatever Model builder says is the final validation and so on. \r\n\r\nI think this would be in line with promoting fairness values. Equally incorrect results for everyone is not fair. Model Builder could probably even show graphically how much better model performs on test dataset than random or null accuracy.\r\n\r\nFirst I think it would be important to record what is the ideal way to evaluate metrics. For example, comparison to null accuracy would probably be good for binary classification but not others. For ranking, there seems to be a few options. Implementation is probably not terribly difficult.\r\n\r\n1. Decide baseline metrics for each ML task\r\n2. Decide programming interface and where to calculate the metrics\r\n3. Implementation","Url":"https://github.com/dotnet/machinelearning/issues/6796","RelatedDescription":"Open issue \"Add baseline metrics to trainer results\" (#6796)"},{"Id":"1848310341","IsPullRequest":false,"CreatedAt":"2023-08-13T00:10:16","Actor":"ooples","Number":"6795","RawContent":null,"Title":"Schema mismatch for label column 'Label': expected Single, got Vector<Single> (Parameter 'labelCol')","State":"open","Body":"I'm trying out this nuget package and experimenting with the different normalization options for regression and prediction trainers. My code is working perfectly when I use Single values (float) but I saw that there were bunch of normalization options that only seemed to work using a vector of Single values so I get the error when I change all of my float values to a float array. Am I just missing something obvious?\r\n\r\nFYI I'm using ML.NET 3.0.0-preview.23266.6 for this example\r\n\r\n```cs\r\nvar trainingCount = 50;\r\nvar modelInputList = new List<ModelInput>();\r\nvar estCount = valuesList.Any() ? valuesList.First().ValueList.Count : 0;\r\n\r\nfor (int j = 0; j < estCount; j++)\r\n{\r\n    var modelInput = new ModelInput();\r\n\r\n    var actual = j < estCount - 1 ? actualList[j] : 0;\r\n    modelInput.Actual = new float[] { Convert.ToSingle(actual) };\r\n\r\n    for (int k = 0; k < valueCount; k++)\r\n    {\r\n        var rvItem = Convert.ToSingle(valuesList[k].ValueList[j]);\r\n\r\n        switch (k)\r\n        {\r\n            case 0:\r\n                modelInput.Input1 = new float[] { rvItem };\r\n                break;\r\n            case 1:\r\n                modelInput.Input2 = new float[] { rvItem };\r\n                break;\r\n            case 2:\r\n                modelInput.Input3 = new float[] { rvItem };\r\n                break;\r\n            default:\r\n                break;\r\n        }\r\n    }\r\n\r\n    modelInputList.Add(modelInput);\r\n}\r\n\r\nvar firstHalf = mlContext.Data.LoadFromEnumerable(modelInputList.Take(trainingCount));\r\nvar secondHalf = mlContext.Data.LoadFromEnumerable(modelInputList.Skip(trainingCount));\r\nvar dataProcessPipeline = mlContext.Transforms\r\n                    .CopyColumns(\"Label\", nameof(ModelInput.Actual))\r\n                    .Append(mlContext.Transforms.NormalizeBinning(outputColumnName: nameof(ModelInput.Input1)))\r\n                    .Append(mlContext.Transforms.NormalizeBinning(outputColumnName: nameof(ModelInput.Input2)))\r\n                    .Append(mlContext.Transforms.NormalizeBinning(outputColumnName: nameof(ModelInput.Input3)))\r\n                    .Append(mlContext.Transforms.Concatenate(\"Features\", nameof(ModelInput.Input1),\r\n                        nameof(ModelInput.Input2), nameof(ModelInput.Input3)));\r\nvar trainer = mlContext.Regression.Trainers.OnlineGradientDescent();\r\nvar trainingPipeline = dataProcessPipeline.Append(trainer);\r\nvar trainedModel = trainingPipeline.Fit(firstHalf); // getting the exception here\r\nvar trainingData = trainedModel.Transform(firstHalf);\r\nvar predictions = trainedModel.Transform(secondHalf);\r\n\r\npublic class ModelInput\r\n{\r\n    [LoadColumn(0)]\r\n    [VectorType(1)]\r\n    public float[] Actual { get; set; }\r\n\r\n    [LoadColumn(1)]\r\n    [VectorType(1)]\r\n    public float[] Input1 { get; set; }\r\n\r\n    [LoadColumn(2)]\r\n    [VectorType(1)]\r\n    public float[] Input2 { get; set; }\r\n\r\n    [LoadColumn(3)]\r\n    [VectorType(1)]\r\n    public float[] Input3 { get; set; }\r\n}","Url":"https://github.com/dotnet/machinelearning/issues/6795","RelatedDescription":"Open issue \"Schema mismatch for label column 'Label': expected Single, got Vector<Single> (Parameter 'labelCol')\" (#6795)"},{"Id":"1845883949","IsPullRequest":true,"CreatedAt":"2023-08-10T20:51:07","Actor":"Lehonti","Number":"6794","RawContent":null,"Title":"File-scoped namespaces in files directly under `Microsoft.ML.Core`","State":"open","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/6794","RelatedDescription":"Open PR \"File-scoped namespaces in files directly under `Microsoft.ML.Core`\" (#6794)"},{"Id":"1845881904","IsPullRequest":true,"CreatedAt":"2023-08-10T20:49:26","Actor":"Lehonti","Number":"6793","RawContent":null,"Title":"File-scoped namespaces in files under `Utilities` (`Microsoft.ML.Core`)","State":"open","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/6793","RelatedDescription":"Open PR \"File-scoped namespaces in files under `Utilities` (`Microsoft.ML.Core`)\" (#6793)"},{"Id":"1845842012","IsPullRequest":true,"CreatedAt":"2023-08-10T20:21:43","Actor":"Lehonti","Number":"6787","RawContent":null,"Title":"File-scoped namespaces in files under `CommandLine` (`Microsoft.ML.Core`)","State":"open","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/6787","RelatedDescription":"Open PR \"File-scoped namespaces in files under `CommandLine` (`Microsoft.ML.Core`)\" (#6787)"},{"Id":"1840516080","IsPullRequest":false,"CreatedAt":"2023-08-09T07:49:58","Actor":"torronen","Number":"6786","RawContent":null,"Title":"CV + SamplingKey: How to drop from being as a training feature?","State":"closed","Body":"I am using cross-validation with a sampling key, because I know the sampling key column is a shortcut and will lead to overfitting.\r\n\r\nI can also observe this by having a look at the global feature index of the trained model:\r\nSampling.1  1\r\nSampling.4  0.85616875\r\n\r\nMy challenge is now how do I drop the sampling key column from being used as a feature to train on?\r\n\r\nSimplified code below:\r\n\r\n```\r\n  experiment\r\n                .SetPipeline(pipeline)\r\n                .SetRegressionMetric(metric, labelColumn: columnInference.ColumnInformation.LabelColumnName)\r\n                .SetTrainingTimeInSeconds(trainingTimeSeconds)\r\n                .SetDataset(data, fold: NumFolds, samplingKeyColumnName: SamplingKeyColumn)\r\n                .SetCostFrugalTuner();\r\n                \r\n            TrialResult experimentResults = await experiment.RunAsync(cts.Token);\r\n```\r\n\r\nIf I drop as part of pipeline, I will get error \"failed with exception Could not find input column 'Sampling' (Parameter 'inputSchema')\"\r\n\r\n\r\nRemoving sampling key from columninformation did not help\r\n```\r\n //Remove sampling column key\r\n columnInference.ColumnInformation.CategoricalColumnNames.Remove(SamplingKeyColumn);\r\n```\r\n                \r\n                ","Url":"https://github.com/dotnet/machinelearning/issues/6786","RelatedDescription":"Closed issue \"CV + SamplingKey: How to drop from being as a training feature?\" (#6786)"},{"Id":"1838289055","IsPullRequest":false,"CreatedAt":"2023-08-06T17:45:12","Actor":"torronen","Number":"6784","RawContent":null,"Title":"AutoML2.0: Ranking ","State":"open","Body":"I believe ranking is current missing from AutoML 2.0 and thus missing from the improved performance. Are there plan to include ranking in AutoML 2.0, and any estimated timeframes?\r\n\r\nctx.Auto().CreateRankingExperiment\r\nbut no ctx.Auto().Regression\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6784","RelatedDescription":"Open issue \"AutoML2.0: Ranking \" (#6784)"},{"Id":"1836532772","IsPullRequest":false,"CreatedAt":"2023-08-04T11:03:01","Actor":"torronen","Number":"6783","RawContent":null,"Title":"Include params in .zip file","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nI may have a model that has been in use for a while. If I was not careful, the params are lost. When updating the model, and retraining, I may accidentally use different parameters or AutoML may arrive at different end result. This can lead to a model with different charasteristics. I think it especially happens when calibrator is used, the distribution on scores seems often very different across models.\r\n\r\n**Describe the solution you'd like**\r\nOption A) save params inside the zip. There is already a text file with the version number, so why not also training paramterers. One could also consider including performance metrics, training data stats, or even dataset name.  \r\n\r\nOption B) expose a way to load the model and then read the training parameters, at least those that exists, are readable or can be logically infered (such as how many leafs and trees)\r\n\r\n**Describe alternatives you've considered**\r\nIt is possible to open the contents on the zip file in notepad, and sometimes I can see the params. \r\n\r\n**Additional context**\r\nSaving this information and especially dataset information may cause data leaks. For example a desktop app manufacturer might not want to make it easy to replicate his model. I think this is not big concern in my opinion, but including documentation on what is inside zip file and how to remove it can be useful. However, I do not think many users will extract zip files, perhaps even with .zip removed.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6783","RelatedDescription":"Open issue \"Include params in .zip file\" (#6783)"},{"Id":"1830514689","IsPullRequest":true,"CreatedAt":"2023-08-03T19:24:12","Actor":"zewditu","Number":"6781","RawContent":null,"Title":"Add QA sweepable estimator in AutoML ","State":"closed","Body":"We are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [ ] There's a descriptive title that will make sense to other developers some time from now. \r\n- [ ] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [ ] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [ ] You have included any necessary tests in the same PR.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/6781","RelatedDescription":"Closed or merged PR \"Add QA sweepable estimator in AutoML \" (#6781)"},{"Id":"1828040573","IsPullRequest":false,"CreatedAt":"2023-07-30T19:17:58","Actor":"80LevelElf","Number":"6780","RawContent":null,"Title":"Ability to express the maximum variance in the specified properties","State":"open","Body":"**Is your feature request related to a problem? Please describe.**\r\nIt is very common approach (at least at our side) when you train classifier or regression model to sort through the parameters of this model to find out the best combination for biggest predict.\r\n\r\nFor example we have some subscription form on our site and we want to maximize subscription conversion.\r\nWe know:\r\n- Current site page info\r\n- Current user info (device and so on)\r\n- Behavior info (session number)\r\n\r\nAlso we can change:\r\n- Form image\r\n- Form CTA text\r\n- Form text\r\n- Discount type\r\n\r\nSo we train binary classification model with site, user and behavior properties + form image, CTA text, form text, discount type properties to understand how different params combination influence on conversion rate.\r\n\r\nWe assume that different properties are related which each other somehow. So if we train 100 models and check weight of used properties we will have different mostly used properties in each model. Because the same variance could be expressed via different properties.\r\n\r\n**Describe the solution you'd like**\r\nIt would be really great to add some option for AutoML experiments to mark properties we mostly prefer to use. Like in this case - form image, CTA text, form text, discount type properties with some specified epsilon of quality we can lose because of that.\r\n\r\n            var settings = new BinaryExperimentSettings()\r\n            {\r\n                OptimizingMetric = BinaryClassificationMetric.AreaUnderRocCurve,\r\n                PreferedPropertiesSettings = new ExperimentPreferedPropertiesSettings \r\n                        {\r\n                                Properties = new [] {\"Prop1\", \"Prop2\"},\r\n                                AcceptableQualityLossInPercent = 5\r\n                        }\r\n            };","Url":"https://github.com/dotnet/machinelearning/issues/6780","RelatedDescription":"Open issue \"Ability to express the maximum variance in the specified properties\" (#6780)"},{"Id":"1827822888","IsPullRequest":false,"CreatedAt":"2023-07-30T06:27:22","Actor":"aarindam10","Number":"6779","RawContent":null,"Title":"Method 'call' in type 'Microsoft.ML.TorchSharp.NasBert.Models.TransformerEncoder' from assembly 'Microsoft.ML.TorchSharp","State":"open","Body":"the bug was reported earlier in following post.\r\nhttps://github.com/dotnet/machinelearning/issues/6665\r\nthough the bug was marked as resolved and closed the thread, but still the issues is there. \r\n\r\nReopening the issue:\r\n----------------------\r\nNow I am using following versions.\r\n PackageReference Include=\"Microsoft.ML\" Version=\"2.0.1\" \r\n PackageReference Include=\"Microsoft.ML.TorchSharp\" Version=\"0.20.1\" \r\n PackageReference Include=\"TorchSharp\" Version=\"0.100.3\" \r\n PackageReference Include=\"TorchSharp-cpu\" Version=\"0.100.3\" \r\n\r\nstill getting following errors\r\n\r\nSystem.TypeLoadException\r\n  HResult=0x80131522\r\n  Message=Method 'call' in type 'Microsoft.ML.TorchSharp.NasBert.Models.TransformerEncoder' from assembly 'Microsoft.ML.TorchSharp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51' does not have an implementation.\r\n  Source=Microsoft.ML.TorchSharp\r\n  StackTrace:\r\n   at Microsoft.ML.TorchSharp.NasBert.NasBertTrainer`2.TrainerBase..ctor(NasBertTrainer`2 parent, IChannel ch, IDataView input)\r\n   at Microsoft.ML.TorchSharp.NasBert.TextClassificationTrainer.Trainer..ctor(NasBertTrainer`2 parent, IChannel ch, IDataView input)\r\n   at Microsoft.ML.TorchSharp.NasBert.TextClassificationTrainer.CreateTrainer(NasBertTrainer`2 parent, IChannel ch, IDataView input)\r\n   at Microsoft.ML.TorchSharp.NasBert.NasBertTrainer`2.Fit(IDataView input)\r\n   at Microsoft.ML.Data.EstimatorChain`1.Fit(IDataView input)\r\n   at Microsoft.ML.Data.EstimatorChain`1.Fit(IDataView input)\r\n   at CustomerReview.CustomerReviewAnalysis.ReviewAnalysis() in F:\\MachineL\\MLNet\\CustomerReviewAnalysis\\CustomerReviewAnalysis.cs:line 80\r\n   at Program.<Main>$(String[] args) in F:\\MachineL\\MLNet\\CustomerReviewAnalysis\\Program.cs:line 6\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/6779","RelatedDescription":"Open issue \"Method 'call' in type 'Microsoft.ML.TorchSharp.NasBert.Models.TransformerEncoder' from assembly 'Microsoft.ML.TorchSharp\" (#6779)"}],"ResultType":"GitHubIssue"}},"RunOn":"2023-09-03T03:30:20.9097767Z","RunDurationInMilliseconds":563}