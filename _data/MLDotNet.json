{"Data":{"GitHub":{"Issues":[{"Id":"785644572","IsPullRequest":false,"CreatedAt":"2021-01-14T04:39:21","Actor":"clthorre","Number":"5589","RawContent":null,"Title":"Install CLI tool using nuget. ","State":"open","Body":"Hello. I am wondering if there is a way to get the command line utilities while installing using nuget.\r\n\r\nCurrently I have a system where I clone the repo, and do something along these lines.\r\n```\r\nbash build.sh -release  \r\ndotnet publish -c Release --no-build  machinelearning/src/Microsoft.ML.Console --output mlnet\r\n```\r\n\r\nThen in the resulting folder I have the following files, MML.dll, MML.pdb, MML.xml, MML.runtimeconfig.json, MML.deps.json and many other dll files of the library.\r\n\r\nI use this by doing dotnet mlnet/MML.dll <my argument string>\r\n\r\nI am wondering if it is possible to use nuget to install the dotnet machine learning package and have the same files available?\r\nI have tried installing https://www.nuget.org/packages/Microsoft.ML/ and https://www.nuget.org/packages/mlnet/.\r\nThe first does not have the same MML.dll file I would use, and the second is a command line tool but has a totally different API.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5589","RelatedDescription":"Open issue \"Install CLI tool using nuget. \" (#5589)"},{"Id":"785597157","IsPullRequest":false,"CreatedAt":"2021-01-14T02:22:56","Actor":"mthalman","Number":"5588","RawContent":null,"Title":"Broken link for SqueezeNet","State":"open","Body":"\r\nThe link for SqueezeNet needs to be changed from https://github.com/onnx/models/tree/master/squeezenet to https://github.com/onnx/models/tree/master/vision/classification/squeezenet\r\n\r\n\r\n---\r\n#### Document Details\r\n\r\n⚠ *Do not edit this section. It is required for docs.microsoft.com ➟ GitHub issue linking.*\r\n\r\n* ID: 272720a2-7bf8-4e2c-743c-114ca1ce887b\r\n* Version Independent ID: 6c82d0d0-a8cc-868c-3d20-1a210658c8d2\r\n* Content: [ImageEstimatorsCatalog.ExtractPixels(TransformsCatalog, String, String, ImagePixelExtractingEstimator+ColorBits, ImagePixelExtractingEstimator+ColorsOrder, Boolean, Single, Single, Boolean) Method (Microsoft.ML)](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.imageestimatorscatalog.extractpixels?view=ml-dotnet)\r\n* Content Source: [dotnet/xml/Microsoft.ML/ImageEstimatorsCatalog.xml](https://github.com/dotnet/ml-api-docs/blob/live/dotnet/xml/Microsoft.ML/ImageEstimatorsCatalog.xml)\r\n* Product: **dotnet-ml-api**\r\n* GitHub Login: @natke\r\n* Microsoft Alias: **nakersha**","Url":"https://github.com/dotnet/machinelearning/issues/5588","RelatedDescription":"Open issue \"Broken link for SqueezeNet\" (#5588)"},{"Id":"781449346","IsPullRequest":false,"CreatedAt":"2021-01-13T23:47:42","Actor":"voges316","Number":"5580","RawContent":null,"Title":"Mlnet CLI tool 16.2 generated model fails when being loaded to create a prediction engine","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**: CentOS 7.9\r\n- **.NET Version (eg., dotnet --info)**: 3.1.404\r\n\r\n### Issue\r\nUpgrading from the mlnet cli tool from v0.15.1 to v16.2 I am unable to import a generated model using the C# api that is generated using the most recent mlnet cli tool on a yelp sentiment dataset.\r\n\r\n- **What did you do?**\r\nTried to import a model generated from mlnet cli v16.2 and predict an input.\r\n```\r\nITransformer predictionPipeline = mlContext.Model.Load(\"models/MLModelv16.2.zip\", out predictionPipelineSchema);\r\nPredictionEngine<ModelInput, ModelOutput> predictionEngine = mlContext.Model.CreatePredictionEngine<ModelInput, ModelOutput>(predictionPipeline);\r\nModelInput input = new ModelInput{ Col0 = \"Meh, food was cold.\" };\r\nvar result = predictionEngine.Predict(input);\r\n```\r\n\r\n- **What happened?**\r\n\r\nI get the following error\r\n```\r\nUnhandled exception. System.ArgumentOutOfRangeException: Could not find input column 'col1' (Parameter 'inputSchema')\r\n   at Microsoft.ML.Data.OneToOneTransformerBase.CheckInput(DataViewSchema inputSchema, Int32 col, Int32& srcCol)\r\n   at Microsoft.ML.Data.OneToOneTransformerBase.OneToOneMapperBase..ctor(IHost host, OneToOneTransformerBase parent, DataViewSchema inputSchema)\r\n   at Microsoft.ML.Transforms.ValueToKeyMappingTransformer.Mapper..ctor(ValueToKeyMappingTransformer parent, DataViewSchema inputSchema)\r\n   at Microsoft.ML.Transforms.ValueToKeyMappingTransformer.MakeRowMapper(DataViewSchema schema)\r\n   at Microsoft.ML.Data.RowToRowTransformerBase.Microsoft.ML.ITransformer.GetRowToRowMapper(DataViewSchema inputSchema)\r\n   at Microsoft.ML.Data.TransformerChain`1.Microsoft.ML.ITransformer.GetRowToRowMapper(DataViewSchema inputSchema)\r\n   at Microsoft.ML.Data.TransformerChain`1.Microsoft.ML.ITransformer.GetRowToRowMapper(DataViewSchema inputSchema)\r\n   at Microsoft.ML.Data.TransformerChain`1.Microsoft.ML.ITransformer.GetRowToRowMapper(DataViewSchema inputSchema)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MLConsoleApp.Program.Main(String[] args) in /home/devel/code/mlnet_example/MLConsoleApp/Program.cs:line 34\r\n```\r\n- **What did you expect?**\r\nUsing v0.15.1 of the mlnet cli tool I am able to train a model, and then in dotnet load the model, create a prediction engine and score a new input correctly.\r\nI expected to have the same behavior using the latest mlnet cli tool, but it doesn't work.\r\n\r\n### Source code / logs\r\n\r\nPrevious mlnet cli tool:\r\nMlnet --version => 0.15.28007.4\r\nYelp labelled datset from here http://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\r\n\r\nPrevious command to train a model on a dataset:\r\nmlnet auto-train --task binary-classification --dataset data/yelp_labelled.txt --label-column-index 1 --has-header false --max-exploration-time 30 --name YelpDemo\r\n\r\nNow I can load that model into a console app and run it fine. Showing the ModelInput.cs generated by the mlnet cli tool. I can just create another input class like this and use it to load the model, create a prediction engine, and score a new input.\r\n\r\n![image](https://user-images.githubusercontent.com/6192880/103916547-2dff6a80-50d2-11eb-9cf2-fb32dc363bcb.png)\r\n\r\nHowever, if I upgrade the mlnet cli tool it doesn't work\r\nmlnet –version => 16.2.0\r\nCommand to train a model:\r\nmlnet classification --dataset data/yelp_labelled.txt --has-header false --label-col 1 --train-time 30 --name YelpML16\r\nThe tool appears to pring a warning about a header being detected in the dataset, even though no header is used in this dataset.\r\n![image](https://user-images.githubusercontent.com/6192880/103916757-6b63f800-50d2-11eb-85dd-fc93df9b2c6c.png)\r\n\r\nWhen I try and import the model and create a prediction engine I encounter the error I pasted above, saying: \r\nUnhandled exception. System.ArgumentOutOfRangeException: Could not find input column 'col1' (Parameter 'inputSchema')\r\n\r\nDiving into the mlnet v16.2 classes, it appears the ModelInput.cs has changed. The second column is no longer a boolean Label, but a string Col1\r\n![image](https://user-images.githubusercontent.com/6192880/103917197-ee854e00-50d2-11eb-8539-897211a81e78.png)\r\n\r\nThis is different than the ModelInput.cs generated by v0.15.1, but if I change the modelinput class to match the mlnet cli output I still get an exception loading the model and creating the prediction engine.\r\n\r\n```\r\nUnhandled exception. System.InvalidOperationException: Can't bind the IDataView column 'PredictedLabel' of type 'String' to field or property 'Prediction' of type 'System.Boolean'.\r\n   at Microsoft.ML.Data.TypedCursorable`1..ctor(IHostEnvironment env, IDataView data, Boolean ignoreMissingColumns, InternalSchemaDefinition schemaDefn)\r\n   at Microsoft.ML.Data.TypedCursorable`1.Create(IHostEnvironment env, IDataView data, Boolean ignoreMissingColumns, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2.PredictionEngineCore(IHostEnvironment env, InputRow`1 inputRow, IRowToRowMapper mapper, Boolean ignoreMissingColumns, SchemaDefinition outputSchemaDefinition, Action& disposer, IRowReadableAs`1& outputRow)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MLConsoleApp.Program.Main(String[] args) in /home/devel/code/mlnet_example/MLConsoleApp/Program.cs:line 37\r\n\r\n```\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5580","RelatedDescription":"Closed issue \"Mlnet CLI tool 16.2 generated model fails when being loaded to create a prediction engine\" (#5580)"},{"Id":"785443500","IsPullRequest":false,"CreatedAt":"2021-01-13T20:46:04","Actor":"michaelgsharp","Number":"5587","RawContent":null,"Title":"Migrate to VSTest for all Unit Tests","State":"open","Body":"Based on the conversation in #5583 and this [comment](https://github.com/dotnet/machinelearning/pull/5583#issuecomment-759711391), we need to migrate all our unit testing over to VSTest to make debugging easier. This issue is being created to track that so we don't lose it.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5587","RelatedDescription":"Open issue \"Migrate to VSTest for all Unit Tests\" (#5587)"},{"Id":"783735050","IsPullRequest":true,"CreatedAt":"2021-01-13T20:43:46","Actor":"michaelgsharp","Number":"5583","RawContent":null,"Title":"Added in note in the documentation that the PredictionEngine is not thread safe.","State":"closed","Body":"Since we have had several questions about PredictionEngine and whether its thread safe (#5582 for example), I have just added a small comment to the PredictionEngine class docs saying its not thread safe.","Url":"https://github.com/dotnet/machinelearning/pull/5583","RelatedDescription":"Closed or merged PR \"Added in note in the documentation that the PredictionEngine is not thread safe.\" (#5583)"},{"Id":"785337400","IsPullRequest":true,"CreatedAt":"2021-01-13T18:29:40","Actor":"michaelgsharp","Number":"5586","RawContent":null,"Title":"Fixed Averaged Perceptron default value","State":"open","Body":"Fixes #5568. Several months ago the default value of iterations for averaged perceptron was changed from 1 to 10. It was missed in 1 location, so this pr addresses that and fixes it there as well.","Url":"https://github.com/dotnet/machinelearning/pull/5586","RelatedDescription":"Open PR \"Fixed Averaged Perceptron default value\" (#5586)"},{"Id":"783818389","IsPullRequest":true,"CreatedAt":"2021-01-13T05:59:13","Actor":"michaelgsharp","Number":"5584","RawContent":null,"Title":"Nuget.config url fix for roslyn compilers","State":"closed","Body":"The old `dotnet.myget.org` for `myget-roslyn` was removed. This PR updates the url to be the correct one, and then updates the versions to be the versions the new Nuget store has. Fixes the couple of test failures due to version updates as well.","Url":"https://github.com/dotnet/machinelearning/pull/5584","RelatedDescription":"Closed or merged PR \"Nuget.config url fix for roslyn compilers\" (#5584)"},{"Id":"784699625","IsPullRequest":false,"CreatedAt":"2021-01-13T00:50:53","Actor":"s-tory","Number":"5585","RawContent":null,"Title":"Why `RecursionLimit = 10` in `OnnxTransformer`?","State":"open","Body":"What is the purpose that `Google.Protobuf.CodedInputStream.RecursionLimit` is set `10` at the following code?\r\n\r\nhttps://github.com/dotnet/machinelearning/blob/2a6cf9d9c9655f9e7d8ab7332efb5e2a2b70ce7e/src/Microsoft.ML.OnnxTransformer/OnnxUtils.cs#L208\r\n\r\n----\r\nIt is set `100` by default in protocol-buffers C#-wrapper.\r\n\r\nhttps://github.com/protocolbuffers/protobuf/blob/10599e6c8dde8a9875258e03054a696d53cadebd/csharp/src/Google.Protobuf/CodedInputStream.cs#L83\r\n```\r\n        internal const int DefaultRecursionLimit = 100;\r\n```\r\n\r\n----\r\nI could not load some network / `.onnx` file by the following exception be caused by `RecursionLimit = 10` without the monkey-patched `Microsoft.ML.OnnxTransformer.dll`.\r\n```\r\nGoogle.Protobuf.InvalidProtocolBufferException: Protocol message had too many levels of nesting.  May be malicious.  Use CodedInputStream.SetRecursionLimit() to increase the depth limit.\r\n```\r\n\r\nI think that the networks are including `Inception` construction especially.\r\n(ex. [tensorflow/models`s Faster-RCNN-Inception-V2](https://github.com/tensorflow/models/blob/master/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py) converted)\r\n\r\nThank you for coding great tools!","Url":"https://github.com/dotnet/machinelearning/issues/5585","RelatedDescription":"Open issue \"Why `RecursionLimit = 10` in `OnnxTransformer`?\" (#5585)"},{"Id":"783642971","IsPullRequest":false,"CreatedAt":"2021-01-11T21:41:58","Actor":"SpeedyCraftah","Number":"5582","RawContent":null,"Title":"AutoML - Cannot multi-thread predictions","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**: Windows 10 Pro\r\n- **.NET Version (eg., dotnet --info)**: .NET Core 3.1\r\n\r\n### Issue\r\n\r\n- **What did you do?** I compiled a model with the visual studio model builder then attempted to predict multiple labels with multi-threading.\r\n- **What happened?** An exception gets thrown: An exception of type 'System.IndexOutOfRangeException' occurred in Microsoft.ML.Transforms.dll but was not handled in user code - Index was outside the bounds of the array.\r\n- **What did you expect?** Not throw an error and run the predictions in parallel.\r\n\r\n### Source code / logs\r\n\r\n```cs\r\nusing Ml_net_sentimentML.Model;\r\nusing System;\r\nusing System.Threading.Tasks;\r\n\r\nnamespace myMLApp\r\n{\r\n    class Program\r\n    {\r\n        public static void Main()\r\n        {\r\n            MainAsync().GetAwaiter().GetResult();\r\n        }\r\n\r\n        async static Task MainAsync()\r\n        {\r\n            string[] toPredict = { \"This is a great!\", \"This is bad\", \"It doesn't work\" };\r\n            Task<string>[] resultTasks = new Task<string>[toPredict.Length];\r\n\r\n            for (int i = 0; i < toPredict.Length; i++)\r\n            {\r\n                string review = toPredict[i];\r\n\r\n                resultTasks[i] = Task.Run(() => PredictAndParse(review));\r\n            }\r\n\r\n            string[] results = await Task.WhenAll(resultTasks);\r\n\r\n            for (int i = 0; i < results.Length; i++)\r\n            {\r\n                Console.WriteLine(results[i] + \"\\n\");\r\n            }\r\n        }\r\n\r\n        static string PredictAndParse(string review)\r\n        {\r\n            ModelInput input = new ModelInput()\r\n            {\r\n                Review = review\r\n            };\r\n\r\n            ModelOutput prediction = ConsumeModel.Predict(input);\r\n\r\n            string sentiment = prediction.Prediction == \"1\" ? \"Positive\" : \"Negative\";\r\n\r\n            return $\"Text: {input.Review}\\nSentiment: {sentiment}\\nScore: {prediction.Score[0]} | {prediction.Score[1]}\";\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n**Full exception**:\r\n```\r\nSystem.IndexOutOfRangeException\r\n  HResult=0x80131508\r\n  Message=Index was outside the bounds of the array.\r\n  Source=Microsoft.ML.Transforms\r\n  StackTrace:\r\n   at Microsoft.ML.Transforms.Text.TokenizingByCharactersTransformer.Mapper.<>c__DisplayClass15_0.<MakeGetterOne>b__0(VBuffer`1& dst)\r\n   at Microsoft.ML.Data.RowCursorUtils.<>c__DisplayClass11_0`2.<GetVecGetterAsCore>b__0(VBuffer`1& dst)\r\n   at Microsoft.ML.Transforms.Text.NgramExtractingTransformer.Mapper.<>c__DisplayClass11_0.<MakeGetter>b__2(VBuffer`1& dst)\r\n   at Microsoft.ML.Transforms.LpNormNormalizingTransformer.Mapper.<>c__DisplayClass8_0.<MakeGetter>b__5(VBuffer`1& dst)\r\n   at Microsoft.ML.Data.ColumnConcatenatingTransformer.Mapper.BoundColumn.<>c__DisplayClass20_0`1.<MakeGetter>b__0(VBuffer`1& dst)\r\n   at Microsoft.ML.Transforms.NormalizeTransform.AffineColumnFunction.Sng.ImplVec.<>c__DisplayClass5_0.<GetGetter>b__0(VBuffer`1& dst)\r\n   at Microsoft.ML.Data.SchemaBindablePredictorWrapperBase.<>c__DisplayClass18_0`2.<GetValueGetter>b__0(TDst& dst)\r\n   at Microsoft.ML.Data.PredictedLabelScorerBase.EnsureCachedPosition[TScore](Int64& cachedPosition, TScore& score, DataViewRow boundRow, ValueGetter`1 scoreGetter)\r\n   at Microsoft.ML.Data.MulticlassClassificationScorer.<>c__DisplayClass16_0.<GetPredictedLabelGetter>b__0(UInt32& dst)\r\n   at Microsoft.ML.Transforms.KeyToValueMappingTransformer.Mapper.KeyToValueMap`2.<>c__DisplayClass8_0.<GetMappingGetter>b__0(TValue& dst)\r\n   at Microsoft.ML.Data.TypedCursorable`1.TypedRowBase.<>c__DisplayClass9_0`2.<CreateConvertingActionSetter>b__0(TRow row)\r\n   at Microsoft.ML.Data.TypedCursorable`1.TypedRowBase.FillValues(TRow row)\r\n   at Microsoft.ML.Data.TypedCursorable`1.RowImplementation.FillValues(TRow row)\r\n   at Microsoft.ML.PredictionEngineBase`2.FillValues(TDst prediction)\r\n   at Microsoft.ML.PredictionEngine`2.Predict(TSrc example, TDst& prediction)\r\n   at Microsoft.ML.PredictionEngineBase`2.Predict(TSrc example)\r\n   at Ml_net_sentimentML.Model.ConsumeModel.Predict(ModelInput input) in C:\\Users\\USER\\source\\repos\\ml.net sentiment\\ml.net sentimentML.Model\\ConsumeModel.cs:line 20\r\n   at myMLApp.Program.PredictAndParse(String review) in C:\\Users\\USER\\source\\repos\\ml.net sentiment\\ml.net sentiment\\Program.cs:line 48\r\n   at myMLApp.Program.<>c__DisplayClass1_0.<MainAsync>b__0() in C:\\Users\\USER\\source\\repos\\ml.net sentiment\\ml.net sentiment\\Program.cs:line 26\r\n   at System.Threading.Tasks.Task`1.InnerInvoke()\r\n   at System.Threading.Tasks.Task.<>c.<.cctor>b__274_0(Object obj)\r\n   at System.Threading.ExecutionContext.RunFromThreadPoolDispatchLoop(Thread threadPoolThread, ExecutionContext executionContext, ContextCallback callback, Object state)```\r\n\r\n```\r\n\r\nI'm quite new to C# so I wouldn't be surprised if this was an issue on my end.\r\n\r\n### Note\r\n\r\nWhen not attempting to multi-thread while predicting, it works fine (iterating over the `toPredict` array and calling `ConsumeModel.Predict` on all of them separately), no exceptions get thrown other than the fact that it's slow since it runs inconcurrently.","Url":"https://github.com/dotnet/machinelearning/issues/5582","RelatedDescription":"Closed issue \"AutoML - Cannot multi-thread predictions\" (#5582)"},{"Id":"782579558","IsPullRequest":false,"CreatedAt":"2021-01-09T10:32:53","Actor":"CJX-nice","Number":"5581","RawContent":null,"Title":".net for  sqark and  ML","State":"open","Body":"### System information\r\nwindows 10\r\n.net core  3.1\r\nspark 3.0.1\r\n.net for  spark  1.0.0\r\n\r\n### Issue\r\n Learn    <  Sentiment analysis with .NET for Apache Spark and ML.NET>    in  https://docs.microsoft.com/zh-cn/dotnet/spark/tutorials/ml-sentiment-analysis\r\n\r\nI   create  my  project   and   copy     the  official code   ;\r\nbut   An error occurred\r\n **[Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.**\r\n\r\n\r\n### Source code / logs\r\n\r\n// This file was auto-generated by ML.NET Model Builder. \r\n**this is  my Code  :**\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing Microsoft.ML;\r\nusing Microsoft.ML.Data;\r\nusing Microsoft.Spark.Sql;\r\nusing MySparkAppML.Model;\r\n\r\n\r\nnamespace MySparkAppML.ConsoleApp\r\n{\r\n    public class Program\r\n    {\r\n        public static void Main(string[] args)\r\n        {\r\n            SparkSession spark = SparkSession\r\n             .Builder()\r\n             .AppName(\".NET for Apache Spark Sentiment Analysis\")\r\n             .GetOrCreate();\r\n            DataFrame df = spark .Read() .Option(\"header\", true).Option(\"inferSchema\", true) .Csv(\"yelptest.csv\");\r\n            df.Show();\r\n            Console.WriteLine(predict(\"aaa\"));\r\n            Console.WriteLine(predict(\"bbb\"));\r\n            spark.Udf() .Register<string, float>(\"MLudf\", predict);\r\n            df.CreateOrReplaceTempView(\"Reviews\");\r\n            DataFrame sqlDf = spark.Sql(\"SELECT ReviewText, MLudf(ReviewText) FROM Reviews\");\r\n            sqlDf.Show();\r\n            Console.ReadLine();\r\n        }\r\n        static float predict(string text)\r\n        {\r\n            MLContext mlContext = new MLContext();\r\n            ITransformer model = mlContext.Model.Load(\"MLModel.zip\", out var schema);\r\n            var Engine = mlContext.Model.CreatePredictionEngine<ModelInput, ModelOutput>(model);\r\n            return Engine.Predict(new ModelInput() { ReviewText = text }).Score;\r\n        }\r\n    }\r\n}\r\n\r\n\r\n\r\n**my  log** \r\n\r\n\r\nC:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\bin\\Debug\\netcoreapp3.1\\publish>spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local microsoft-spark-3-0_2.12-1.0.0.jar dotnet MySparkAppML.ConsoleApp.dll\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/bin/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\r\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\n21/01/09 18:29:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\n21/01/09 18:29:55 INFO DotnetRunner: Starting DotnetBackend with dotnet.\r\n21/01/09 18:29:55 INFO DotnetBackend: The number of DotnetBackend threads is set to 10.\r\n21/01/09 18:29:57 INFO DotnetRunner: Port number used by DotnetBackend is 53683\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.jars and value=file:/C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/microsoft-spark-3-0_2.12-1.0.0.jar to environment\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.app.name and value=org.apache.spark.deploy.dotnet.DotnetRunner to environment\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.submit.pyFiles and value= to environment\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.submit.deployMode and value=client to environment\r\n21/01/09 18:29:57 INFO DotnetRunner: Adding key=spark.master and value=local to environment\r\n[2021-01-09T10:29:57.7611666Z] [LAPTOP-8R49BD47] [Info] [ConfigurationService] Using port 53683 for connection.\r\n[2021-01-09T10:29:57.7701007Z] [LAPTOP-8R49BD47] [Info] [JvmBridge] JvMBridge port is 53683\r\n[2021-01-09T10:29:57.7750557Z] [LAPTOP-8R49BD47] [Info] [JvmBridge] The number of JVM backend thread is set to 10. The max number of concurrent sockets in JvmBridge is set to 7.\r\n21/01/09 18:29:58 INFO SparkContext: Running Spark version 3.0.1\r\n21/01/09 18:29:58 INFO ResourceUtils: ==============================================================\r\n21/01/09 18:29:58 INFO ResourceUtils: Resources for spark.driver:\r\n\r\n21/01/09 18:29:58 INFO ResourceUtils: ==============================================================\r\n21/01/09 18:29:58 INFO SparkContext: Submitted application: .NET for Apache Spark Sentiment Analysis\r\n21/01/09 18:29:58 INFO SecurityManager: Changing view acls to: YD\r\n21/01/09 18:29:58 INFO SecurityManager: Changing modify acls to: YD\r\n21/01/09 18:29:58 INFO SecurityManager: Changing view acls groups to:\r\n21/01/09 18:29:58 INFO SecurityManager: Changing modify acls groups to:\r\n21/01/09 18:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YD); groups with view permissions: Set(); users  with modify permissions: Set(YD); groups with modify permissions: Set()\r\n21/01/09 18:29:58 INFO Utils: Successfully started service 'sparkDriver' on port 53691.\r\n21/01/09 18:29:58 INFO SparkEnv: Registering MapOutputTracker\r\n21/01/09 18:29:58 INFO SparkEnv: Registering BlockManagerMaster\r\n21/01/09 18:29:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n21/01/09 18:29:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n21/01/09 18:29:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\r\n21/01/09 18:29:58 INFO DiskBlockManager: Created local directory at C:\\Users\\YD\\AppData\\Local\\Temp\\blockmgr-f20af9bd-7dc2-4ff4-9595-1d93a63b47b4\r\n21/01/09 18:29:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\r\n21/01/09 18:29:58 INFO SparkEnv: Registering OutputCommitCoordinator\r\n21/01/09 18:29:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\r\n21/01/09 18:29:58 INFO Utils: Successfully started service 'SparkUI' on port 4041.\r\n21/01/09 18:29:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://LAPTOP-8R49BD47:4041\r\n21/01/09 18:29:59 INFO SparkContext: Added JAR file:/C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/microsoft-spark-3-0_2.12-1.0.0.jar at spark://LAPTOP-8R49BD47:53691/jars/microsoft-spark-3-0_2.12-1.0.0.jar with timestamp 1610188199046\r\n21/01/09 18:29:59 INFO Executor: Starting executor ID driver on host LAPTOP-8R49BD47\r\n21/01/09 18:29:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53706.\r\n21/01/09 18:29:59 INFO NettyBlockTransferService: Server created on LAPTOP-8R49BD47:53706\r\n21/01/09 18:29:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n21/01/09 18:29:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-8R49BD47, 53706, None)\r\n21/01/09 18:29:59 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-8R49BD47:53706 with 434.4 MiB RAM, BlockManagerId(driver, LAPTOP-8R49BD47, 53706, None)\r\n21/01/09 18:29:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-8R49BD47, 53706, None)\r\n21/01/09 18:29:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-8R49BD47, 53706, None)\r\n21/01/09 18:29:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/spark-warehouse').\r\n21/01/09 18:30:00 INFO SharedState: Warehouse path is 'file:/C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/spark-warehouse'.\r\n21/01/09 18:30:00 INFO InMemoryFileIndex: It took 53 ms to list leaf files for 1 paths.\r\n21/01/09 18:30:01 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\r\n21/01/09 18:30:03 INFO FileSourceStrategy: Pruning directories with:\r\n21/01/09 18:30:03 INFO FileSourceStrategy: Pushed Filters:\r\n21/01/09 18:30:03 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\r\n21/01/09 18:30:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\r\n21/01/09 18:30:04 INFO CodeGenerator: Code generated in 307.516 ms\r\n21/01/09 18:30:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 171.3 KiB, free 434.2 MiB)\r\n21/01/09 18:30:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.2 MiB)\r\n21/01/09 18:30:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:04 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4225524 bytes, open cost is considered as scanning 4194304 bytes.\r\n21/01/09 18:30:05 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:05 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\r\n21/01/09 18:30:05 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\r\n21/01/09 18:30:05 INFO DAGScheduler: Parents of final stage: List()\r\n21/01/09 18:30:05 INFO DAGScheduler: Missing parents: List()\r\n21/01/09 18:30:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n21/01/09 18:30:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 434.2 MiB)\r\n21/01/09 18:30:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 434.2 MiB)\r\n21/01/09 18:30:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 5.3 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223\r\n21/01/09 18:30:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\r\n21/01/09 18:30:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\r\n21/01/09 18:30:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, LAPTOP-8R49BD47, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)\r\n21/01/09 18:30:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\r\n21/01/09 18:30:05 INFO Executor: Fetching spark://LAPTOP-8R49BD47:53691/jars/microsoft-spark-3-0_2.12-1.0.0.jar with timestamp 1610188199046\r\n21/01/09 18:30:05 INFO TransportClientFactory: Successfully created connection to LAPTOP-8R49BD47/192.168.11.81:53691 after 24 ms (0 ms spent in bootstraps)\r\n21/01/09 18:30:05 INFO Utils: Fetching spark://LAPTOP-8R49BD47:53691/jars/microsoft-spark-3-0_2.12-1.0.0.jar to C:\\Users\\YD\\AppData\\Local\\Temp\\spark-977ec410-19ef-4910-8e46-6e58ab7def14\\userFiles-7db99229-141a-40b4-ad4c-108082a93a68\\fetchFileTemp1545562991836894884.tmp\r\n21/01/09 18:30:05 INFO Executor: Adding file:/C:/Users/YD/AppData/Local/Temp/spark-977ec410-19ef-4910-8e46-6e58ab7def14/userFiles-7db99229-141a-40b4-ad4c-108082a93a68/microsoft-spark-3-0_2.12-1.0.0.jar to class loader\r\n21/01/09 18:30:05 INFO FileScanRDD: Reading File path: file:///C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/yelptest.csv, range: 0-31220, partition values: [empty row]\r\n21/01/09 18:30:05 INFO CodeGenerator: Code generated in 16.4157 ms\r\n21/01/09 18:30:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1583 bytes result sent to driver\r\n21/01/09 18:30:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 549 ms on LAPTOP-8R49BD47 (executor driver) (1/1)\r\n21/01/09 18:30:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool\r\n21/01/09 18:30:05 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.730 s\r\n21/01/09 18:30:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\r\n21/01/09 18:30:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\r\n21/01/09 18:30:05 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.786605 s\r\n21/01/09 18:30:05 INFO CodeGenerator: Code generated in 15.1734 ms\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 5.3 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Pruning directories with:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Pushed Filters:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Post-Scan Filters:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 171.3 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4225524 bytes, open cost is considered as scanning 4194304 bytes.\r\n21/01/09 18:30:06 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:06 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\r\n21/01/09 18:30:06 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\r\n21/01/09 18:30:06 INFO DAGScheduler: Parents of final stage: List()\r\n21/01/09 18:30:06 INFO DAGScheduler: Missing parents: List()\r\n21/01/09 18:30:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 7.7 KiB, free: 434.3 MiB)\r\n21/01/09 18:30:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223\r\n21/01/09 18:30:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\r\n21/01/09 18:30:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, LAPTOP-8R49BD47, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)\r\n21/01/09 18:30:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO FileScanRDD: Reading File path: file:///C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/yelptest.csv, range: 0-31220, partition values: [empty row]\r\n21/01/09 18:30:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1595 bytes result sent to driver\r\n21/01/09 18:30:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 158 ms on LAPTOP-8R49BD47 (executor driver) (1/1)\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool\r\n21/01/09 18:30:06 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.227 s\r\n21/01/09 18:30:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\r\n21/01/09 18:30:06 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0.244445 s\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Pruning directories with:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Pushed Filters:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Post-Scan Filters:\r\n21/01/09 18:30:06 INFO FileSourceStrategy: Output Data Schema: struct<ReviewText: string, Sentiment: string>\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 171.3 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Removed broadcast_3_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 7.7 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:06 INFO SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4225524 bytes, open cost is considered as scanning 4194304 bytes.\r\n21/01/09 18:30:06 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:06 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\r\n21/01/09 18:30:06 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\r\n21/01/09 18:30:06 INFO DAGScheduler: Parents of final stage: List()\r\n21/01/09 18:30:06 INFO DAGScheduler: Missing parents: List()\r\n21/01/09 18:30:06 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.8 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 434.0 MiB)\r\n21/01/09 18:30:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 4.9 KiB, free: 434.3 MiB)\r\n21/01/09 18:30:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223\r\n21/01/09 18:30:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\r\n21/01/09 18:30:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, LAPTOP-8R49BD47, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)\r\n21/01/09 18:30:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\r\n21/01/09 18:30:06 INFO FileScanRDD: Reading File path: file:///C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/yelptest.csv, range: 0-31220, partition values: [empty row]\r\n21/01/09 18:30:06 INFO CodeGenerator: Code generated in 22.741099 ms\r\n21/01/09 18:30:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2563 bytes result sent to driver\r\n21/01/09 18:30:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 107 ms on LAPTOP-8R49BD47 (executor driver) (1/1)\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool\r\n21/01/09 18:30:06 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.142 s\r\n21/01/09 18:30:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\r\n21/01/09 18:30:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\r\n21/01/09 18:30:06 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.162733 s\r\n21/01/09 18:30:06 INFO CodeGenerator: Code generated in 26.014001 ms\r\n+--------------------+---------+\r\n|          ReviewText|Sentiment|\r\n+--------------------+---------+\r\n|Waitress was swee...|        1|\r\n|I also had to tas...|        1|\r\n|I'd rather eat ai...|        0|\r\n|Cant say enough g...|        1|\r\n|The ambiance was ...|        1|\r\n|The waitress and ...|        1|\r\n|I would not recom...|        0|\r\n|Overall I wasn't ...|        0|\r\n|My gyro was basic...|        0|\r\n|   Terrible service!|        0|\r\n|Thoroughly disapp...|        0|\r\n|I don't each much...|        1|\r\n|Give it a try, yo...|        1|\r\n|By far the BEST c...|        1|\r\n|Reasonably priced...|        1|\r\n|Everything was pe...|        1|\r\n|The food is very ...|        1|\r\n|it was a drive to...|        0|\r\n|At first glance i...|        1|\r\n|Anyway, I do not ...|        0|\r\n+--------------------+---------+\r\nonly showing top 20 rows\r\n\r\n0.61130136\r\n0.61130136\r\n[2021-01-09T10:30:08.1248483Z] [LAPTOP-8R49BD47] [Debug] [ConfigurationService] Using the environment variable to construct .NET worker path: C:\\bin\\Microsoft.Spark.Worker-1.0.0\\Microsoft.Spark.Worker.exe.\r\n21/01/09 18:30:08 INFO BlockManagerInfo: Removed broadcast_5_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 4.9 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:08 INFO FileSourceStrategy: Pruning directories with:\r\n21/01/09 18:30:08 INFO FileSourceStrategy: Pushed Filters:\r\n21/01/09 18:30:08 INFO FileSourceStrategy: Post-Scan Filters:\r\n21/01/09 18:30:08 INFO FileSourceStrategy: Output Data Schema: struct<ReviewText: string>\r\n21/01/09 18:30:08 INFO CodeGenerator: Code generated in 31.628 ms\r\n21/01/09 18:30:08 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 171.3 KiB, free 434.0 MiB)\r\n21/01/09 18:30:08 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.0 MiB)\r\n21/01/09 18:30:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:08 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4225524 bytes, open cost is considered as scanning 4194304 bytes.\r\n21/01/09 18:30:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on LAPTOP-8R49BD47:53706 in memory (size: 24.1 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:08 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\r\n21/01/09 18:30:09 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\r\n21/01/09 18:30:09 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\r\n21/01/09 18:30:09 INFO DAGScheduler: Parents of final stage: List()\r\n21/01/09 18:30:09 INFO DAGScheduler: Missing parents: List()\r\n21/01/09 18:30:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n21/01/09 18:30:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 17.1 KiB, free 434.2 MiB)\r\n21/01/09 18:30:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 434.2 MiB)\r\n21/01/09 18:30:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on LAPTOP-8R49BD47:53706 (size: 8.6 KiB, free: 434.4 MiB)\r\n21/01/09 18:30:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223\r\n21/01/09 18:30:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\r\n21/01/09 18:30:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks\r\n21/01/09 18:30:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)\r\n21/01/09 18:30:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\r\n21/01/09 18:30:09 INFO CodeGenerator: Code generated in 23.0987 ms\r\nDotnetWorker PID:[22312] Args:[-m pyspark.worker] SparkVersion:[3.0.1]\r\n[2021-01-09T10:30:09.6927376Z] [LAPTOP-8R49BD47] [Info] [SimpleWorker] RunSimpleWorker() is starting with port = 53719.\r\n[2021-01-09T10:30:09.7797201Z] [LAPTOP-8R49BD47] [Info] [TaskRunner] [0] Starting with ReuseSocket[False].\r\n21/01/09 18:30:09 INFO FileScanRDD: Reading File path: file:///C:/Users/YD/source/repos/MySparkAppML.ConsoleApp/bin/Debug/netcoreapp3.1/publish/yelptest.csv, range: 0-31220, partition values: [empty row]\r\n[2021-01-09T10:30:09.7980951Z] [LAPTOP-8R49BD47] [Info] [ConfigurationService] 'DOTNETBACKEND_PORT' environment variable is not set.\r\n[2021-01-09T10:30:09.7981653Z] [LAPTOP-8R49BD47] [Info] [ConfigurationService] Using port 5567 for connection.\r\n[2021-01-09T10:30:09.8053556Z] [LAPTOP-8R49BD47] [Info] [JvmBridge] JvMBridge port is 5567\r\n21/01/09 18:30:09 INFO CodeGenerator: Code generated in 19.9682 ms\r\n[2021-01-09T10:30:09.8144165Z] [LAPTOP-8R49BD47] [Info] [JvmBridge] The number of JVM backend thread is set to 10. The max number of concurrent sockets in JvmBridge is set to 7.\r\n[2021-01-09T10:30:10.6518123Z] [LAPTOP-8R49BD47] [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n[2021-01-09T10:30:10.6527954Z] [LAPTOP-8R49BD47] [Error] [TaskRunner] [0] Exiting with exception: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs21/01/09 18:30:10 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\r\norg.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n:line 154\r\n   at Microsoft.Spark.Worker.TaskRunner.Run() in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 66\r\n[2021-01-09T10:30:10.6537382Z] [LAPTOP-8R49BD47] [Info] [TaskRunner] [0] Finished running 0 task(s).\r\n[2021-01-09T10:30:10.6537724Z] [LAPTOP-8R49BD47] [Info] [SimpleWorker] RunSimpleWorker() finished successfully\r\n21/01/09 18:30:10 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver): org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\n21/01/09 18:30:10 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\r\n21/01/09 18:30:10 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool\r\n21/01/09 18:30:10 INFO TaskSchedulerImpl: Cancelling stage 3\r\n21/01/09 18:30:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled\r\n21/01/09 18:30:10 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) failed in 1.700 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver): org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\nDriver stacktrace:\r\n21/01/09 18:30:10 INFO DAGScheduler: Job 3 failed: showString at NativeMethodAccessorImpl.java:0, took 1.716029 s\r\n21/01/09 18:30:10 ERROR DotnetBackendHandler: Failed to execute 'showString' on 'org.apache.spark.sql.Dataset' with args=([Type=java.lang.Integer, Value: 20], [Type=java.lang.Integer, Value: 20], [Type=java.lang.Boolean, Value: false])\r\n[2021-01-09T10:30:10.7275550Z] [LAPTOP-8R49BD47] [Error] [JvmBridge] JVM method execution failed: Nonstatic method 'showString' failed for class '18' when called with 3 arguments ([Index=1, Type=Int32, Value=20], [Index=2, Type=Int32, Value=20], [Index=3, Type=Boolean, Value=False], )\r\n[2021-01-09T10:30:10.7276810Z] [LAPTOP-8R49BD47] [Error] [JvmBridge] org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver): org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\nDriver stacktrace:\r\n        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n        at scala.Option.foreach(Option.scala:407)\r\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n        at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n        at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n        at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n        at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n        at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n        at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n        at org.apache.spark.api.dotnet.DotnetBackendHandler.handleMethodCall(DotnetBackendHandler.scala:159)\r\n        at org.apache.spark.api.dotnet.DotnetBackendHandler.$anonfun$handleBackendRequest$2(DotnetBackendHandler.scala:99)\r\n        at org.apache.spark.api.dotnet.ThreadPool$$anon$1.run(ThreadPool.scala:34)\r\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r\n        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\nCaused by: org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        ... 3 more\r\n\r\n[2021-01-09T10:30:10.7801524Z] [LAPTOP-8R49BD47] [Exception] [JvmBridge] JVM method execution failed: Nonstatic method 'showString' failed for class '18' when called with 3 arguments ([Index=1, Type=Int32, Value=20], [Index=2, Type=Int32, Value=20], [Index=3, Type=Boolean, Value=False], )\r\n   at Microsoft.Spark.Interop.Ipc.JvmBridge.CallJavaMethod(Boolean isStatic, Object classNameOrJvmObjectReference, String methodName, Object[] args)\r\nUnhandled exception. System.Exception: JVM method execution failed: Nonstatic method 'showString' failed for class '18' when called with 3 arguments ([Index=1, Type=Int32, Value=20], [Index=2, Type=Int32, Value=20], [Index=3, Type=Boolean, Value=False], )\r\n ---> Microsoft.Spark.JvmException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, LAPTOP-8R49BD47, executor driver): org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\nDriver stacktrace:\r\n        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n        at scala.Option.foreach(Option.scala:407)\r\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n        at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n        at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n        at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n        at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n        at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n        at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n        at org.apache.spark.api.dotnet.DotnetBackendHandler.handleMethodCall(DotnetBackendHandler.scala:159)\r\n        at org.apache.spark.api.dotnet.DotnetBackendHandler.$anonfun$handleBackendRequest$2(DotnetBackendHandler.scala:99)\r\n        at org.apache.spark.api.dotnet.ThreadPool$$anon$1.run(ThreadPool.scala:34)\r\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r\n        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:835)\r\nCaused by: org.apache.spark.api.python.PythonException: System.TypeLoadException: Could not load type 'Microsoft.ML.Data.DataViewTypeAttribute' from assembly 'Microsoft.ML.DataView, Version=1.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.\r\n   at Microsoft.ML.Data.SchemaDefinition.GetNameAndCustomAttributes(MemberInfo memberInfo, Type userType, HashSet`1 colNames, String& name, IEnumerable`1& customAttributes)\r\n   at Microsoft.ML.Data.SchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.InternalSchemaDefinition.Create(Type userType, Direction direction)\r\n   at Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment env, SchemaDefinition schemaDefinition)\r\n   at Microsoft.ML.PredictionEngineBase`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngine`2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, IHostEnvironment env, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)\r\n   at MySparkAppML.ConsoleApp.Program.predict(String text) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 35\r\n   at Microsoft.Spark.Sql.PicklingUdfWrapper`2.Execute(Int32 splitIndex, Object[] input, Int32[] argOffsets) in /_/src/csharp/Microsoft.Spark/Sql/PicklingUdfWrapper.cs:line 51\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.SingleCommandRunner.Run(Int32 splitId, Object input) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 252\r\n   at Microsoft.Spark.Worker.Command.PicklingSqlCommandExecutor.ExecuteCore(Stream inputStream, Stream outputStream, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 152\r\n   at Microsoft.Spark.Worker.Command.SqlCommandExecutor.Execute(Version version, Stream inputStream, Stream outputStream, PythonEvalType evalType, SqlCommand[] commands) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\SqlCommandExecutor.cs:line 76\r\n   at Microsoft.Spark.Worker.Command.CommandExecutor.Execute(Stream inputStream, Stream outputStream, Int32 splitIndex, CommandPayload commandPayload) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\Command\\CommandExecutor.cs:line 65\r\n   at Microsoft.Spark.Worker.TaskRunner.ProcessStream(Stream inputStream, Stream outputStream, Version version, Boolean& readComplete) in D:\\a\\1\\s\\src\\csharp\\Microsoft.Spark.Worker\\TaskRunner.cs:line 154\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n        at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n        ... 3 more\r\n\r\n   --- End of inner exception stack trace ---\r\n   at Microsoft.Spark.Interop.Ipc.JvmBridge.CallJavaMethod(Boolean isStatic, Object classNameOrJvmObjectReference, String methodName, Object[] args)\r\n   at Microsoft.Spark.Interop.Ipc.JvmBridge.CallNonStaticJavaMethod(JvmObjectReference objectId, String methodName, Object[] args)\r\n   at Microsoft.Spark.Interop.Ipc.JvmObjectReference.Invoke(String methodName, Object[] args)\r\n   at Microsoft.Spark.Sql.DataFrame.Show(Int32 numRows, Int32 truncate, Boolean vertical)\r\n   at MySparkAppML.ConsoleApp.Program.Main(String[] args) in C:\\Users\\YD\\source\\repos\\MySparkAppML.ConsoleApp\\Program.cs:line 28\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5581","RelatedDescription":"Open issue \".net for  sqark and  ML\" (#5581)"},{"Id":"779911512","IsPullRequest":false,"CreatedAt":"2021-01-08T01:37:34","Actor":"nnoradie","Number":"5578","RawContent":null,"Title":"Boundaries and anomalies not correct after implementing 3-sigma approach for < 12 data points","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**: \r\n- **.NET Version (eg., dotnet --info)**: \r\n![image](https://user-images.githubusercontent.com/69877427/103716200-4d35b500-4f77-11eb-9b10-41c9396b4dc1.png)\r\n\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\n    - Ran 3-sigma approach then passed the padded dataset to SR\r\n        1. Calculated a range of 3 standard deviations above and below the mean of the data\r\n        2. Removed any outliers from the sum and recalculated the sum and a new mean without the outliers (in this case, no points were detected as outliers)\r\n        3. Evenly added enough points to fill out the batch to at least 12 points (ex. if the original data had 8 points, we added 2 points of the new mean value to the beginning and another 2 points to the end of the data)\r\n        4. Passed that data (in zip below) to DetectEntireAnomalyBySrCnn\r\n\r\n- **What happened?**\r\n    - Even though we found no outliers, the data point at the end was detected as an anomaly \r\n    - The confidence band was very off, many points were outside of the band, but none were detected as anomalies\r\n    \r\n![image](https://user-images.githubusercontent.com/69877427/103722432-ca1b5b80-4f84-11eb-8a98-3396d36eee41.png)\r\n(One point in the csv is not present here (11/1/1997) it was filtered out in post-processing since it was an interpolated data point)\r\n\r\n- **What did you expect?**\r\n    - No anomalies to be detected in this case\r\n\r\n### Source code / logs\r\n**3sigma code changes**\r\nhttps://powerbi.visualstudio.com/AI/_git/AI/pullrequest/131502?_a=files&path=%2Fsrc%2Fextensions%2FMicrosoft.AI.Dax.Extensions%2FAnomalies%2FDetection%2FAnomalyDetector.cs\r\n\r\n**AD options**\r\n![image](https://user-images.githubusercontent.com/69877427/103720633-a5bd8000-4f80-11eb-928f-9d1a23ca1167.png)\r\n\r\n**Data**\r\n[3sigma_data.zip](https://github.com/dotnet/machinelearning/files/5773621/3sigma_data.zip)\r\n\r\nPlease paste or attach the code or logs or traces that would be helpful to diagnose the issue you are reporting.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5578","RelatedDescription":"Closed issue \"Boundaries and anomalies not correct after implementing 3-sigma approach for < 12 data points\" (#5578)"},{"Id":"781190132","IsPullRequest":true,"CreatedAt":"2021-01-07T10:03:58","Actor":"guinao","Number":"5579","RawContent":null,"Title":"Fix issue in SRCnnEntireAnomalyDetector","State":"open","Body":"We are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [ ] There's a descriptive title that will make sense to other developers some time from now. \r\n- [ ] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [ ] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [ ] You have included any necessary tests in the same PR.\r\n\r\n#5516 ","Url":"https://github.com/dotnet/machinelearning/pull/5579","RelatedDescription":"Open PR \"Fix issue in SRCnnEntireAnomalyDetector\" (#5579)"},{"Id":"779748188","IsPullRequest":true,"CreatedAt":"2021-01-05T23:53:48","Actor":"Lynx1820","Number":"5577","RawContent":null,"Title":"Onnx Export for ValueMapping estimator","State":"open","Body":"LabelEncoder, the onnx operator used for this export, doesn't support mapping between the same types, which is allowed in ML.NET. This can be bypassed by casting to other types, except for String to String mappings. \r\n\r\n   \r\n","Url":"https://github.com/dotnet/machinelearning/pull/5577","RelatedDescription":"Open PR \"Onnx Export for ValueMapping estimator\" (#5577)"},{"Id":"771259310","IsPullRequest":false,"CreatedAt":"2021-01-05T03:52:35","Actor":"Jrjuniorjr","Number":"5562","RawContent":null,"Title":"Question about TFIDF","State":"closed","Body":"Hi to all, I'm new on ML.NET and I'm trying to FeaturizeText with TF-IDF.\r\nIn the Jupyter Notebook and in .NET Interactive, this code gets an error:\r\n\r\n```C#\r\nusing Microsoft.ML;\r\nusing Microsoft.ML.Data;\r\nusing System;\r\nusing Microsoft.ML.Transforms.Text;\r\nusing System.Collections.Generic;\r\n\r\nnamespace SentimentAnalysis\r\n{\r\n    public class Input\r\n    {\r\n        [LoadColumn(0)]\r\n        public string Text { get; set; }\r\n        [LoadColumn(1)]\r\n        public int Rating { get; set; }\r\n    }\r\n    public class Output\r\n    {\r\n        public float[] Features { get; set; }\r\n    }\r\n    class Program\r\n    {\r\n        public static TextFeaturizingEstimator.Options GetOptions()\r\n        {\r\n            var vectorizedTextOptions = new TextFeaturizingEstimator.Options()\r\n            {\r\n                KeepDiacritics = false,\r\n                KeepPunctuations = false,\r\n                KeepNumbers = true,\r\n                CaseMode = TextNormalizingEstimator.CaseMode.Lower,\r\n                StopWordsRemoverOptions = new StopWordsRemovingEstimator.Options()\r\n                {\r\n                    Language = TextFeaturizingEstimator.Language.English\r\n                },\r\n\r\n                // ngram options\r\n                WordFeatureExtractor = new WordBagEstimator.Options()\r\n                {\r\n                    NgramLength = 1,\r\n                    UseAllLengths = false, // Produce both unigrams and bigrams\r\n                    Weighting = NgramExtractingEstimator.WeightingCriteria.TfIdf, // TF-IDF\r\n                },\r\n\r\n                // chargram options\r\n                CharFeatureExtractor = null\r\n            };\r\n            return vectorizedTextOptions;\r\n\r\n        }\r\n        static void Main(string[] args)\r\n        {\r\n            var context = new MLContext();\r\n            var list = new List<Input>()\r\n            {\r\n                new Input()\r\n                {\r\n                    Text = \"This is machine learning example\", Rating = 4\r\n                },\r\n                new Input()\r\n                {\r\n                    Text = \"I like .NET\", Rating = 5\r\n                }\r\n            };\r\n            var samples = context.Data.LoadFromEnumerable<Input>(list);\r\n            var options = GetOptions();\r\n            var transformFitted = context.Transforms.Text.FeaturizeText(\r\n                    \"Features\",\r\n                    options,\r\n                    \"Text\"\r\n                ).Fit(samples);\r\n\r\n            var tfIdfTransformed = transformFitted.Transform(samples);\r\n\r\n            var predictionEngine = context.Model.CreatePredictionEngine<Input, Output>(transformFitted);\r\n            Output prediction = null;\r\n\r\n            VBuffer<ReadOnlyMemory<char>> slotNames = default;\r\n            tfIdfTransformed.Schema[\"Features\"].GetSlotNames(ref slotNames);\r\n\r\n            var tfIdfColumn = tfIdfTransformed.GetColumn<VBuffer<float>>(tfIdfTransformed.Schema[\"Features\"]);\r\n            var slots = slotNames.GetValues();\r\n\r\n            Console.Write(\"NGrams: \");\r\n            foreach (var featureRow in tfIdfColumn)\r\n            {\r\n                foreach (var item in featureRow.Items())\r\n                {\r\n                    Console.Write($\"{slots[item.Key]}  \");\r\n                }\r\n                Console.WriteLine();\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nAnd I do var slot = slotNames.GetValues() what I'm getting is:\r\n\r\nError: (3.1): error CS8345: The field or a self-implemented property cannot be of type 'ReadOnlySpan <ReadOnlyMemory <char>>', unless it is an instance member of a reference struct.\r\n\r\nAnd in Visual Studio 2019, it works well, but when I try to loop over the columns:\r\n\r\n```C#\r\n       Console.Write(\"NGrams: \");\r\n        foreach (var featureRow in tfIdfColumn)\r\n        {\r\n            foreach (var item in featureRow.Items())\r\n            {\r\n                Console.Write($\"{slots[item.Key]}  \");\r\n            }\r\n            Console.WriteLine();\r\n        }\r\n```\r\n\r\nIt repeats some words.\r\n\r\nFor example: If I get two input: \"This is a test\", \"I like dotnet\", what I got print is:\r\nThis is a test\r\ntest I like dotnet\r\n\r\nIt is just an example.\r\n\r\nAnother question is how can I get do something like this in ML.NET:\r\n\r\nThis code is in python:\r\n\r\n```python\r\ndocs = []\r\nfor index, row in df_balenceado.iterrows():\r\n    docs.append(row[\"CleanReview-NoStemming\"])\r\n\r\ntfidf_vectorizer=TfidfVectorizer()\r\nfitted_vectorizer=tfidf_vectorizer.fit(docs)\r\ntfidf_vectorizer_vectors=fitted_vectorizer.transform(docs)\r\nfeature_names = tfidf_vectorizer.get_feature_names()\r\ndictonary = pd.DataFrame(tfidf_vectorizer_vectors.todense().tolist(), columns=feature_names)\r\ndictonary.head()\r\npickle.dump(fitted_vectorizer, open(\"tfidf_nostemming.pkl\", \"wb\"))\r\ndictonary = variance_threshold_selector(dictonary, 0.00005)\r\ndictonary.to_csv(\"DictionaryNoStemming.csv\", index=False)\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/5562","RelatedDescription":"Closed issue \"Question about TFIDF\" (#5562)"},{"Id":"778292188","IsPullRequest":false,"CreatedAt":"2021-01-04T18:55:52","Actor":"eerhardt","Number":"5576","RawContent":null,"Title":"AutoML should skip/ignore trainers that don't work on the current machine","State":"open","Body":"Following up from https://github.com/dotnet/machinelearning/issues/3903#issuecomment-739542538.\r\n\r\nWe should consider not failing an AutoML experiment because the dependencies necessary for MKL to load are not available on the current machine. We could log a warning or tell the user some other way that MKL can't be loaded. But this shouldn't block the user, and force them to figure out how to exclude the problematic trainer (Ols in the case above).","Url":"https://github.com/dotnet/machinelearning/issues/5576","RelatedDescription":"Open issue \"AutoML should skip/ignore trainers that don't work on the current machine\" (#5576)"},{"Id":"776063367","IsPullRequest":false,"CreatedAt":"2021-01-04T11:42:48","Actor":"WEMAPP","Number":"5570","RawContent":null,"Title":"Deep learning and text recognition ","State":"closed","Body":"Hi everyone.\r\ncould you help me understand how to use deep learning for text recognition in ml.net","Url":"https://github.com/dotnet/machinelearning/issues/5570","RelatedDescription":"Closed issue \"Deep learning and text recognition \" (#5570)"},{"Id":"777945435","IsPullRequest":false,"CreatedAt":"2021-01-04T09:11:04","Actor":"justinormont","Number":"5575","RawContent":null,"Title":"OVA-PriorPredictor fails","State":"open","Body":"When placed in a One-vs-All (OVA), `PriorPredictor` throws a \"Nullable object must have a value\" error.\r\n\r\n**Error:**\r\n```\r\nUnhandled exception. System.InvalidOperationException: Nullable object must have a value.\r\n   at System.Nullable`1.get_Value()\r\n   at Microsoft.ML.Data.SchemaBindablePredictorWrapperBase.SingleValueRowMapper.GetInputColumnRoles()+MoveNext()\r\n   at Microsoft.ML.Data.RoleMappedSchema.MapFromNames(DataViewSchema schema, IEnumerable`1 roles, Boolean opt)\r\n   at Microsoft.ML.Data.RoleMappedSchema..ctor(DataViewSchema schema, IEnumerable`1 roles, Boolean opt)\r\n   at Microsoft.ML.Data.PredictedLabelScorerBase.BindingsImpl.ApplyToSchema(DataViewSchema input, ISchemaBindableMapper bindable, IHostEnvironment env)\r\n   at Microsoft.ML.Data.PredictedLabelScorerBase..ctor(IHostEnvironment env, PredictedLabelScorerBase transform, IDataView newSource, String registrationName)\r\n   at Microsoft.ML.Data.MulticlassClassificationScorer..ctor(IHostEnvironment env, MulticlassClassificationScorer transform, IDataView newSource)\r\n   at Microsoft.ML.Data.MulticlassClassificationScorer.ApplyToDataCore(IHostEnvironment env, IDataView newSource)\r\n   at Microsoft.ML.Data.RowToRowScorerBase.ApplyToData(IHostEnvironment env, IDataView newSource)\r\n   at Microsoft.ML.Data.PredictionTransformerBase`1.Transform(IDataView input)\r\n   at Microsoft.ML.Data.EstimatorChain`1.Fit(IDataView input)\r\n   at Program.Main()\r\n```\r\n\r\n`PriorPredictor` does work properly when not in an OVA (aka, used as a normal binary trainer).\r\n\r\n**Repro:**\r\n\r\nSee fiddle: https://dotnetfiddle.net/bs5hiw\r\n\r\nFiddle demonstrates:\r\n* Binary `PriorPredictor` is successful\r\n* `OVA-AveragedPerceptron` is successful\r\n* `OVA-PriorPredictor` **fails**\r\n\r\nRelevant code:\r\n```c#\r\nvar baselineMulticlassPipeline = mlContext.Transforms.Conversion.MapValueToKey(\"Label\", \"Label\")\r\n\t\t\t.Append(mlContext.MulticlassClassification.Trainers.OneVersusAll(mlContext.BinaryClassification.Trainers.Prior(labelColumnName: \"Label\"), labelColumnName: \"Label\"));\r\n\t\tvar multiclassModel = baselineMulticlassPipeline.Fit(split.TrainSet);\r\n```\r\n\r\n**Cause:**\r\n`PriorPredictor` differs from other trainers in part as it doesn't take in a `Features` column (and only looks at the `Label` column). My first guess would be when wrapping the One-vs-All in an Estimators API, we missed the part that the `Features` column won't exist for the `PriorPredictor`.","Url":"https://github.com/dotnet/machinelearning/issues/5575","RelatedDescription":"Open issue \"OVA-PriorPredictor fails\" (#5575)"},{"Id":"777808773","IsPullRequest":false,"CreatedAt":"2021-01-04T04:41:57","Actor":"justinormont","Number":"5574","RawContent":null,"Title":"Improve usability of AutoML column not found error","State":"open","Body":"Let's make the error message more actionable. \r\n\r\n**Error user sees:**\r\n![image](https://user-images.githubusercontent.com/4080826/103501081-fe1a4380-4e01-11eb-9376-e41af9105813.png)\r\n\r\nI would recommend adding similar named column(s):\r\n```diff\r\n- $\"Provided {columnPurpose} column '{columnName}' not found in training data.\"\r\n+ $\"Provided {columnPurpose} column '{columnName}' not found in training data. Did you mean '{closestNamed}'.\"\r\n```\r\n\r\nFor my current example, this would print: `Provided ignored column 'tagMaxTotalItem' not found in training data. Did you mean 'tagMaxTotalItems'.`\r\n\r\nI'd recommend using Levenshtein distance to find the closest named column ([code](https://github.com/dotnet/command-line-api/blob/34a2df49f1dcf3fdc46d0c4a3daedf84b3057e05/src/System.CommandLine/Invocation/TypoCorrection.cs#L79-L146)).\r\n\r\n**Code location:**\r\nhttps://github.com/dotnet/machinelearning/blob/5dbfd8acac0bf798957eea122f1413209cdf07dc/src/Microsoft.ML.AutoML/Utils/UserInputValidationUtil.cs#L248-L252\r\n\r\n**Background:**\r\nIt took me ~20min to debug why this error was occurring (obvious in retrospect). My column existed in the dataset, it existed in my loader function, it existed in my IDataView, ...; simply was just misspelt (\"tagMaxTotalItem\" instead of \"tagMaxTotalItems\"). \r\n\r\nImproving the usability of this error message will save future users' time.","Url":"https://github.com/dotnet/machinelearning/issues/5574","RelatedDescription":"Open issue \"Improve usability of AutoML column not found error\" (#5574)"},{"Id":"777288669","IsPullRequest":false,"CreatedAt":"2021-01-01T13:51:53","Actor":"justinormont","Number":"5573","RawContent":null,"Title":"AutoML doesn't allow ignored columns of incorrect type","State":"open","Body":"When we are validating the datatypes of the incoming columns, we throw an error even if the column is marked as ignored.\r\n\r\nFor AutoML, any columns marked as ignored should skip this validation check. Users are currently blocked from handling the column featurization independently of AutoML in a [preFeaturizer](https://github.com/dotnet/machinelearning-samples/blob/0f01ae23a204d142923fb678c5f178b10592674d/samples/csharp/getting-started/AdvancedExperiment_AutoML/AdvancedTaxiFarePrediction/Program.cs#L99-L105) or for other purposes after AutoML completes.\r\n\r\n### Error\r\n<!--![image](https://user-images.githubusercontent.com/4080826/103439305-8b864980-4bf0-11eb-94cf-dc63a8130551.png)-->\r\n\r\nUnhandled exception. System.ArgumentException: Only supported feature column types are Boolean, Single, and String. Please change the feature column unixTimeStamp of type Int64 to one of the supported types. (Parameter 'trainData')\r\n\r\nStack:\r\n````\r\nUnhandled exception. System.ArgumentException: Only supported feature column types are Boolean, Single, and String. Please change the feature column unixTimeStamp of type Int64 to one of the supported types. (Parameter 'trainData')\r\n   at Microsoft.ML.AutoML.UserInputValidationUtil.ValidateTrainData(IDataView trainData, ColumnInformation colum\r\nnInformation)\r\n   at Microsoft.ML.AutoML.UserInputValidationUtil.ValidateExperimentExecuteArgs(IDataView trainData, ColumnInformation columnInformation, IDataView validationData, TaskKind task)\r\n   at Microsoft.ML.AutoML.ExperimentBase`2.ExecuteTrainValidate(IDataView trainData, ColumnInformation columnInfo, IDataView validationData, IEstimator`1 preFeaturizer, IProgress`1 progressHandler)\r\n   at Microsoft.ML.AutoML.ExperimentBase`2.Execute(IDataView trainData, IDataView validationData, ColumnInformation columnInformation, IEstimator`1 preFeaturizer, IProgress`1 progressHandler)\r\n   at ImgurClassifier.ConsoleApp.ModelBuilder.TrainAutoMLSubPipeline2(MLContext mlContext, IDataView trainDataView, IDataView validationDataView) in /Users/justinormont/Documents/src/ImgurClassifier/ImgurClassifier.ConsoleApp/ModelBuilder.cs:line 439\r\n   at ImgurClassifier.ConsoleApp.ModelBuilder.BuildTrainingPipelineUsingAutoML(MLContext mlContext, IDataView trainDataView, IDataView validationDataView, Boolean useAutoML) in /Users/justinormont/Documents/src/ImgurClassifier/ImgurClassifier.ConsoleApp/ModelBuilder.cs:line 159\r\n   at ImgurClassifier.ConsoleApp.ModelBuilder.CreateModel(MLContext mlContext) in /Users/justinormont/Documents/src/ImgurClassifier/ImgurClassifier.ConsoleApp/ModelBuilder.cs:line 65\r\n   at ImgurClassifier.ConsoleApp.Program.Main(String[] args) in /Users/justinormont/Documents/src/ImgurClassifier/ImgurClassifier.ConsoleApp/Program.cs:line 32\r\n````\r\nLocation in code:\r\nhttps://github.com/dotnet/machinelearning/blob/5dbfd8acac0bf798957eea122f1413209cdf07dc/src/Microsoft.ML.AutoML/Utils/UserInputValidationUtil.cs#L87-L100\r\n\r\n### Repro\r\nAdd a column of type `long` to your loader class:\r\n```c#\r\n        [ColumnName(\"unixTimeStamp\"), LoadColumn(25)]\r\n        public long UnixTimeStamp { get; set; }\r\n```\r\n\r\nIn the AutoML column information, set column as ignored: ([example code](https://github.com/dotnet/machinelearning-samples/blob/0f01ae23a204d142923fb678c5f178b10592674d/samples/csharp/getting-started/AdvancedExperiment_AutoML/AdvancedTaxiFarePrediction/Program.cs#L107-L110))\r\n```c#\r\ncolumnInformation.IgnoredColumnNames.Add(\"unixTimeStamp\");\r\n```\r\n\r\nLoad dataset in using AutoML, and receive the given error.\r\n\r\n#### Work around\r\n\r\nMitigations:\r\n* User can modify their loader function to either remove the column completely, though as the column is fully removed from the IDataView, this blocks them from working with the column in a [preFeaturizer](https://github.com/dotnet/machinelearning-samples/blob/0f01ae23a204d142923fb678c5f178b10592674d/samples/csharp/getting-started/AdvancedExperiment_AutoML/AdvancedTaxiFarePrediction/Program.cs#L99-L105) or for other ML․NET uses (e.g. having an [AutoML based stacked ensemble](https://github.com/justinormont/ImgurClassifier/blob/13b034fa311e0e314a74faf955066ca276296be7/ImgurClassifier.ConsoleApp/ModelBuilder.cs#L238-L240), and handling the specific column featurization in the main ML․NET pipeline).<br/><br/>\r\n* User can load the column as a `string` type and use the [ConvertType](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.conversionsextensionscatalog.converttype?view=ml-dotnet) transform to convert to the needed type (e.g. `long`). AutoML will see the column as the acceptable `string` type, and still rightfully ignore the column.\r\n\r\n### Fix\r\nWe should skip validation of any columns which are marked as `ColumnPurpose.Ignore`.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5573","RelatedDescription":"Open issue \"AutoML doesn't allow ignored columns of incorrect type\" (#5573)"},{"Id":"776655431","IsPullRequest":false,"CreatedAt":"2020-12-31T11:59:10","Actor":"nsulikowski","Number":"5572","RawContent":null,"Title":"ConvertToOnnx produces binary not compatible with TSQL PREDICT","State":"closed","Body":"### System information\r\n\r\nWindows, NET 5\r\n\r\n### Issue\r\n\r\nI converted my binary classification model to ONNX format using context.Model.ConvertToOnnx and inserted it into SQL 2019.\r\nThen I tried to run a prediction using the TSQL command PREDICT, and it complained that the model was invalid or corrupt.","Url":"https://github.com/dotnet/machinelearning/issues/5572","RelatedDescription":"Closed issue \"ConvertToOnnx produces binary not compatible with TSQL PREDICT\" (#5572)"},{"Id":"776536968","IsPullRequest":false,"CreatedAt":"2020-12-30T16:40:03","Actor":"AniaBerthelot","Number":"5571","RawContent":null,"Title":"producengrams error","State":"closed","Body":"### System information\r\n- ML.NET 1.5.4\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\nI'm trying to reproduce this tutorial\r\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.textcatalog.producengrams?view=ml-dotnet\r\n\r\n- **What happened?**\r\nI was surprised by this error \r\n`System.InvalidOperationException: Can't bind the IDataView column 'Tokens' of type 'Vector<Key<UInt32, 0-47>>' to field or property 'Tokens' of type 'System.Single[]'.`\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5571","RelatedDescription":"Closed issue \"producengrams error\" (#5571)"},{"Id":"775136444","IsPullRequest":false,"CreatedAt":"2020-12-28T00:34:19","Actor":"justinormont","Number":"5569","RawContent":null,"Title":"OMP: Error #15: Initializing libiomp5.dylib in Image Classification with AutoML","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**:\r\n>  OS Name:     Mac OS X\r\n>  OS Version:  10.14\r\n>  OS Platform: Darwin\r\n>  RID:         osx.10.14-x64\r\n>  Base Path:   /usr/local/share/dotnet/sdk/5.0.101/\r\n\r\n- **.NET Version (eg., dotnet --info)**: \r\n> Version:   5.0.101\r\n>  Commit:    d05174dc5a\r\n>  \r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\nI was creating a new ML․NET sample showcasing [multi-modal ](https://en.wikipedia.org/wiki/Multimodal_learning)(photos, text, categorical, numeric) techniques and [model stacking](https://en.wikipedia.org/wiki/Model_stacking). \r\n\r\n- **What happened?**\r\nWhen I stack an AutoML model using image features, I get the below error.\r\n\r\nError:\r\n\r\n> `OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.`\r\n> `OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented work a round you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.`\r\n\r\n![image](https://user-images.githubusercontent.com/4080826/103181044-5c1eb800-4851-11eb-8585-83703553c332.png)\r\n\r\nThis seems to occur only after adding the second AutoML model which works on the image features:\r\nhttps://github.com/justinormont/ImgurClassifier/blob/13b034fa311e0e314a74faf955066ca276296be7/ImgurClassifier.ConsoleApp/ModelBuilder.cs#L154-L169\r\n\r\nThe same image pipeline is present (and causes no errors) within the main pipeline:\r\nhttps://github.com/justinormont/ImgurClassifier/blob/13b034fa311e0e314a74faf955066ca276296be7/ImgurClassifier.ConsoleApp/ModelBuilder.cs#L222-L232\r\n\r\nSpeculation on cause:\r\n* Error didn't occur until an AutoML stacked model was created for the image features. Perhaps it occurs when having two sets of the image featurizers? \r\n* The error is possibly/likely system dependent (and on macOS)\r\n* NuGet [runtime.osx.10.10-x64.CoreCompat.System.Drawing](https://www.nuget.org/packages/runtime.osx.10.10-x64.CoreCompat.System.Drawing/) is included in the project to get around a [libgdiplus error](https://gist.github.com/mtolly/384dbe51f4a78d6d2818#gistcomment-3573216); this may link to a separate version of libomp\r\n\r\n### Source code / logs\r\n\r\n#### Repro\r\nCheckout sample at current commit:\r\n```bash\r\ngit clone https://github.com/justinormont/ImgurClassifier.git\r\ncd ImgurClassifier\r\ngit checkout 13b034fa311e0e314a74faf955066ca276296be7\r\n```\r\nRuntime is expected to be ~2hr, and fails at ~30-45min. You may want reduce the AutoML time to `0` seconds ([code](https://github.com/justinormont/ImgurClassifier/blob/13b034fa311e0e314a74faf955066ca276296be7/ImgurClassifier.ConsoleApp/ModelBuilder.cs#L139), [code](https://github.com/justinormont/ImgurClassifier/blob/13b034fa311e0e314a74faf955066ca276296be7/ImgurClassifier.ConsoleApp/ModelBuilder.cs#L178)), so only one model will be created. Though, the error may be occurring within the fourth model created (SymbolicSgdLogisticRegressionOva); I'll catch the stack trace on my next run.\r\n\r\n#### Anti-Repro\r\nThe error doesn't occur if the [second AutoML sub-pipeline](https://github.com/justinormont/ImgurClassifier/blob/13b034fa311e0e314a74faf955066ca276296be7/ImgurClassifier.ConsoleApp/ModelBuilder.cs#L135-L170) (using images) is removed, even though the same image featurizers are still utilized in the [main pipeline](https://github.com/justinormont/ImgurClassifier/blob/13b034fa311e0e314a74faf955066ca276296be7/ImgurClassifier.ConsoleApp/ModelBuilder.cs#L199-L252).\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5569","RelatedDescription":"Open issue \"OMP: Error #15: Initializing libiomp5.dylib in Image Classification with AutoML\" (#5569)"},{"Id":"774819768","IsPullRequest":false,"CreatedAt":"2020-12-26T04:27:05","Actor":"justinormont","Number":"5568","RawContent":null,"Title":"AveragedPerceptron default iterations","State":"open","Body":"When changing AveragedPerceptron's default number of iterations from 1 to 10 in https://github.com/dotnet/machinelearning/pull/5258, we missed one.\r\n\r\nThe main interface continues to default to 1 iteration:\r\n![image](https://user-images.githubusercontent.com/4080826/103145371-35943c00-46ee-11eb-99c8-3f44f0b27ee4.png)\r\n\r\nPrevious issues:\r\n* https://github.com/dotnet/machinelearning/issues/4749\r\n* https://github.com/dotnet/machinelearning/pull/5258\r\n\r\n### Code location\r\nDefault is set here:\r\nhttps://github.com/dotnet/machinelearning/blob/5dbfd8acac0bf798957eea122f1413209cdf07dc/src/Microsoft.ML.StandardTrainers/Standard/Online/AveragedPerceptron.cs#L172\r\n\r\nWhich originates from:\r\nhttps://github.com/dotnet/machinelearning/blob/5dbfd8acac0bf798957eea122f1413209cdf07dc/src/Microsoft.ML.StandardTrainers/Standard/Online/AveragedLinear.cs#L108-L113\r\n\r\nWhich in-turn inherits its value from:\r\nhttps://github.com/dotnet/machinelearning/blob/5dbfd8acac0bf798957eea122f1413209cdf07dc/src/Microsoft.ML.StandardTrainers/Standard/Online/OnlineLinear.cs#L62-L65\r\n\r\n### Possible fix\r\nWe may want to make a new class within AveragedPerceptron to hold its overrides:\r\n```c#\r\n        internal class AveragedPerceptronDefault : AveragedDefault\r\n        { \r\n            public const float NumberOfIterations = 10; \r\n        } \r\n```\r\n\r\n/cc @michaelgsharp ","Url":"https://github.com/dotnet/machinelearning/issues/5568","RelatedDescription":"Open issue \"AveragedPerceptron default iterations\" (#5568)"},{"Id":"774066605","IsPullRequest":false,"CreatedAt":"2020-12-23T22:08:30","Actor":"justinormont","Number":"5567","RawContent":null,"Title":"Improve SamplingKeyColumn documentation and usability","State":"open","Body":"The use `SamplingKeyColumn` is rather confusing. Perhaps we can improve it with documentation and runtime checks/warnings.\r\n\r\n> @tasmektep: In your sample, you're using the [SamplingKeyColumn with your Label](https://github.com/tasmektep/DotNetMLSplit/blob/cf7ee532fd00221e5ad1fa420bd28678c4f52c85/DotNetMLSplit/Program.cs#L23 ) in it. \r\n> \r\n> Using your Label as your SamplingKeyColumn will cause all rows with the same Label value to be placed together in the same splits/folds (as you're seeing). \r\n> \r\n> Description from docs: \r\n> > **SamplingKeyColumn**: \r\n> > Name of a column to use for grouping rows. If two examples share the same value of the samplingKeyColumnName, they are guaranteed to appear in the same subset (train or test). This can be used to ensure no label leakage from the train to the test set. Note that when performing a Ranking Experiment, the samplingKeyColumnName must be the GroupId column. If null no row grouping will be performed.\r\n> >\r\n> > https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.dataoperationscatalog.traintestsplit?view=ml-dotnet\r\n> \r\n> You are likely thinking of the related, but inverse, concept of [Stratification](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold) where the rows are evenly represented between the splits/folds. Stratification has some downsides causing it be less helpful.\r\n\r\n_Originally posted by @justinormont in https://github.com/dotnet/machinelearning/issues/5563#issuecomment-749811593_\r\n\r\n> @tasmektep: Keep posting issues that you run into.  And thanks for posting your repro.\r\n>\r\n> Work on ML․NET side:\r\n> * Warning -- Have the splitter warn when zero rows are present in a split, with a special warning if SamplingKeyColumn is used. In the same fix, we could warn of unbalanced splits/folds to help https://github.com/dotnet/machinelearning/issues/3711. Down side is the user would need to attach a logger to see the warning.\r\n> * Documentation\r\n>   * Param hover -- Ensure the hover description for SamplingKeyColumn in Visual Studio is well worded to explain the concept, and perhaps mention what it does not do.\r\n>    * Samples/main docs -- Further explain the concept of SamplingKeyColumn, why its useful, and also what it does not do.\r\n> \r\n\r\n_Originally posted by @justinormont in https://github.com/dotnet/machinelearning/issues/5563#issuecomment-749822566_\r\n\r\nIn the param hover for `SamplingKeyColumn` in Visual Studio, mentioned above, we can also say to not put your `Label` column in `SamplingKeyColumn`.\r\n\r\nIt would be nice to automatically check this in `TrainTestSplit`, but the `SamplingKeyColumn` and `Label` aren't in the `TrainTestSplit` parameters together. The AutoML APIs can have this check as `SamplingKeyColumn` and `Label` are both in the parameters (and may want to throw an `ArgumentException` instead of warn).","Url":"https://github.com/dotnet/machinelearning/issues/5567","RelatedDescription":"Open issue \"Improve SamplingKeyColumn documentation and usability\" (#5567)"},{"Id":"772088487","IsPullRequest":false,"CreatedAt":"2020-12-23T20:00:10","Actor":"minotru","Number":"5565","RawContent":null,"Title":"IHost, IHostEnvironment and IChannel usage","State":"closed","Body":"Hello all,\r\nThank you for such a great framework as ML.NET.\r\n\r\nI would like to know is there any documentation for IHost and Channels and how should I used them in custom algorithms.\r\n\r\nBackground:\r\n* We have a custom algorithm on my project, I am trying to implement this algorithm as an ML.NET-compatible transformer so that it can be used inside ML.NET Pipelines.\r\n* This algorithm is a variation of FastText/Glove with vectors stored in a database.\r\n* I decided to investigate `WordEmbeddingExtractor` source code as a starting point for my implementation.\r\n* And was surprised to see a lot of `Host`, `IHostEnvironment` and `Channel` calls.\r\n\r\nThere are a lot of parts like:\r\n```cs\r\nusing (var ch = host.Start(\"step X\"))\r\n{\r\n   //\r\n}\r\n```\r\n\r\nUnfortunately, I have not found any comprehensive documentation on `IHost` and `IChannel`, why it is needed, how it is used.\r\n\r\nCould you please explain in short what is `IChannel` and `IHost` and how to use them in custom code?\r\n\r\nThanks,\r\nSimon","Url":"https://github.com/dotnet/machinelearning/issues/5565","RelatedDescription":"Closed issue \"IHost, IHostEnvironment and IChannel usage\" (#5565)"},{"Id":"771508773","IsPullRequest":false,"CreatedAt":"2020-12-23T16:31:15","Actor":"tasmektep","Number":"5563","RawContent":null,"Title":"TrainTestSplit is not working properly, when the column name provided","State":"closed","Body":"I think there is a issue with \"TrainTestSplit\" function.\r\n\r\nWhen I tried to split data that consist of 500 samples which have equal number of sample from each class, it returns the testset as 0 number of rows. However when I try to split it without \"label\" with 0.1 ratio, it returns as 49 to testset rows and 451 to trainset rows.\r\n\r\nIs there a way to solve this problem?\r\n\r\nI'm using final packages.","Url":"https://github.com/dotnet/machinelearning/issues/5563","RelatedDescription":"Closed issue \"TrainTestSplit is not working properly, when the column name provided\" (#5563)"},{"Id":"772962719","IsPullRequest":false,"CreatedAt":"2020-12-22T13:52:18","Actor":"justinormont","Number":"5566","RawContent":null,"Title":"SVMLightLoader Fails above 128 dense rows","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: macOS 10.14.6\r\n- **.NET Version (eg., dotnet --info)**: 5.0.101\r\n\r\n### Issue\r\n\r\nSVMLightLoader dies if when you load >128 dense rows.\r\n\r\nWhen the feature [column sparsity is >0.25](https://github.com/dotnet/machinelearning/blob/cf7dbffa50f7bd65172bfcb11537207a81250994/src/Microsoft.ML.Core/Utilities/VBufferUtils.cs#L21), internally the column is represented in sparse format, else dense. SVMLightLoader works if either the column is sparse (many missing values), or if the number of rows is < 128.\r\n\r\n#### Error\r\nFails with one of three errors: _(dataset dependent)_\r\n* > System.InvalidOperationException: Duplicate keys found in dataset  \r\n* > System.ArgumentException: Destination is too short. (Parameter 'destination')\r\n* > System.IndexOutOfRangeException: Index was outside the bounds of the array.\r\n\r\nStack trace:\r\n\r\n> ```\r\n> Unhandled exception. System.InvalidOperationException: Splitter/consolidator worker encountered exception while consuming source data\r\n>  ---> System.InvalidOperationException: Duplicate keys found in dataset\r\n>    at Microsoft.ML.Data.SvmLightLoader.OutputMapper.MapCore(VBuffer`1& keys, VBuffer`1& values, Output output)\r\n>    at Microsoft.ML.Data.SvmLightLoader.OutputMapper.Map(IntermediateOut intermediate, Output output)\r\n>    at Microsoft.ML.Transforms.CustomMappingTransformer`2.Mapper.<>c__DisplayClass5_0.<Microsoft.ML.Data.IRowMapper.CreateGetters>b__0()\r\n>    at Microsoft.ML.Transforms.CustomMappingTransformer`2.Mapper.<>c__DisplayClass6_0`1.<GetDstGetter>b__0(T& dst)\r\n>    at Microsoft.ML.Data.DataViewUtils.Splitter.InPipe.Impl`1.Fill()\r\n>    at Microsoft.ML.Data.DataViewUtils.Splitter.<>c__DisplayClass7_1.<ConsolidateCore>b__2()\r\n>    --- End of inner exception stack trace ---\r\n>    at Microsoft.ML.Data.DataViewUtils.Splitter.Batch.SetAll(OutPipe[] pipes)\r\n>    at Microsoft.ML.Data.DataViewUtils.Splitter.Cursor.MoveNextCore()\r\n>    at Microsoft.ML.Data.RootCursorBase.MoveNext()\r\n>    at Microsoft.ML.Data.SynchronizedCursorBase.MoveNext()\r\n>    at SVMLightLoaderTest.Program.PrintData(IDataView svmData) in /Users/justinormont/Projects/SVMLightLoaderTest/SVMLightLoaderTest/Program.cs:line 121\r\n>    at SVMLightLoaderTest.Program.Main() in /Users/justinormont/Projects/SVMLightLoaderTest/SVMLightLoaderTest/Program.cs:line 45\r\n> ```\r\n\r\nPoints to:\r\nhttps://github.com/dotnet/machinelearning/blob/5dbfd8acac0bf798957eea122f1413209cdf07dc/src/Microsoft.ML.Transforms/SvmLight/SvmLightLoader.cs#L364-L388\r\n\r\nSide note: It looks like Visual Studio on MacOS is not loading the symbols (or source) for ML․NET.\r\n\r\n\r\n### Source code / logs\r\n\r\nRepro:\r\n* .NET Fiddle -- https://dotnetfiddle.net/WbKlzS\r\n* Visual Studio Solution: [SVMLightLoaderTest.zip](https://github.com/dotnet/machinelearning/files/5730051/SVMLightLoaderTest.zip)\r\n\r\n\r\nBug exists in ML․NET v1.5.0 to v.1.5.4 (current). SvmLightLoader was added in v1.5.0.\r\n\r\n## Background\r\n\r\nI was attempting to run AutoML․NET on a [SVM Light dataset](https://www.sysnet.ucsd.edu/projects/url/) ([download](https://www.sysnet.ucsd.edu/projects/url/url_svmlight.tar.gz)) using the CLI. But we lack SVM Light support in AutoML․NET, so I was attempting to convert the SVM Light file to a sparse TSV. The goal was to have AutoML․NET read the converted sparse TSV file, but the conversion failed.\r\n\r\nUsing MAML in v1.5.4: _(fails)_\r\n`dotnet ./bin/AnyCPU.Release/Microsoft.ML.Console/netcoreapp2.1/MML.dll SaveData data=Day0.svm loader=SvmLightLoader{} xf=SelectColumns{keep=Label keep=Features} saver=Text{schema=- dense=-} dout=Day0.tsv`\r\n\r\nThis fails with the above errors, as the current SvmLightLoader fails.\r\n\r\nUsing TLC's MAML: _(works)_\r\n`maml.exe SaveData data=Day0.svm loader=SvmLightLoader{} xf=KeepColumns{col=Label col=Features} saver=Text{schema=- dense=-} dout=Day0.tsv`\r\n\r\nThe old internal version of ML․NET (TLC) works properly in reading the SVM Light format and writing a TSV. The implies there was a bug introduced when we released SvmLightLoader with v1.5.0 of ML․NET.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5566","RelatedDescription":"Open issue \"SVMLightLoader Fails above 128 dense rows\" (#5566)"},{"Id":"771751039","IsPullRequest":false,"CreatedAt":"2020-12-21T00:17:26","Actor":"dcostea","Number":"5564","RawContent":null,"Title":"Microsoft.ML.Data attributes are not supported by C# 9 positional records","State":"open","Body":"### System information\r\n\r\n- Windows 10:\r\n- .NET 5.0.1: \r\n\r\n### Issue\r\n\r\n- I have replaced class with record inside an input data model class.\r\n- Positional records from C# 9 doesn't not support Microsoft.ML.Data attributes like: ColumnNameAttribute, LoadColumnAttribute, VectorTypeAttribute.\r\n- I expect that all Microsoft.ML.Data attributes to be supported by records.\r\n\r\n```record Product([VectorType(2)] string Brand, string Model, decimal Price);```\r\n\r\n![attribute](https://user-images.githubusercontent.com/15055082/102728066-f2bb1880-4329-11eb-9f91-dc9531fd7f49.png)\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5564","RelatedDescription":"Open issue \"Microsoft.ML.Data attributes are not supported by C# 9 positional records\" (#5564)"},{"Id":"770462769","IsPullRequest":true,"CreatedAt":"2020-12-18T03:42:31","Actor":"harishsk","Number":"5561","RawContent":null,"Title":"Renamed release notes file","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/5561","RelatedDescription":"Closed or merged PR \"Renamed release notes file\" (#5561)"},{"Id":"770461057","IsPullRequest":true,"CreatedAt":"2020-12-18T03:42:01","Actor":"harishsk","Number":"5560","RawContent":null,"Title":"Updated version to 1.5.5 and 0.17.5","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/5560","RelatedDescription":"Closed or merged PR \"Updated version to 1.5.5 and 0.17.5\" (#5560)"}],"ResultType":"GitHubIssue"}},"RunOn":"2021-01-14T05:30:36.2018636Z","RunDurationInMilliseconds":783}