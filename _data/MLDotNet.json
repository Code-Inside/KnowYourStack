{"Data":{"GitHub":{"Issues":[{"Id":"862393566","IsPullRequest":false,"CreatedAt":"2021-04-20T04:12:37","Actor":"justinbhopper","Number":"5754","RawContent":null,"Title":"Non-deterministic SdcaLogisticRegression results when using fixed seeds","State":"open","Body":"I am getting wildly different results using (F1 scores ranging from 0.5% to 30%) each time I run my simple pipeline with the SdcaLogisticRegressionBinaryTrainer on the same dataset.  I have set the MLContext's seed, as well as the seed on TrainTestSplit, but still the results are non-deterministic.\r\n\r\nMy dataset consists of ~1.3 million rows.  Some pseudo code of what my pipeline has:\r\n\r\n```\r\nvar context = new MLContext(seed: 0); // Set seed\r\nvar data = context.Data.LoadFromTextFile<DataInput>(_dataPath);\r\n\r\nvar split = _context.Data.TrainTestSplit(data, testFraction: 0.1, seed: 0);\r\n\r\nvar pipeline = context.Transforms.CustomMapping<DataInput, Data>(new DataFactory().GetMapping(), contractName: \"Data\")\r\n    .Append(context.Transforms.CopyColumns(\"Label\", nameof(Data.NoShow)))\r\n    .Append(context.Transforms.Concatenate(\"Features\", allFeatureNames));\r\n    \r\nvar trainer = _context.BinaryClassification.Trainers.SdcaLogisticRegression();\r\nvar model = pipeline.Append(trainer).Fit(split.TrainSet);\r\n\r\nvar predictions = model.Transform(split.TestSet);\r\nvar metrics = context.BinaryClassification.Evaluate(predictions);\r\nConsole.WriteLine(metrics.F1Score);\r\n```\r\n\r\nIs the SdcaLogisticRegressionBinaryTrainer just inheritently non-deterministic?  Is there an explanation as to why my F1Scores would jump so dramatically each time?","Url":"https://github.com/dotnet/machinelearning/issues/5754","RelatedDescription":"Open issue \"Non-deterministic SdcaLogisticRegression results when using fixed seeds\" (#5754)"},{"Id":"862153117","IsPullRequest":true,"CreatedAt":"2021-04-20T00:08:29","Actor":"stephentoub","Number":"5753","RawContent":null,"Title":"Fix erroneous use of TaskContinuationOptions in ThreadUtils.cs","State":"open","Body":"This was supposed to be TaskCreationOptions.  TaskContinuationOptions here will end up binding to the wrong ctor, and won't actually have the desired effect, which is to avoid blocking the thread completing this TCS.\r\n\r\nThanks @sharwell for flagging it with CA2247.","Url":"https://github.com/dotnet/machinelearning/pull/5753","RelatedDescription":"Open PR \"Fix erroneous use of TaskContinuationOptions in ThreadUtils.cs\" (#5753)"},{"Id":"853653532","IsPullRequest":false,"CreatedAt":"2021-04-19T13:05:47","Actor":"Lazar-source","Number":"5740","RawContent":null,"Title":"[Question] Model serialization and store in database","State":"closed","Body":"### System information\r\n\r\n- OS version/distro: Win x64\r\n- .NET Version (eg., dotnet --info): netcore 3.1\r\n\r\n### Issue\r\n\r\n\r\n I'm in a project and we are using mlnet. We are using a lot of models with different trainers. \r\n I know there is a way to save it as a zip file, but i didnt like that solution. My question is:\r\n Is there a way to serialize the trained model(object) and store it in database? I havent found any information related to it, and i dont know a lot about serialization.\r\n\r\nThanks for answers!\r\n \r\n \r\n\r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5740","RelatedDescription":"Closed issue \"[Question] Model serialization and store in database\" (#5740)"},{"Id":"861071741","IsPullRequest":false,"CreatedAt":"2021-04-19T08:55:52","Actor":"uzfm","Number":"5752","RawContent":null,"Title":"ML.NET support CUDA 11","State":"open","Body":"ML.NET support CUDA 11\r\nLoading model from: F:\\V2Sorter\\DataSet\\03.01.2021\\imageClassifier_2.zip\r\n2021-04-16 16:06:33.457209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:03:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.905GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2021-04-16 16:06:33.458465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-16 16:06:33.459114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-04-16 16:06:33.459639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-04-16 16:06:33.460164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-04-16 16:06:33.460705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-04-16 16:06:33.461270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-04-16 16:06:33.461946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-04-16 16:06:33.462589: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-04-16 16:06:33.463409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-04-16 16:06:33.464111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-04-16 16:06:33.464830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0\r\n2021-04-16 16:06:33.465221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N\r\n2021-04-16 16:06:33.465788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7745 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:03:00.0, compute capability: 8.6)\r\nException thrown: 'System.EntryPointNotFoundException' in Microsoft.ML.Vision.dll\r\ndevice is disconnected\r\nException thrown: 'System.IO.FileNotFoundException' in mscorlib.dll\r\nException thrown: 'System.IO.FileNotFoundException' in mscorlib.dll\r\nThe thread 0x2f80 has exited with code 0 (0x0).\r\n\r\nI'm using \"SciSharp.TensorFlow.Redist-Windows-GPU\" with CUDA 2.4.0 support but get the exception \"Exception thrown: 'System.EntryPointNotFoundException' in Microsoft.ML.Vision.dll\"\r\nwhat could be the problem?","Url":"https://github.com/dotnet/machinelearning/issues/5752","RelatedDescription":"Open issue \"ML.NET support CUDA 11\" (#5752)"},{"Id":"860583786","IsPullRequest":false,"CreatedAt":"2021-04-18T06:04:13","Actor":"hirotakekadowaki","Number":"5751","RawContent":null,"Title":"Error message 'Dataset row count exceeded the maximum count of 2146435071' in Fit() method","State":"open","Body":"### System information\r\n\r\n- **Windows10 Home**:\r\n- **.NET Version 2.1.0**: \r\n- **ML.NET Version 1.5.5**: \r\n\r\n### Issue\r\n\r\n- **We are currently dealing with large sizes of data. \r\nIs it possible to include a record size larger than INTMAX in the size of training data?**\r\n\r\n![exception_row_count](https://user-images.githubusercontent.com/13389351/115135773-0a937680-a056-11eb-82c2-df471f964815.png)\r\n\r\n### Source code / logs\r\n\r\n        public static void CreateModel1()\r\n        {\r\n            // Load Data\r\n            IDataView trainingDataView = mlContext.Data.LoadFromTextFile<ModelInput>(\r\n                                            path: USDJPY_CSV_FILEPATH,\r\n                                            hasHeader: true,\r\n                                            separatorChar: ',',\r\n                                            allowQuoting: true,\r\n                                            allowSparse: false);\r\n\r\n            // Build training pipeline\r\n            IEstimator<ITransformer> trainingPipeline = BuildTrainingPipeline(mlContext);\r\n\r\n            // Evaluate quality of Model\r\n            Evaluate(mlContext, trainingDataView, trainingPipeline);\r\n\r\n            // Train Model\r\n            ITransformer mlModel = TrainModel(mlContext, trainingDataView, trainingPipeline);\r\n\r\n            // Save model\r\n            SaveModel(mlContext, mlModel, MODEL_FILEPATH, trainingDataView.Schema);\r\n\r\n\r\n        }\r\n        public static IEstimator<ITransformer> BuildTrainingPipeline(MLContext mlContext)\r\n        {\r\n            // Data process configuration with pipeline data transformations \r\n            var dataProcessPipeline = mlContext.Transforms.Categorical.OneHotHashEncoding(new[] { new InputOutputColumnPair(\"period\", \"period\") })\r\n                                      .Append(mlContext.Transforms.Concatenate(\"Features\", new[] { \"period\", \"st_price\", \"volume\" }));\r\n\r\n            // Set the training algorithm \r\n            var trainer = mlContext.Regression.Trainers.LightGbm(labelColumnName: \"ed_price\", featureColumnName: \"Features\");\r\n            var trainingPipeline = dataProcessPipeline.Append(trainer);\r\n\r\n            return trainingPipeline;\r\n        }\r\n\r\n        public static ITransformer TrainModel(MLContext mlContext, IDataView trainingDataView, IEstimator<ITransformer> trainingPipeline)\r\n        {\r\n            Console.WriteLine(\"=============== Training  model ===============\");\r\n\r\n            ITransformer model = trainingPipeline.Fit(trainingDataView);\r\n\r\n            Console.WriteLine(\"=============== End of training process ===============\");\r\n            return model;\r\n        }\r\n\r\n        private static void Evaluate(MLContext mlContext, IDataView trainingDataView, IEstimator<ITransformer> trainingPipeline)\r\n        {\r\n            // Cross-Validate with single dataset (since we don't have two datasets, one for training and for evaluate)\r\n            // in order to evaluate and get the model's accuracy metrics\r\n            Console.WriteLine(\"=============== Cross-validating to get model's accuracy metrics ===============\");\r\n            //var crossValidationResults = mlContext.Regression.CrossValidate(trainingDataView, trainingPipeline, numberOfFolds: 5, labelColumnName: \"ed_price\");\r\n            //PrintRegressionFoldsAverageMetrics(crossValidationResults);\r\n        }\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5751","RelatedDescription":"Open issue \"Error message 'Dataset row count exceeded the maximum count of 2146435071' in Fit() method\" (#5751)"},{"Id":"860372952","IsPullRequest":false,"CreatedAt":"2021-04-17T09:54:52","Actor":"tangbinbinyes","Number":"5750","RawContent":null,"Title":"How to know the correspondence between the score and the category output by the model?","State":"open","Body":"Hello,Thanks for the good project.After the model training is completed, how do I know the corresponding relationship between the score and the category output by the model? At present, the model only outputs a category and a float array. I only know which label is the highest score, and I don’t know the score what is the second or third label.","Url":"https://github.com/dotnet/machinelearning/issues/5750","RelatedDescription":"Open issue \"How to know the correspondence between the score and the category output by the model?\" (#5750)"},{"Id":"859755781","IsPullRequest":false,"CreatedAt":"2021-04-16T12:09:36","Actor":"francescomazzurco","Number":"5749","RawContent":null,"Title":"Publish single-file does not include MklImports","State":"open","Body":"Hi,\r\n\r\nI just migrated a simple console application depending on `Microsoft.ML.AutoML` from .NET Core 3.1 to .NET 5.0\r\nEverything builds fine, but the publishing behavior is different. \r\nIn .NET Core 3.1, I used to get a large single file containing all Mkl dlls.\r\n![image](https://user-images.githubusercontent.com/33526441/115021932-ef413380-9ebc-11eb-98b8-961345019cdc.png)\r\n\r\nIn .NET 5.0 I get many separated files where the main .exe is small (clearly too small to contain the large Mkl dlls).\r\n![image](https://user-images.githubusercontent.com/33526441/115021737-a5f0e400-9ebc-11eb-84f9-bd18ee4aa18e.png)\r\n\r\nBelow my publish profile. Switching back to .NET Core 3.1 solves the problem.  \r\n\r\n\r\n```\r\n<Project ToolsVersion=\"4.0\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\">\r\n  <PropertyGroup>\r\n    <Configuration>Release</Configuration>\r\n    <Platform>Any CPU</Platform>\r\n    <PublishDir>bin\\Release\\net5.0\\publish\\</PublishDir>\r\n    <PublishProtocol>FileSystem</PublishProtocol>\r\n    <TargetFramework>net5.0</TargetFramework>\r\n    <SelfContained>false</SelfContained>\r\n    <RuntimeIdentifier>win-x64</RuntimeIdentifier>\r\n    <PublishSingleFile>True</PublishSingleFile>\r\n    <PublishReadyToRun>False</PublishReadyToRun>\r\n  </PropertyGroup>\r\n</Project>\r\n```\r\n\r\nThis may not be the right place to post the issue, but `Microsoft.ML.AutoML` I experienced the problem with.\r\n\r\nThanks\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5749","RelatedDescription":"Open issue \"Publish single-file does not include MklImports\" (#5749)"},{"Id":"859080191","IsPullRequest":false,"CreatedAt":"2021-04-16T07:17:26","Actor":"AmirAliShokri","Number":"5748","RawContent":null,"Title":"Missing MatrixFactorizationTrainer In Current ML.Net (1.5.5)","State":"closed","Body":"### System information\r\n\r\n- **OS Windows 10 Pro**:\r\n- **.NET Version (5.1)**: \r\n- **Last Visual Studio**: \r\n\r\n\r\n### Screen Shot \r\nhttps://pasteboard.co/JXt92l9.png\r\n\r\nHey Every One \r\nI Learn a Course About Ml.Net And In This Course My Master Add Matrix Factorization Trainer Into His Code but Is This Version Of Ml.Net I Cant Find This Class \r\n\r\nPlease Help Me Thanks ","Url":"https://github.com/dotnet/machinelearning/issues/5748","RelatedDescription":"Closed issue \"Missing MatrixFactorizationTrainer In Current ML.Net (1.5.5)\" (#5748)"},{"Id":"848648121","IsPullRequest":true,"CreatedAt":"2021-04-14T22:19:02","Actor":"ericstj","Number":"5735","RawContent":null,"Title":"Use Official package for SharpZipLib","State":"closed","Body":"In reviewing our package usage in ML.NET I found we were using [SharpZipLib.NETStandard](https://www.nuget.org/packages/SharpZipLib.NETStandard/), a [fork](https://github.com/PingmanTools/SharpZipLib) of [SharpZipLib](https://www.nuget.org/packages/SharpZipLib/) rather than the official release.  The fork added a `netstandard1.3`  target (presumably not supported at the time the fork was created) but the latest SharpZipLib supports netstandard2.0 and did [at the time](https://github.com/dotnet/machinelearning/pull/3249) this dependency was added.\r\n\r\nThis package is used only in samples, but we should still use the official release and not a fork.  Looking at how this works, we don't even use this assembly in the project that references it, but do in https://github.com/dotnet/machinelearning/blob/main/docs/samples/Microsoft.ML.Samples/Microsoft.ML.Samples.csproj.  I'm assuming that's intentional and done so that folks can copy-paste our samples and only reference a single package, but if that's not intentional we could clean things up by moving this dependency to Microsoft.ML.Samples.csproj which doesn't produce a package.","Url":"https://github.com/dotnet/machinelearning/pull/5735","RelatedDescription":"Closed or merged PR \"Use Official package for SharpZipLib\" (#5735)"},{"Id":"858206119","IsPullRequest":false,"CreatedAt":"2021-04-14T19:38:33","Actor":"Blecovich","Number":"5747","RawContent":null,"Title":"Spelling error","State":"open","Body":"\r\n\"Feature engineering\" spelled \"...enginering\" in summary paragraph.\r\n\r\n---\r\n#### Document Details\r\n\r\n⚠ *Do not edit this section. It is required for docs.microsoft.com ➟ GitHub issue linking.*\r\n\r\n* ID: 7da38ee7-510b-6abf-1a33-2057c3dc2bd2\r\n* Version Independent ID: 05da2519-98ab-2f29-0e39-106a89b911af\r\n* Content: [MLContext Class (Microsoft.ML)](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.mlcontext?view=ml-dotnet)\r\n* Content Source: [dotnet/xml/Microsoft.ML/MLContext.xml](https://github.com/dotnet/ml-api-docs/blob/live/dotnet/xml/Microsoft.ML/MLContext.xml)\r\n* Product: **dotnet-ml-api**\r\n* GitHub Login: @natke\r\n* Microsoft Alias: **nakersha**","Url":"https://github.com/dotnet/machinelearning/issues/5747","RelatedDescription":"Open issue \"Spelling error\" (#5747)"},{"Id":"856423994","IsPullRequest":false,"CreatedAt":"2021-04-12T23:05:27","Actor":"pgovind","Number":"5746","RawContent":null,"Title":"DataFrame should support Array data","State":"open","Body":"Currently, DataFrame only supports primitive types. Specifically, a `DataFrameColumn` can only only 1 primitive value per row. There are applications however that need to hold a `DataFrameColumn` of `Array<T>`, so each row holds an array of values. ML.NET supports holding Array<T>, so `IDataView.ToDataFrame` might run into this issue at some point.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5746","RelatedDescription":"Open issue \"DataFrame should support Array data\" (#5746)"},{"Id":"856232587","IsPullRequest":false,"CreatedAt":"2021-04-12T20:23:52","Actor":"HashGrammer","Number":"5745","RawContent":null,"Title":"Resource/Meta files unavailable for Image Classification","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**: Windows 10 1909\r\n- **.NET Version (eg., dotnet --info)**:  2.1.519\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\nInstalled the MLNET Marketplace package for fresh install of visual studio 2017, opened the Model Builder, attempted to train a model.\r\n- **What happened?**\r\n\r\nAs noted, but not responded to, in the most recent comments of issue #5052, the site no longer exists and and .meta is not available. Have these resources been moved, or is this just an outage type of failure with the server? Where can we acquire them manually in the meantime? Why hasn't the URL been updated in the NuGet package?\r\n\r\n```\r\n[Source=ImageClassificationTrainer; Ensuring meta files are present., Kind=Info] Downloading resnet_v2_50_299.meta from https://aka.ms/mlnet-resources/meta/resnet_v2_50_299.meta to C:\\Users\\user\\AppData\\Local\\Temp\\MLNET\\resnet_v2_50_299.meta\r\n[Source=ImageClassificationTrainer; Ensuring meta files are present., Kind=Error] resnet_v2_50_299.meta: Could not download. WebClient returned the following error: The remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\r\n```\r\n- **What did you expect?**\r\nModel to train\r\n\r\n### Source code / logs\r\n\r\n```\r\n[Source=ImageClassificationTrainer; Ensuring meta files are present., Kind=Info] Downloading resnet_v2_50_299.meta from https://aka.ms/mlnet-resources/meta/resnet_v2_50_299.meta to C:\\Users\\user\\AppData\\Local\\Temp\\MLNET\\resnet_v2_50_299.meta\r\n[Source=ImageClassificationTrainer; Ensuring meta files are present., Kind=Error] resnet_v2_50_299.meta: Could not download. WebClient returned the following error: The remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\r\n[Source=ImageClassificationTrainer; Ensuring meta files are present., Kind=Trace] Channel finished. Elapsed 00:00:50.2436251.\r\n[Source=AutoML, Kind=Error] Pipeline crashed: xf=ValueToKeyMapping{ col=Label:Label} xf=RawByteImageLoading{ col=ImageSource_featurized:ImageSource imageFolder=} xf=ColumnCopying{ col=Features:ImageSource_featurized} tr=ImageClassification{} xf=KeyToValueMapping{ col=PredictedLabel:PredictedLabel} cache=- . Exception: System.InvalidOperationException: Error downloading resource from 'https://aka.ms/mlnet-resources/meta/resnet_v2_50_299.meta': The remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\nThe remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\nThe remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\nThe remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\nThe remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\n\r\nMeta file could not be downloaded! Please copy the model file 'resnet_v2_50_299.meta' from 'meta\\resnet_v2_50_299.meta' to 'C:\\Users\\user\\AppData\\Local\\Temp\\MLNET'.\r\n   at Microsoft.ML.TensorFlow.TensorFlowUtils.DownloadIfNeeded(IHostEnvironment env, String url, String dir, String fileName, Int32 timeout)\r\n   at Microsoft.ML.Vision.ImageClassificationTrainer.LoadTensorFlowSessionFromMetaGraph(IHostEnvironment env, Architecture arch)\r\n   at Microsoft.ML.Vision.ImageClassificationTrainer.InitializeTrainingGraph(IDataView input)\r\n   at Microsoft.ML.Vision.ImageClassificationTrainer.TrainModelCore(TrainContext trainContext)\r\n   at Microsoft.ML.Trainers.TrainerEstimatorBase`2.TrainTransformer(IDataView trainSet, IDataView validationSet, IPredictor initPredictor)\r\n   at Microsoft.ML.Data.EstimatorChain`1.Fit(IDataView input)\r\n   at Microsoft.ML.Data.EstimatorChain`1.Fit(IDataView input)\r\n   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String groupId, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, IChannel logger)\r\n[Source=ImageClassificationTrainer; Ensuring meta files are present., Kind=Trace] Channel disposed\r\n[Source=ImageClassificationTrainer; ImageClassificationTrainer, Kind=Trace] Channel started\r\nError downloading resource from 'https://aka.ms/mlnet-resources/meta/resnet_v2_50_299.meta': The remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\nThe remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\nThe remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\nThe remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\nThe remote name could not be resolved: 'mlnetresources.blob.core.windows.net'\\n\r\nMeta file could not be downloaded! Please copy the model file 'resnet_v2_50_299.meta' from 'meta\\resnet_v2_50_299.meta' to 'C:\\Users\\user\\AppData\\Local\\Temp\\MLNET'.\r\n```\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5745","RelatedDescription":"Closed issue \"Resource/Meta files unavailable for Image Classification\" (#5745)"},{"Id":"855923989","IsPullRequest":false,"CreatedAt":"2021-04-12T12:28:08","Actor":"davipeag","Number":"5744","RawContent":null,"Title":"Memory leak ","State":"open","Body":"### System information\r\n- Windows 10\r\n- .net 5.0\r\n- ONNX Runtime installed from nugget\r\n- ONNX Runtime version: 1.5.2\r\n- Microsoft.ML version: 1.5.2\r\n- CPU only\r\n\r\n### Issue\r\nEven after disposing the Microsoft.ML.PredictionEngine and Microsoft.ML.Transforms.Onnx.OnnxTransformer objects, memory is not freed\r\n\r\n### Source code \r\n\r\n```csharp\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing Microsoft.ML.Data;\r\nusing Microsoft.ML;\r\n\r\nnamespace Test\r\n{\r\n    class LstmCellInput\r\n    {\r\n        [ColumnName(\"attributes\")]\r\n        [VectorType(1,1,8)]\r\n        public float[] Attributes { get; set; }\r\n\r\n        [ColumnName(\"h_in\")]\r\n        [VectorType(1, 1, 16)]\r\n        public float[] HiddenState { get; set; }\r\n\r\n        [ColumnName(\"c_in\")]\r\n        [VectorType(1, 1, 16)]\r\n        public float[] CellState { get; set; }\r\n    }\r\n\r\n    class LSTMCellOutput\r\n    {\r\n        [ColumnName(\"prediction\")]\r\n        [VectorType(1, 1, 1)]\r\n        public float[] Prediction { get; set; }\r\n\r\n        [ColumnName(\"h_out\")]\r\n        [VectorType(1, 1, 16)]\r\n        public float[] HiddenState { get; set; }\r\n\r\n        [ColumnName(\"c_out\")]\r\n        [VectorType(1, 1, 16)]\r\n        public float[] CellState { get; set; }\r\n    }\r\n\r\n    class Program\r\n    {\r\n        static void Main(string[] args)\r\n        {\r\n            Console.WriteLine(\"start\");\r\n\r\n            var x = new LstmCellInput {\r\n                HiddenState = new float[16],\r\n                CellState = new float[16],\r\n                Attributes = new float[8]\r\n            };\r\n\r\n            for (int i = 0; i < 5000; i++)\r\n            {\r\n                var context = new MLContext();\r\n                var transforms = context.Transforms.ApplyOnnxModel(\r\n                    modelFile: \"celllstm.onnx\",\r\n                    inputColumnNames: new String[] { \"attributes\", \"h_in\", \"c_in\" },\r\n                    outputColumnNames: new String[] { \"prediction\", \"h_out\", \"c_out\" }\r\n                );\r\n                \r\n                var model = transforms.Fit(context.Data.LoadFromEnumerable(new List<LstmCellInput>()));\r\n                var engine = context.Model.CreatePredictionEngine<LstmCellInput, LSTMCellOutput>(model);\r\n                var y = engine.Predict(x);\r\n                model.Dispose();\r\n                engine.Dispose();\r\n            }\r\n\r\n            Console.WriteLine(\"finish\");\r\n        }\r\n    }\r\n}\r\n```\r\n### Additional Info\r\nThe onnx model was converted from pytorch\r\nThis sample program along with the onnx model is attached\r\nI originally posted this  question to ONNX RUNTIME (https://github.com/microsoft/onnxruntime/issues/7303)\r\n\r\n[Test (1).zip](https://github.com/dotnet/machinelearning/files/6296641/Test.1.zip)\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5744","RelatedDescription":"Open issue \"Memory leak \" (#5744)"},{"Id":"855282250","IsPullRequest":false,"CreatedAt":"2021-04-11T11:25:42","Actor":"PeterKottas","Number":"5743","RawContent":null,"Title":"Confidence when using text classification","State":"open","Body":"I am developing an intent classification model using the text classification pipeline. The model simply predicts intent based on a question provided (that comes from the user). Things work relatively well when the phrases are close to the training set but I start getting some weird random results for totally unrelated terms. Looking at scores, these appear to be normalized so they add up to 1. That means that I can get a relatively high score (often around +-0.8) for a phrase that have nothing to do with any question from the training set. I am wondering if:\r\na) Is there a way to get unnormalized scores, e.g. the sum of the scores is <0,1> where the closer it is to 1, the higher confidence there is.\r\nb) General confidence value that would simply tell me if I should trust the output of the prediction?\r\n\r\nI was considering creating 2 models, one with a bunch of random text that is labeled as 0 and the actual training phrases labeled as 1. The other model with just training phrases.\r\nThe first model would then give me confidence, while the other one would give me the actual result. This however seems like a huge overkill, especially considering that I would probably have to include thousands of text samples carefully picked not to resemble actual text phrases ... Seems awful but nothing else comes to mind. Anybody approached this in a more reasonable way?\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5743","RelatedDescription":"Open issue \"Confidence when using text classification\" (#5743)"},{"Id":"854924678","IsPullRequest":false,"CreatedAt":"2021-04-09T23:45:53","Actor":"LittleLittleCloud","Number":"5742","RawContent":null,"Title":"[Question] Create IDataVew/Dataframe from .arff format","State":"open","Body":"As above\r\n\r\nDo we want to add support of .arff to `TextLoader` if the implementation is straightforward?\r\n\r\nSome additional info about .arff format\r\n> https://www.cs.waikato.ac.nz/ml/weka/arff.html","Url":"https://github.com/dotnet/machinelearning/issues/5742","RelatedDescription":"Open issue \"[Question] Create IDataVew/Dataframe from .arff format\" (#5742)"},{"Id":"854470766","IsPullRequest":false,"CreatedAt":"2021-04-09T12:16:27","Actor":"PeterKottas","Number":"5741","RawContent":null,"Title":"Lazy load models + Reload on demand","State":"open","Body":"This is a potential feature request/question or maybe an opportunity for a new lib that builds on ML.NET. Hope you help me figure out which one of these.\r\n\r\nWe have a service used by N clients. Each of these clients uses (and frequently retrain) a single Engine (model). Clients also come and go so I can have N clients at one point and M a little while later where M != N. Sidenote, the number of clients could be anywhere between few tens to few hundreds or maybe thousands.\r\n\r\nIt is my understanding that fetching a prediction model from URI (that's our case, some people fetch from file system) and subsequent deserialization and setup is relatively expensive. \r\n\r\nAs far as I know, the prediction engine is also not thread-safe.\r\n\r\nI assume that's why the PredictionEnginePool was created. \r\n\r\nOur problem is the prediction engine pool is the only population during the app start. Also, there is some model auto-update functionality (watch) which doesn't work very well in our case as it introduces a lot of overhead. Imagine I have 1000 clients and I refetch 1000 models every X seconds.\r\n\r\nWhat I am thinking about is having the ability to:\r\n1. Add and remove model definitions from the pool on the fly.\r\n2. Reload models (by name) from code (only when the model is retrained). Sidenote: This might need to be synced by some mechanism if there is more than one server (Redis, messaging, or something like that).\r\n3. Have some sort of mechanism where only the most frequently used models are kept in memory.\r\na) Maybe timeout? E.g. when the model is requested and it doesn't exist, create it. Then keep it for X minutes with a moving window that is refreshed always when the same engine is requested again.\r\nb) Probably introduce some fixed limit of concurrent models where if we run out of this limit, we simply drop the oldest model and create this new one.\r\nc) Point 3.b) could introduce some callbacks that would help with autoscaling which would not only be very useful but also pretty cool IMHO.\r\nd) Naturally because it's not threadsafe, we actually have to keep more than one instance of each engine which is again something that can be optimized via some timeout, moving windows, fixed limits, and so on.\r\ne) Would be good to have some sort of middleware architecture here that allows one to gain insights into how models are requested.\r\n\r\nWhat do you think? I know this probably has no place to exist in the CORE repo and maybe belongs more in the userland. But maybe parts of it could? I would very much appreciate any feedback that would lead me to the correct path. Cheers!","Url":"https://github.com/dotnet/machinelearning/issues/5741","RelatedDescription":"Open issue \"Lazy load models + Reload on demand\" (#5741)"},{"Id":"853085917","IsPullRequest":false,"CreatedAt":"2021-04-08T05:45:50","Actor":"LittleLittleCloud","Number":"5739","RawContent":null,"Title":"image classification api - cudnn fail to initialize when using GPU to train","State":"open","Body":"### System information\r\n\r\nGPU: GTX 1060\r\nTF binary: 2.3.1\r\nCuda: 10.1\r\nCudnn: 7.6.4\r\n\r\n### Issue\r\nError log\r\n>Pipeline crashed: xf=ValueToKeyMapping{ col=Label:Label} xf=RawByteImageLoading{ col=ImageSource_featurized:ImageSource imageFolder=} xf=ColumnCopying{ col=Features:ImageSource_featurized} tr=ImageClassification{} xf=KeyToValueMapping{ col=PredictedLabel:PredictedLabel} cache=- . Exception: Tensorflow.TensorflowException: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node resnet_v2_50/conv1/Conv2D}}]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node resnet_v2_50/conv1/Conv2D}}]]\r\n\t [[resnet_v2_50/SpatialSqueeze/_7]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n   at Microsoft.ML.TensorFlow.TensorFlowUtils.Runner.Run()\r\n   at Microsoft.ML.Vision.ImageClassificationTrainer.CacheFeaturizedImagesToDisk(IDataView input, String labelColumnName, String imageColumnName, ImageProcessor imageProcessor, String inputTensorName, String outputTensorName, String cacheFilePath, Dataset dataset, Action`1 metricsCallback, Nullable`1 validationFraction)\r\n   at Microsoft.ML.Vision.ImageClassificationTrainer.TrainModelCore(TrainContext trainContext)\r\n   at Microsoft.ML.Trainers.TrainerEstimatorBase`2.TrainTransformer(IDataView trainSet, IDataView validationSet, IPredictor initPredictor)\r\n   at Microsoft.ML.Data.EstimatorChain`1.Fit(IDataView input)\r\n   at Microsoft.ML.Data.EstimatorChain`1.Fit(IDataView input)\r\n   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String groupId, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, IChannel logger) (Microsoft.ML.ModelBuilder.Utils.Logger.Info)\r\n2021-04-07 22:33:17.0887 TRACE [Source=AutoML, Kind=Error] Pipeline crashed: xf=ValueToKeyMapping{ col=Label:Label} xf=RawByteImageLoading{ col=ImageSource_featurized:ImageSource imageFolder=} xf=ColumnCopying{ col=Features:ImageSource_featurized} tr=ImageClassification{} xf=KeyToValueMapping{ col=PredictedLabel:PredictedLabel} cache=- . Exception: Tensorflow.TensorflowException: 2 root error(s) found.\r\n\r\n### Suggest fix\r\nAfter some investigation, it appears to be a bug in TF 2.x ( see this [issue](https://github.com/tensorflow/tensorflow/issues/44885). And after I set `TF_FORCE_GPU_ALLOW_GROWTH` to true, the error has gone.\r\n\r\nHowever, a better place to fix this bug might be in ImageClassification API, where it can call \r\n>tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\nbefore loading model so that user doesn't need to set up envrionment variable everytime they uses this API","Url":"https://github.com/dotnet/machinelearning/issues/5739","RelatedDescription":"Open issue \"image classification api - cudnn fail to initialize when using GPU to train\" (#5739)"},{"Id":"853039270","IsPullRequest":false,"CreatedAt":"2021-04-08T04:11:17","Actor":"iluveu28","Number":"5738","RawContent":null,"Title":"Numerical columns missing","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: Linux\r\n- **.NET Version (eg., dotnet --info)**: 3.1\r\n\r\n### Issue\r\n\r\nThis is a follow to the previous issue (https://github.com/dotnet/machinelearning/issues/5459) that @antoniovs1029 previously helped with.\r\n\r\nI have a project where the training data has 27 columns. Somehow the InferColumns excluded all the numerical columns.\r\n\r\n        var columnInference = _mlContext.Auto().InferColumns(trainingFilePath, labelColumnName);\r\n        var textLoader = _mlContext.Data.CreateTextLoader(columnInference.TextLoaderOptions);\r\n\r\n        var trainingDataView = textLoader.Load(trainingFilePath);\r\nWhen I inspect the columnInference variable, I see only the string columns. Why?\r\n\r\nAlso, when I inspect the trainingDataView variable, I see an additional column created called Features which is a vector type.\r\n\r\nAlthough the training was successful and saved. The prediction is not working because it's complaining the Features column is missing. How do I get around this?\r\n\r\n<img width=\"406\" alt=\"Predict-error\" src=\"https://user-images.githubusercontent.com/2324842/113967410-8249f000-9863-11eb-99b7-3791ef3bad96.png\">\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5738","RelatedDescription":"Open issue \"Numerical columns missing\" (#5738)"},{"Id":"850962787","IsPullRequest":false,"CreatedAt":"2021-04-06T05:04:23","Actor":"nnoradie","Number":"5737","RawContent":null,"Title":"Order of rows affects results","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**:\r\n- **.NET Version (eg., dotnet --info)**: \r\n\r\n![image](https://user-images.githubusercontent.com/69877427/113660091-40009300-9658-11eb-9ddf-eb7f77bda83c.png)\r\n\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\nRun key drivers on two datasets with a different row order\r\n\r\n- **What happened?**\r\nThe scores returned for the same data with different row order differs\r\n\r\n- **What did you expect?**\r\nThe same score to be returned for the same data no matter the row order\r\n\r\n\r\nOriginal Data returns \r\n![image](https://user-images.githubusercontent.com/69877427/113658902-d089a400-9655-11eb-94f9-0911fea5062e.png)\r\n\r\n\r\nReorderdData returns\r\n![image](https://user-images.githubusercontent.com/69877427/113658888-c4054b80-9655-11eb-9d04-f23f5e52259c.png)\r\n\r\n\r\n\r\n\r\nThe data is identical after reordering with excel custom sort left to right\r\n![image](https://user-images.githubusercontent.com/69877427/113659813-b51f9880-9657-11eb-9b06-40cdeddb6d35.png)\r\n\r\n### Source code / logs\r\n\r\nPlease paste or attach the code or logs or traces that would be helpful to diagnose the issue you are reporting.\r\n\r\nThe column names are slightly different. The data is the same, but reordered\r\n[OrderAffectsResults.zip](https://github.com/dotnet/machinelearning/files/6262140/OrderAffectsResults.zip)\r\nContains two csv files: OriginalData.csv and ReorderedData.csv\r\n\r\n\r\nFrieght is Discretized role\r\nWeight is Continuous role\r\nSubtotal is Continuous role\r\nFreightBool is default role\r\n![image](https://user-images.githubusercontent.com/69877427/113658459-f2cef200-9654-11eb-91ed-97fb92551d9f.png)\r\n\r\n\r\nOriginal data options:\r\n![image](https://user-images.githubusercontent.com/69877427/113660648-738fed00-9659-11eb-87d4-d7194586a9cd.png)\r\n\r\n\r\nReordered data options (The LabelColumnName is different, but has the same values):\r\n![image](https://user-images.githubusercontent.com/69877427/113661238-9242b380-965a-11eb-98be-8c7af2f53129.png)\r\n\r\n\r\n**Please let me know if you need any more information**\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5737","RelatedDescription":"Open issue \"Order of rows affects results\" (#5737)"},{"Id":"849036594","IsPullRequest":false,"CreatedAt":"2021-04-02T07:29:17","Actor":"stoyandimov","Number":"5736","RawContent":null,"Title":"Pause and Resume AutoML experiments","State":"open","Body":"I wonder if there are any plans to provide a way to pause/resume ML.NET AutoML experiments (with exiting the ML.NET process).\r\n\r\nOne can use AutoML to run experiments and set duration limit in seconds. However, this requires the process running for the entire duration of the experiment.\r\n\r\nHas anyone tried running and experiment and implementing some sort of pause/resume functionality?\r\n\r\nUse Case: Pause and experiment and resume after reboot\r\n- Developer starts an AutoML Regression experiment that should run for 20 hours\r\n- 8 hours into the experiment developer pauses the experiment\r\n- ML.NET persists the current state and results of the experiment\r\n- Developer shutdowns or reboots the OS\r\n- Developer resumes the experiment\r\n- ML.NET picks up where it left of and continues the experiment until it has run the whole 20 hours\r\n\r\nThe real use case for me is running long experiments at night. I'm not using my computer between 02:00 and 07:00. How can I run experiment for those 5 hours, pause the experiment and continue the next night?","Url":"https://github.com/dotnet/machinelearning/issues/5736","RelatedDescription":"Open issue \"Pause and Resume AutoML experiments\" (#5736)"},{"Id":"846114478","IsPullRequest":false,"CreatedAt":"2021-03-31T07:48:36","Actor":"bbday","Number":"5734","RawContent":null,"Title":"[Feature] transform image rotation","State":"open","Body":"Should be possible implement something like it (at the moment we have  only resize for image manipulation):\r\n\r\nmlContext.Transforms.RotateImage(outputColumnName: \"ImgRotated\", inputColumnName: \"ImagePath\", angle:\"Angle\", targetWidth:\"Width\", targetHeight:\"Height\")\r\n\r\nthat use a function like this (rotate on center and crop image)\r\n```\r\npublic static Bitmap GetRotatedByteArrayToBitmap(byte[] bytes, float angle, int targetWidth, int targetHeight)\r\n        {\r\n            using var ms = new MemoryStream(bytes);\r\n            var bm = new Bitmap(ms);\r\n\r\n            // Make a Matrix to represent rotation by this angle.\r\n            using var rotateAtOrigin = new Matrix();\r\n            rotateAtOrigin.Rotate(angle);\r\n\r\n            // Rotate the image's corners to see how big\r\n            // it will be after rotation.\r\n            PointF[] points =\r\n            {\r\n                new PointF(0, 0),\r\n                new PointF(bm.Width, 0),\r\n                new PointF(bm.Width, bm.Height),\r\n                new PointF(0, bm.Height)\r\n            };\r\n\r\n            rotateAtOrigin.TransformPoints(points);\r\n            GetPointBounds(points, out var xmin, out var xmax, out var ymin, out var ymax);\r\n\r\n            // Make a bitmap to hold the rotated result.\r\n            var wid = (int)Math.Round(xmax - xmin);\r\n            var hgt = (int)Math.Round(ymax - ymin);\r\n            using var result = new Bitmap(wid, hgt);\r\n\r\n            // Create the real rotation transformation.\r\n            var rotateAtCenter = new Matrix();\r\n            rotateAtCenter.RotateAt(angle, new PointF(wid / 2f, hgt / 2f));\r\n\r\n            // Draw the image onto the new bitmap rotated.\r\n            using (var gr = Graphics.FromImage(result))\r\n            {\r\n                // Use smooth image interpolation.\r\n                gr.InterpolationMode = InterpolationMode.High;\r\n\r\n                // Clear with the color in the image's upper left corner.\r\n                gr.Clear(bm.GetPixel(0, 0));\r\n\r\n                //// For debugging. (Makes it easier to see the background.)\r\n                //gr.Clear(Color.LightBlue);\r\n\r\n                // Set up the transformation to rotate.\r\n                gr.Transform = rotateAtCenter;\r\n\r\n                // Draw the image centered on the bitmap.\r\n                var x = (wid - bm.Width) / 2;\r\n                var y = (hgt - bm.Height) / 2;\r\n                gr.DrawImage(bm, x, y);\r\n            }\r\n\r\n            int xt = result.Width / 2 - targetWidth / 2;\r\n            int yt = result.Height / 2 - targetHeight / 2;\r\n\r\n            Rectangle cloneRect = new Rectangle(xt, yt, targetWidth, targetHeight);\r\n            return result.Clone(cloneRect, result.PixelFormat);\r\n        }\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/5734","RelatedDescription":"Open issue \"[Feature] transform image rotation\" (#5734)"},{"Id":"844272847","IsPullRequest":false,"CreatedAt":"2021-03-30T09:20:46","Actor":"ToshiyaIsomoto","Number":"5733","RawContent":null,"Title":"[Question] Why comparing (threshold and score), not (threshold and probability)","State":"open","Body":"https://github.com/dotnet/machinelearning/blob/2c8afeb64ed92f838e804fc6f4995163a7c92e3f/src/Microsoft.ML.Data/Scorers/BinaryClassifierScorer.cs#L275\r\n\r\nI want to output a high confident \"predicted label\", that the label with a \"probability\" higher than e.g., 0.8. I know the \"score\" and \"probability\" has the almost same meaning and changing \"threshold\" will output the high confidence \"predicted label\" (with such a code below).\r\n> BinaryClassification.ChangeModelThreshold(model, 0.8f);\r\n\r\nHowever, in the current code, a comparison is done between \"threshold\" and \"score\", and the \"score\" has not constant range like [0, 1] (ones for \"probability\"), right? How should I determine the \"threshold\" value? Why comparison is not done between \"threshold\" and \"probability\". \r\n ","Url":"https://github.com/dotnet/machinelearning/issues/5733","RelatedDescription":"Open issue \"[Question] Why comparing (threshold and score), not (threshold and probability)\" (#5733)"},{"Id":"842463583","IsPullRequest":false,"CreatedAt":"2021-03-30T05:12:34","Actor":"R0Wi","Number":"5731","RawContent":null,"Title":"Markdown format issue for LogLoss and LogLossReduction","State":"closed","Body":"Looking at the `MulticlassClassificationMetrics.LogLoss`, `CalibratedBinaryClassificationMetrics.LogLossReduction` and `MulticlassClassificationMetrics.LogLossReduction` it seems that the math formulas inside the remarks are not formatted correctly.\r\n\r\n---\r\n#### Document Details\r\n\r\n⚠ *Do not edit this section. It is required for docs.microsoft.com ➟ GitHub issue linking.*\r\n\r\n* ID: db0c3c11-e826-253d-983d-bc64f22bb609\r\n* Version Independent ID: db553462-9f1c-789d-caf2-408edc18d7d1\r\n* Content: [MulticlassClassificationMetrics.LogLoss Property (Microsoft.ML.Data)](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.data.multiclassclassificationmetrics.logloss?view=ml-dotnet)\r\n* Content Source: [dotnet/xml/Microsoft.ML.Data/MulticlassClassificationMetrics.xml](https://github.com/dotnet/ml-api-docs/blob/live/dotnet/xml/Microsoft.ML.Data/MulticlassClassificationMetrics.xml)\r\n* Product: **dotnet-ml-api**\r\n* GitHub Login: @natke\r\n* Microsoft Alias: **nakersha**","Url":"https://github.com/dotnet/machinelearning/issues/5731","RelatedDescription":"Closed issue \"Markdown format issue for LogLoss and LogLossReduction\" (#5731)"},{"Id":"842463889","IsPullRequest":true,"CreatedAt":"2021-03-30T02:32:42","Actor":"R0Wi","Number":"5732","RawContent":null,"Title":"Fix doc markdown Fixes #5731","State":"closed","Body":"Fixed documentation markdown remarks for\r\n* MulticlassClassificationMetrics.LogLoss\r\n* MulticlassClassificationMetrics.LogLossReduction\r\n* CalibratedBinaryClassificationMetrics.LogLossReduction\r\n\r\nSigned-off-by: Robin Windey <ro.windey@gmail.com>","Url":"https://github.com/dotnet/machinelearning/pull/5732","RelatedDescription":"Closed or merged PR \"Fix doc markdown Fixes #5731\" (#5732)"},{"Id":"842175591","IsPullRequest":false,"CreatedAt":"2021-03-26T17:40:42","Actor":"voges316","Number":"5730","RawContent":null,"Title":"AutoML binary experiment continues using poorly performing model","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: CentOS 7-9.2009\r\n- **.NET Version (eg., dotnet --info)**:  3.1.112, Microsoft.ML v1.5.5, Microsoft.ML.AutoML v0.17.5\r\n\r\n### Issue\r\n\r\nWas using automl for sentiment analysis on the imdb dataset https://ai.stanford.edu/~amaas/data/sentiment/ . Created a binary classification experiment that would use all trainers (9 by default), optimize Accuracy (also default), and run for 2 hours. I also added a progress handler that would print metrics of the results as the experiment progressed.\r\nAfter running for some time, the progress handler showed the experiment was switching between AveragedPerceptronBinary, LbfgsLogisticRegressionBinary, and SgdCalibratedBinary. The second two made some sense since the Accuracy results were higher (approx 0.90) and occasionally would increase, but the first one \"AveragedPerceptronBinary\" confused me, since it consistently had  low results (approx 0.5, 0.7, etc). \r\n\r\nI'm wondering if this is a bug in the Experiment Logic or not. I would expect the experiment to discard a poorly performing Model instead of continuing to come back to it again, which appears to be what it does for all the other models except for AveragedPerceptronBinary. For example, FastTreeBinary is only tried once and then ignored, but AveragedPerceptronBinary is tried all the way until the end.\r\n\r\n\r\n### Source code / logs\r\n\r\n```\r\nShow data in DataView: Showing 1 rows with the columns\r\n######################################################\r\nRow--> | Review:I honestly don't understand how tripe like this gets made. The worst junior-high talent show skit you've ever seen is more entertaining than this film. Will Ferrell's wrestling fetish provides the only (briefly) humorous moments. Utterly horrible.| Label:False\r\n\r\nAutoML RunExperiment (s): 7200\r\nAutoML keeping Models in memory\r\nAutoML Optizing Metric: Accuracy\r\nAutoML Trainers being used:\r\n        AveragedPerceptron\r\n        FastForest\r\n        FastTree\r\n        LightGbm\r\n        LinearSvm\r\n        LbfgsLogisticRegression\r\n        SdcaLogisticRegression\r\n        SgdCalibrated\r\n        SymbolicSgdLogisticRegression\r\n     Trainer                               Accuracy    AUC       AUPRC   F1-score  Duration\r\n   1 AveragedPerceptronBinary               0.8984    0.9591    0.9550    0.9003   33.5595 *\r\n   2 SdcaLogisticRegressionBinary           0.8845    0.9539    0.9488    0.8870   42.0153\r\n   3 LinearSvmBinary                        0.8806    0.9555    0.9504    0.8764   31.1247\r\n   4 FastTreeBinary                         0.8706    0.9444    0.9424    0.8730  182.7707\r\n   5 LbfgsLogisticRegressionBinary          0.8938    0.9579    0.9527    0.8960   63.6329\r\n   6 FastForestBinary                       0.7451    0.8190    0.8076    0.7597  145.4131\r\n   7 SgdCalibratedBinary                    0.9034    0.9606    0.9556    0.9044   33.7678 *\r\n   8 AveragedPerceptronBinary               0.4983    0.7931    0.7769    0.0000   62.3400\r\n   9 LbfgsLogisticRegressionBinary          0.8876    0.9558    0.9496    0.8893   53.2443\r\n  10 SgdCalibratedBinary                    0.9031    0.9605    0.9556    0.9040   34.0770\r\n  11 AveragedPerceptronBinary               0.4983    0.7759    0.7782    0.0000   70.6186\r\n  12 LbfgsLogisticRegressionBinary          0.8980    0.9612    0.9563    0.9006   49.4685\r\n  13 SgdCalibratedBinary                    0.8772    0.9493    0.9442    0.8743   30.8654\r\n  14 AveragedPerceptronBinary               0.5048    0.8151    0.8021    0.0317   32.4597\r\n  15 LbfgsLogisticRegressionBinary          0.8891    0.9563    0.9507    0.8912   94.6190\r\n  16 SgdCalibratedBinary                    0.9034    0.9606    0.9556    0.9041   33.7996\r\n  17 AveragedPerceptronBinary               0.5017    0.7522    0.7453    0.6682   32.2783\r\n  18 LbfgsLogisticRegressionBinary          0.8946    0.9583    0.9527    0.8966   94.1862\r\n  19 SgdCalibratedBinary                    0.8961    0.9587    0.9537    0.8957   31.7183\r\n  20 AveragedPerceptronBinary               0.4986    0.8256    0.8027    0.0015   42.6899\r\n  21 LbfgsLogisticRegressionBinary          0.8938    0.9586    0.9528    0.8963   43.6216\r\n  22 SgdCalibratedBinary                    0.9031    0.9604    0.9555    0.9037   33.1641\r\n\r\n\r\n 122 AveragedPerceptronBinary               0.4983    0.8266    0.8285    0.0000   51.6914\r\n 123 LbfgsLogisticRegressionBinary          0.9019    0.9614    0.9564    0.9033   48.7525\r\n 124 SgdCalibratedBinary                    0.9027    0.9604    0.9555    0.9033   33.3645\r\n 125 AveragedPerceptronBinary               0.6771    0.8222    0.8145    0.7431   34.6309\r\n 126 LbfgsLogisticRegressionBinary          0.9046    0.9618    0.9570    0.9063   68.3563 *\r\n 127 SgdCalibratedBinary                    0.9034    0.9605    0.9556    0.9044   33.7517\r\n 128 AveragedPerceptronBinary               0.7126    0.8588    0.8618    0.7650   39.4687\r\n 129 LbfgsLogisticRegressionBinary          0.9023    0.9617    0.9569    0.9041   67.2922\r\n 130 SgdCalibratedBinary                    0.9003    0.9599    0.9551    0.9009   31.9746\r\n 131 AveragedPerceptronBinary               0.6829    0.8260    0.8218    0.7468   34.5324\r\n 132 LbfgsLogisticRegressionBinary          0.9042    0.9620    0.9573    0.9059   62.4062\r\n 133 SgdCalibratedBinary                    0.9031    0.9605    0.9556    0.9039   33.5832\r\n 134 AveragedPerceptronBinary               0.7173    0.8573    0.8521    0.6398   31.4024\r\n 135 LbfgsLogisticRegressionBinary          0.9027    0.9617    0.9568    0.9045   62.7962\r\n 136 SgdCalibratedBinary                    0.9023    0.9605    0.9555    0.9030   33.3611\r\n 137 AveragedPerceptronBinary               0.5342    0.8236    0.8271    0.6825   32.9347\r\n 138 LbfgsLogisticRegressionBinary          0.8973    0.9605    0.9552    0.8993   80.9124\r\n 139 SgdCalibratedBinary                    0.9031    0.9605    0.9556    0.9041   34.1757\r\n 140 AveragedPerceptronBinary               0.7810    0.8643    0.8585    0.7861   32.6319\r\n 141 LbfgsLogisticRegressionBinary          0.9038    0.9618    0.9570    0.9056   61.3063\r\n 142 SgdCalibratedBinary                    0.9023    0.9605    0.9555    0.9031   34.0737\r\n 143 AveragedPerceptronBinary               0.4990    0.8360    0.8169    0.0031   31.8536\r\n 144 LbfgsLogisticRegressionBinary          0.9034    0.9611    0.9563    0.9049   50.3616\r\n 145 SgdCalibratedBinary                    0.8810    0.9495    0.9447    0.8840   30.7382\r\n 146 AveragedPerceptronBinary               0.7374    0.8331    0.8291    0.6962   31.7820\r\n 147 LbfgsLogisticRegressionBinary          0.9046    0.9620    0.9573    0.9063   62.7225\r\n 148 SgdCalibratedBinary                    0.9011    0.9588    0.9537    0.9016   31.6420\r\n 149 AveragedPerceptronBinary               0.5280    0.8004    0.7997    0.6789   38.1294\r\n 150 LbfgsLogisticRegressionBinary          0.9034    0.9620    0.9572    0.9052   59.0457\r\n 151 SgdCalibratedBinary                    0.8980    0.9586    0.9537    0.8978   31.4805\r\n 152 AveragedPerceptronBinary               0.7771    0.8568    0.8515    0.7761   31.2145\r\n\r\nTotal models produced: 154\r\nBest model's trainer: LbfgsLogisticRegressionBinary\r\nMetrics of best model from validation data --\r\nAccuracy: 0.9045963692545385\r\nAreaUnderPrecisionRecallCurve: 0.9570456314512586\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/5730","RelatedDescription":"Open issue \"AutoML binary experiment continues using poorly performing model\" (#5730)"},{"Id":"837849914","IsPullRequest":false,"CreatedAt":"2021-03-25T22:07:25","Actor":"luisquintanilla","Number":"5726","RawContent":null,"Title":"Why not newer than CUDA 10.0?","State":"closed","Body":"Transferring product question posted in .NET Docs repo from @zleepy\r\n\r\nOriginal Issue (dotnet/docs#23312)\r\n\r\nWhy should i not install CUDA newer than v10.0? 10.0 does not support newer versions of VS 2019.\r\nA newer version of CUDA 10 says it does support VS 2019, see: https://docs.nvidia.com/cuda/archive/10.2/cuda-installation-guide-microsoft-windows/index.html.\r\n\r\nAnd even newer, CUDA 11.2.1 does also say it supports VS, see: https://docs.nvidia.com/cuda/archive/11.2.1/cuda-installation-guide-microsoft-windows/index.html\r\n\r\nWhat is the reason this page explicit says that i should not install anything that Nvidia says is supported by newer versions of VS?\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5726","RelatedDescription":"Closed issue \"Why not newer than CUDA 10.0?\" (#5726)"},{"Id":"841047300","IsPullRequest":false,"CreatedAt":"2021-03-25T15:29:44","Actor":"IvoTops","Number":"5729","RawContent":null,"Title":"Support ExpandoObject dynamic members for PredictionEngine<TIn,Out> where TIn is an ExpandoObject","State":"open","Body":"The PredictionEngine raises 'Type should contain a member named xyz' when using dynamic properties from an expandoobject that actually does have the member xyz.\r\n\r\n_var expando = new ExpandoObject();\r\n// generic code to fill object with lots of properties removed\r\nvar predictionEngine = mlContext.Model.CreatePredictionEngine<ExpandoObject, ModelPrediction>(mlModel, mlSchema);                       \r\nvar prediction = predictionEngine.Predict(expando);_\r\n\r\nPlease fix this. I train lots of models with each a different and large set of features. Trying to do this with ML.NET now. For training I have a generic solution with my own IDataView where I create dynamic getter functions and use the schemabuilder. That works to train a model and create a schema dynamically supporting thousands of features.\r\n\r\nFor prediction I have to specify my input as a type like this;\r\n\r\nvar predictionEngine = mlContext.Model.CreatePredictionEngine<**REQUIREDTYPE**, ModelPrediction>(mlModel, mlSchema);                       \r\n\r\nI do not have type definitions with thousands of fields. It has to be generic. I create an expandoobject and add all the members and values that the engine needs, But it complains that the fields are not there -> 'Type should contain a member named xx'.  So that check does not take dynamic properties into account. Support for dynamic properties (or a boolean flag to skip the type check) would open a lot of usecases for flexible featuresets.\r\n\r\nHope someone can fix this test to support also dynamic properties....","Url":"https://github.com/dotnet/machinelearning/issues/5729","RelatedDescription":"Open issue \"Support ExpandoObject dynamic members for PredictionEngine<TIn,Out> where TIn is an ExpandoObject\" (#5729)"},{"Id":"839331529","IsPullRequest":false,"CreatedAt":"2021-03-24T04:24:14","Actor":"ChengYen-Tang","Number":"5728","RawContent":null,"Title":"Does ml.net add a pipeline similar to nvidia deepstream? ","State":"open","Body":"In real-time streaming image analysis, such as rtmp and rtsp, we use emgucv, opencvsharp... or other tools to pull the stream, and capture the image in it for object detection and object classification. Video stream -> GPU decoding -> CPU conversion image format -> ML.net GPU detection, and then returns the result -> C# statistics and drawing the box.\r\n\r\nThis process seems inefficient and the CPU usage is very high. If ML.net's pipelines can pull images -> detection -> drawing are all done on the GPU, it should greatly reduce the load.","Url":"https://github.com/dotnet/machinelearning/issues/5728","RelatedDescription":"Open issue \"Does ml.net add a pipeline similar to nvidia deepstream? \" (#5728)"},{"Id":"838575046","IsPullRequest":false,"CreatedAt":"2021-03-23T10:28:26","Actor":"Xilosof","Number":"5727","RawContent":null,"Title":"[Question] How realize ranking (scoring) images? ","State":"open","Body":"Hi! I didn't have any problems, but I want to ask a question.\r\nI don't know where it is better to ask a question, so I decided to do it in the developer repository. \r\nSorry if I shouldn't have done this here.\r\n\r\nI train in ml and came up with a task for myself.\r\nThe essence of the task:\r\n- **Find images with cars (Using ImageClassification with 2 labels)**\r\n- **Then the images with the cars are additionally tagged, for example, brand, color, purity, etc.**\r\n\r\nI think the best way would be to use a separate model for each tag. Check it out if I'm wrong.\r\nI think that the task of determining the brand or model will be more difficult and decided to start with the ranking.\r\n\r\nThe question is how to implement image ranking in ml.net. Should I use ImageClassification with 10 labels (1, 2, 3, ...), which are the evaluation of the beauty/cleanliness of the car? \r\nOr is there another more concise way to get the output of a single normalized number from 0 to 1 that represents the beauty/cleanliness score of the machine","Url":"https://github.com/dotnet/machinelearning/issues/5727","RelatedDescription":"Open issue \"[Question] How realize ranking (scoring) images? \" (#5727)"},{"Id":"837515425","IsPullRequest":false,"CreatedAt":"2021-03-22T09:29:09","Actor":"manuelfuchs","Number":"5725","RawContent":null,"Title":"Support for Hierarchical Clustering","State":"open","Body":"### Issue\r\n\r\nThis issue is based on #4961.\r\nIt would be nice to see support for Hierarchical Clustering besides the KMeans implementation in ML.NET.\r\nAn open source implementation of the AGNES algorithm [Kaufman & Rousseeuw, 1990] already exists in this open-source library [Aglomera](https://github.com/pedrodbs/Aglomera) that's licensed under the MIT license.\r\n\r\n> Currently, Aglomera.NET implements program AGNES (AGglomerative NESting) of [Kaufman & Rousseeuw, 1990], i.e., the bottom-up approach, the It supports different linkage criteria and also provides several metrics to perform internal and external evaluation of clustering results. The results of clustering can be exported to a Json file to be visualized as a dendrogram in Dendrogram Viewer, an interactive web-application using D3.js.\r\n\r\nBased on the description of Hierarchical Clustering by Aglomera ...\r\n\r\n> The clustering result is a list containing the cluster-set and the corresponding dissimilarity / distance at which it was created at each step of the algorithm. The result is organized in a hierarchical form, i.e., where each cluster references either the two parents that were merged for its creation (in the agglomerative approach), or the two children resulting from splitting the cluster (in the divisive approach). Due to their hierarchical nature, clustering results can be visualized via a dendrogram.\r\n\r\n... the expected result of this additional Model would not be a fixed amount of clusters (as with KMeans), but rather a tree containing the d.\r\nThe actual clusters can then be obtained by cutting the three at a certain height.\r\nBesides visualizing the hierarchical dependencies between clusters, this would give the developer more control regarding the amount of clusters, since this can be decided after the tree was calculated.\r\n\r\nReferences\r\n\r\n1. Kaufman, L., & Rousseeuw, P. J. (1990). [Finding groups in data: an introduction to cluster analysis](https://books.google.com/books?hl=en&lr=&id=YeFQHiikNo0C&oi=fnd&pg=PR11&ots=5ApcG5OEwC&sig=Sx5Bhqfaymzg1U9aRQVIFxmqiHY). John Wiley & Sons.\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/5725","RelatedDescription":"Open issue \"Support for Hierarchical Clustering\" (#5725)"}],"ResultType":"GitHubIssue"}},"RunOn":"2021-04-20T05:30:33.8934297Z","RunDurationInMilliseconds":558}